% Compile with the "lualatex main && biber main && makeglossaries main && lualatex main && lualatex main" command.

\RequirePackage{pdfmanagement}

\SetKeys[document/metadata]{
    lang = en,
    pdfversion = 1.7,
    pdfstandard = A-3b
}

\documentclass[headsepline=true,parskip=half,open=any,12pt]{scrbook}

\title{Algorithms of Quantum Chemistry}\author{Tomáš \textsc{Jíra}}

\usepackage[hidelinks]{hyperref}\makeatletter\hypersetup{pdfauthor={\@author},pdftitle={\@title}}\makeatother % hyperlinks and metadata

\usepackage{amsmath} % all the math environments and symbols
\usepackage{amssymb} % additional math symbols
\usepackage[toc,page]{appendix} % appendices
\usepackage[backend=biber,style=chem-acs]{biblatex} % bibliography package
\usepackage{fontspec} % font selection
\usepackage{braket} % braket notation
\usepackage[left=1.5cm,top=2cm,right=1.5cm,bottom=2cm]{geometry} % page layout
\usepackage[acronym,nogroupskip,nomain,toc]{glossaries} % glossary package
\usepackage{mathrsfs} % mathscr environment
\usepackage[automark]{scrlayer-scrpage} % page styles
\usepackage{unicode-math} % unicode math support
\usepackage{microtype} % better typesetting

\addbibresource{library.bib} % set the library file
\makeglossaries % make the glossary

\clearpairofpagestyles % clear the default page styles
\ihead{\pagemark} % add page number to the inner header
\ohead{\headmark} % add chapter name to the outer header

\setmainfont{Libertinus Serif} % set the main font
\setmathfont{Libertinus Math} % set the math font
\setsansfont{Libertinus Sans} % set the sans-serif font

\newtheorem{definition}{Definition}\numberwithin{definition}{section}
\newtheorem{theorem}{Theorem}\numberwithin{theorem}{section}

\newacronym{rhf}{RHF}{Restricted Hartree--Fock}
\newacronym{post-hf}{post-HF}{post-Hartree--Fock}
\newacronym{hf}{HF}{Hartree--Fock}
\newacronym{mp2}{MP2}{Møller--Plesset Perturbation Theory of 2nd Order}
\newacronym{mp3}{MP3}{Møller--Plesset Perturbation Theory of 3rd Order}
\newacronym{mppt}{MPPT}{Møller--Plesset Perturbation Theory}
\newacronym{cisdt}{CISDT}{Configuration Interaction Singles, Doubles and Triples}
\newacronym{cisd}{CISD}{Configuration Interaction Singles and Doubles}
\newacronym{fci}{FCI}{Full Configuration Interaction}
\newacronym{ci}{CI}{Configuration Interaction}
\newacronym{ccsd}{CCSD}{Coupled Cluster Singles and Doubles}
\newacronym{ccd}{CCD}{Coupled Cluster Doubles}
\newacronym{cc}{CC}{Coupled Cluster}
\newacronym{tdse}{TDSE}{Time-Dependent Schrödinger Equation}
\newacronym{tise}{TISE}{Time-Independent Schrödinger Equation}
\newacronym{diis}{DIIS}{Direct Inversion in the Iterative Subspace}
\newacronym{ms}{MS}{Molecular Spinorbital}
\newacronym{fft}{FFT}{Fast Fourier Transform}
\newacronym{ift}{IFT}{Inverse Fourier Transform}
\newacronym{dft}{DFT}{Discrete Fourier Transform}
\newacronym{ft}{FT}{Fourier Transform}
\newacronym{scf}{SCF}{Self-Consistent Field}
\newacronym{tdc}{TDC}{Time Derivative Coupling}
\newacronym{nacv}{NACV}{Nonadiabatic Coupling Vector}
\newacronym{tsh}{TSH}{Trajectory Surface Hopping}
\newacronym{boa}{BOA}{Born--Oppenheimer Approximation}
\newacronym{pes}{PES}{Potential Energy Surface}
\newacronym{hst}{HST}{Hammes-Schiffer Tully}
\newacronym{npi}{NPI}{Norm-Preserving Interpolation}

\begin{document}
\makeatletter\begin{titlepage}
    \center
    \textsc{\LARGE University of Chemistry and Technology, Prague}\\[1.5cm]
    \rule{\linewidth}{0.5mm}\\[0.4cm]
    {\huge\bfseries\@title}\\[0.4cm]
    \rule{\linewidth}{0.5mm}\\[1.5cm]
    {\large\textit{Author}}\\\@author
    \vfill\vfill\vfill
    {\large\today}
    \vfill
\end{titlepage}\makeatother
\tableofcontents
\chapter{Mathematical Background}\label{sec:mathematical_background}
\section{Vector Spaces and Linear Operators}\label{sec:vector_spaces_and_linear_operators}
Many concepts in quantum mechanics rest on the algebra of vector spaces and the action of linear operators. We review what is needed here. More complete accounts can be found in standard linear algebra and functional analysis texts. Throughout, the scalar field is a general field $F$, but in quantum mechanics we take $F=\mathbb{C}$ unless stated otherwise.
\subsection{Vector Spaces and Basic Properties}\label{sec:vector_spaces_basic_properties}
The object of interest is a vector space: a set with addition and scalar multiplication satisfying compatibility rules. The following definition states these axioms explicitly.
\begin{definition}
A vector space $(V,+,\cdot)$ over a field $F$ is a non-empty set that is closed under vector addition and scalar multiplication. Every vector space must satisfy the following axioms for all $\symbf{u},\symbf{v},\symbf{w}\in V$ and all scalars $a,b\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Associativity of vector addition: $(\symbf{u}+\symbf{v})+\symbf{w}=\symbf{u}+(\symbf{v}+\symbf{w})$
\item Commutativity of vector addition: $\symbf{u}+\symbf{v}=\symbf{v}+\symbf{u}$
\item Identity element of vector addition: $\exists\,\symbf{0}\in V$ such that $\symbf{u}+\symbf{0}=\symbf{u}$
\item Inverse element of vector addition: $\exists\,(-\symbf{u})\in V$ such that $\symbf{u}+(-\symbf{u})=\symbf{0}$
\item Compatibility of scalar multiplication with field multiplication: $a(b\symbf{u})=(ab)\symbf{u}$
\item Identity element of scalar multiplication: $1\symbf{u}=\symbf{u}$, where $1$ is the multiplicative identity in $F$
\item Distributivity of scalar multiplication with respect to vector addition: $a(\symbf{u}+\symbf{v})=a\symbf{u}+a\symbf{v}$
\item Distributivity of scalar multiplication with respect to field addition: $(a+b)\symbf{u}=a\symbf{u}+b\symbf{u}$
\end{enumerate}
\end{definition}
These axioms are necessary for a set to be considered a vector space. Examples of vector spaces include the Euclidean space $\mathbb{R}^n$, the set of all functions with continuous $m$-th derivatives $C^m(\mathbb{R}^n)$ or $C^\infty(\mathbb{R}^n)$, which is the set of all infinitely differentiable functions. In quantum mechanics we usually work in Hilbert spaces, which will be described in more detail in the next section.

Before we can use vector spaces effectively, we must identify and characterize smaller structures that inherit their properties. These are known as subspaces.
\begin{definition}
A subset $W\subset V$ is a subspace if it is non-empty and closed under addition and scalar multiplication. For all $\symbf{u},\symbf{v}\in W$ and $a\in F$, one has $\symbf{u}+\symbf{v}\in W$ and $a\symbf{u}\in W$. Equivalently, $W$ is a subspace iff $\symbf{0}\in W$, $\symbf{u}+\symbf{v}\in W$ and $a\symbf{u}\in W$.
\end{definition}
Every subspace necessarily contains the zero vector and is closed under both addition and scalar multiplication so it is itself a vector space. Subspaces are important because they capture smaller, self-contained portions of a larger space where similar operations remain valid.

We now proceed to the concept of linear combinations, which provides a mechanism for generating new vectors from existing ones, and the related concept of the span, which describes the set of all such combinations.
\begin{definition}
Given a vector space $V$ over a field $F$, a linear combination of vectors $\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\in V$ is a vector
\begin{equation}
\symbf{u}=a_1\symbf{v}_1+a_2\symbf{v}_2+\ldots+a_n\symbf{v}_n,
\end{equation}
where $a_1,a_2,\ldots,a_n\in F$. The set of all linear combinations of ${\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n}$ is called the span of these vectors
\begin{equation}
\text{span}\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace=\left\lbrace\sum_{i=1}^n a_i\symbf{v}_i\middle\vert a_i\in F\right\rbrace.
\end{equation}
The span of a set of vectors is the smallest subspace of $V$ that contains all the vectors in the set.
\end{definition}
The span formalizes the idea of building new vectors from known ones. In finite-dimensional spaces, the span of a finite set can fill the entire space (as in the case of a basis) or a lower-dimensional subspace.

To understand how vectors relate to one another within a span, we must determine whether some vectors can be expressed as combinations of others. This leads us to the notion of linear independence.
\begin{definition}
A set of vectors $\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace$ in a vector space $V$ over a field $F$ is said to be linearly independent if the only solution to the equation
\begin{equation}
a_1\symbf{v}_1+a_2\symbf{v}_2+\ldots+a_n\symbf{v}_n=\symbf{0}
\end{equation}
is $a_1=a_2=\ldots=a_n=0$, where $a_1,a_2,\ldots,a_n\in F$. If there exists a non-trivial solution (i.e., not all $a_i$ are zero), then the set is said to be linearly dependent.
\end{definition}
Linear independence ensures that no vector in the set can be constructed from others. This property is essential when defining a minimal generating set for the entire space. For linear operators (defined later), eigenvectors associated with distinct eigenvalues are linearly independent. No orthogonality is assumed here.

Finally, the concepts of basis and dimension summarize the structure of a vector space in a concise form. A basis provides coordinates for states and a matrix representation for operators.
\begin{definition}
A basis of a vector space $V$ over a field $F$ is a set of vectors $\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace$ in $V$ that is linearly independent and spans $V$. The dimension of $V$, denoted as $\text{dim}(V)$, is the number of vectors in any basis of $V$.
\end{definition}
Every vector in a vector space can be written uniquely as a linear combination of basis vectors. The dimension quantifies the minimal number of coordinates required to specify any element of the space, which is a fundamental concept underlying state representation in quantum mechanics.
\subsection{Normed, Inner Product and Complete Vector Space}\label{sec:normed_inner_product_complete_vector_space}
To introduce geometric concepts into vector spaces, we first define the normed vector space.
\begin{definition}
A normed vector space $(V,\lvert\cdot\rvert)$ is a vector space $V$ over a field $F$ (here, $F$ is $\mathbb{R}$ or $\mathbb{C}$) equipped with a norm, which is a function $\lvert\cdot\rvert:V\to\left[0,\infty\right)\subset\mathbb{R}$ that satisfies the following properties for all $\symbf{u},\symbf{v}\in V$ and all scalars $a\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Non-negativity: $\lvert\symbf{u}\rvert\geq 0$, with equality iff $\symbf{u}=\symbf{0}$
\item Absolute scalability: $\lvert a\symbf{u}\rvert=\lvert a\rvert\lvert\symbf{u}\rvert$
\item Triangle inequality: $\lvert\symbf{u}+\symbf{v}\rvert\leq\lvert\symbf{u}\rvert+\lvert\symbf{v}\rvert$
\end{enumerate}
\end{definition}
A norm provides a measure of the \emph{length} or \emph{magnitude} of vectors in the space, allowing us to discuss concepts such as convergence and continuity. The norm induces a metric (or distance function) on the vector space defined briefly as
\begin{equation}
d(\symbf{u},\symbf{v})=\lvert\symbf{u}-\symbf{v}\rvert,
\end{equation}
which satisfies all the properties of a metric, thereby enabling the study of geometric properties within the vector space. More specialized normed vector space emerges after we define an inner product.

The inner product generalizes the familiar Euclidean dot product to abstract vector spaces.
\begin{definition}\label{def:inner_product}
An inner product on a vector space $V$ over a field $F$ is a function $\langle\cdot,\cdot\rangle:V\times V\to F$ that satisfies the following properties for all $\symbf{u},\symbf{v},\symbf{w}\in V$ and all scalars $a\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Conjugate symmetry: $\langle\symbf{u},\symbf{v}\rangle=\overline{\langle\symbf{v},\symbf{u}\rangle}$
\item Linearity in the second argument: $\langle \symbf{u},a\symbf{v}+\symbf{w}\rangle=a\langle\symbf{u},\symbf{v}\rangle+\langle\symbf{u},\symbf{w}\rangle$
\item Positive-definiteness: $\langle\symbf{u},\symbf{u}\rangle\geq 0$, with equality if and only if $\symbf{u}=\symbf{0}$
\end{enumerate}
\end{definition}
The linearity in second argument and conjugate symmetry together imply conjugate-linearity in the first argument. We note here that we use the definition that is commonly used un quantum mechanics. Mathematicians often require linearity in the first argument and conjugate-linearity in the second argument. For real vector spaces, the conjugate symmetry reduces to ordinary symmetry. The inner product induces a norm (or length) of a vector $\symbf{u}$ defined by
\begin{equation}
\lvert\symbf{u}\rvert=\sqrt{\langle\symbf{u},\symbf{u}\rangle}.
\end{equation}
We can also generalize the concept of perpendicularity to abstract vector spaces using the inner product. Two vectors $\symbf{u}$ and $\symbf{v}$ are said to be orthogonal if their inner product is zero, i.e., $\langle\symbf{u},\symbf{v}\rangle=0$. If their lengths are both one, they are called orthonormal.

The inner product gives rise to two key inequalities that determine much of the geometric structure of the space. The Cauchy-Schwarz inequality states that for any vectors $\symbf{u},\symbf{v}\in V$,
\begin{equation}
\lvert\langle\symbf{u},\symbf{v}\rangle\rvert\leq\lvert\symbf{u}\rvert\lvert\symbf{v}\rvert.
\end{equation}
This inequality ensures that the angle between two vectors is well-defined and implies the triangle inequality for the induced norm. A vector space equipped with an inner product is called an inner product space. Together, these results ensure that the norm induced by an inner product satisfies all metric axioms, endowing the space with a well-defined notion of distance. Thus, every inner product space is a normed vector space, although the converse is not generally true.

Before we get to linear operators, we add one final definition that will become important when we discuss infinite-dimensional spaces.
\begin{definition}
Let $(V,\lvert\cdot\rvert)$ be a normed vector space. A sequence $\lbrace\symbf{v}_n\rbrace$ in $V$ is called a Cauchy sequence if for every $\epsilon>0$, there exists an integer $N$ such that $\lvert\symbf{v}_m-\symbf{v}_n\rvert<\epsilon$ for all $m,n>N$. The space $V$ is said to be complete if every Cauchy sequence in $V$ converges to a limit that is also in $V$. A complete normed vector space is called a Banach space.
\end{definition}
\subsection{Linear Operators}\label{sec:linear_operators}
Linear operators lie at the core of quantum mechanics, representing physical observables and the evolution of quantum states. We now define linear operators and explore their properties.
\begin{definition}
A linear operator on a vector space $V$ over a field $F$ is a map $A:V\to V$ with $A(\symbf{u}+\symbf{v})=A\symbf{u}+A\symbf{v}$ and $A(a\symbf{u})=aA\symbf{u}$ for all $\symbf{u},\symbf{v}\in V$, $a\in F$.
\end{definition}
Using a basis of a vector space, we can represent linear operators as matrices. If $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ is a basis of an $n$-dimensional vector space $V$, an operator $A$ can be represented by a matrix $[A]_{ij}=a_{ij}$ with the coefficients defined by
\begin{equation}
A\symbf{e}_j=\sum_{i=1}^n a_{ij}\symbf{e}_i.
\end{equation}
The action of $A$ on any vector $\symbf{v}=\sum_{j=1}^n v_j\symbf{e}_j$ can then be computed as
\begin{equation}
A\symbf{v}=\sum_{i=1}^n\left(\sum_{j=1}^n a_{ij}v_j\right)\symbf{e}_i.
\end{equation}
If the vector space $V$ is equipped with an inner product and the basis $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ is orthonormal, the matrix elements can also be expressed as
\begin{equation}
a_{ij}=\langle\symbf{e}_i,A\symbf{e}_j\rangle.
\end{equation}
This equation is consistent with Def~\ref{def:inner_product}, where we adopt the physics convention for the inner product. Note that the matrix representation of an operator depends on the choice of basis. If $U$ is a change of basis matrix from a new basis $\lbrace\symbf{f}_i\rbrace_{i=1}^n$ to the old basis $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ (i.e., $\symbf{e}_i=\sum_{j=1}^n U_{ji}\symbf{f}_j$), the matrix representation of $A$ in the new basis is given by
\begin{equation}\label{eq:change_of_basis}
[A]_{\symbf{e}}=U^{-1}[A]_{\symbf{f}}U.
\end{equation}
Let's now define some additional properties of linear operators.
\begin{definition}
For $A:V\to V$, the kernel (or null space) of $A$ is the set of vectors $\symbf{v}\in V$ such that $A\symbf{v}=\symbf{0}$. The image (or range) of $A$ is the set of vectors $\symbf{w}\in V$ such that $\symbf{w}=A\symbf{v}$ for some $\symbf{v}\in V$. If $\text{dim}(V)$ is finite, the rank-nullity theorem states that $\text{dim}(\text{ker}(A))+\text{dim}(\text{im}(A))=\text{dim}(V)$.
\end{definition}
Another important concept is the concept of eigenvalues and eigenvectors.
\begin{definition}
A scalar $\lambda\in F$ is called an eigenvalue of a linear operator $A:V\to V$ if there exists a non-zero vector $\symbf{v}\in V$ such that $A\symbf{v}=\lambda\symbf{v}$. The vector $\symbf{v}$ is called an eigenvector associated with the eigenvalue $\lambda$. For a finite-dimensional vector space, the set of all eigenvalues of $A$ is called the spectrum of $A$. The subspace $E_\lambda=\symrm{ker}(A-\lambda I)$ is called the eigenspace associated with the eigenvalue $\lambda$, where $I$ is the identity operator on $V$.
\end{definition}
If $V$ is finite-dimensional, the eigenvalues of $A$ can be found by solving the characteristic polynomial equation $\text{det}(A-\lambda I)=0$.
\section{Hilbert Spaces and Quantum States}\label{sec:hilbert_spaces}
Hilbert spaces are fundamental concepts in quantum mechanics, providing the mathematical framework for describing quantum states and their evolution. In this section, we will explore the definition of Hilbert spaces, their properties, and how they relate to quantum states. We will start with the definition.
\begin{definition}
A Hilbert space $\mathcal{H}$ is a complete inner product space, which means it is a vector space equipped with an inner product that allows for the definition of length and angle, and it is complete with respect to the norm induced by the inner product.
\end{definition}
In the context of quantum mechanics we will always take $\mathcal{H}$ to be a complex vector space, and we adopt the physics convention for the inner product, where $\langle\cdot,\cdot\rangle$ is conjugate-linear in its first argument and linear in its second, consistent with Def~\ref{def:inner_product}. The norm induced by the inner product is
\begin{equation}
\lvert\psi\rvert = \sqrt{\langle\psi,\psi\rangle}.
\end{equation}
Some examples of Hilbert spaces include the finite-dimensional vector spaces $\mathbb{R}^n$ or $\mathbb{C}^n$ or, as is often the case in quantum mechanics, the space of all square-integrable functions $L^2(\mathbb{R}^n)$. Formally\footnote{We will skip here the talk about the Lebesgue measure, since we do not want to complicate our lives with edge cases.}
\begin{equation}
L^2(\mathbb{R}^n)=\left\lbrace\psi :\mathbb{R}^n\to\mathbb{C}\middle\vert\int_{\mathbb{R}^n}\psi^*(\symbf{r})\psi(\symbf{r})\mathrm{d}\symbf{r}<\infty\right\rbrace
\end{equation}
where $\psi^*(\symbf{r})$ is the complex conjugate of $\psi(\symbf{r})$. In Dirac notation, elements of $\mathcal{H}$ are written as kets $\ket{\psi}$. The inner product of $\psi,\phi\in\mathcal{H}$ is then denoted by
\begin{equation}
\braket{\psi\vert\phi}:=\langle\psi,\phi\rangle.
\end{equation}
To each ket $\ket{\psi}$ there corresponds a bra $\bra{\psi}$, which is a linear functional
\begin{equation}
\bra{\psi} : \mathcal{H}\to\mathbb{C},\qquad\bra{\psi}\left(\ket{\phi}\right):=\braket{\psi|\phi}.
\end{equation}
This correspondence is guaranteed by the Riesz representation theorem.

In quantum mechanics, physical (pure) states are represented by \emph{rays} in a Hilbert space, i.e. one-dimensional subspaces of $\mathcal{H}$. Vectors that differ only by a nonzero complex scalar multiplication represent the same physical state:
\begin{equation}
\ket{\psi}\sim\ket{\phi}\Longleftrightarrow\exists\,c\in\mathbb{C}\setminus\lbrace 0\rbrace:\ket{\phi}=c\ket{\psi}.
\end{equation}
It is customary to choose a normalized representative $\ket{\psi}$ with
\begin{equation}
\braket{\psi\vert\psi}=1,
\end{equation}
in which case $\ket{\psi}$ is called a (normalized) state vector. The overall phase $e^{\mathrm i\theta}\ket{\psi}$ has no observable physical effect.

In most physical applications one assumes that $\mathcal{H}$ is \emph{separable}, i.e. it possesses a countable orthonormal basis $\lbrace\ket{n}\rbrace_{n\in\mathbb{N}_0}$. Any vector $\ket{\psi}\in\mathcal{H}$ can then be then expanded as
\begin{equation}
\ket{\psi}=\sum_{n}c_n\ket{n},\qquad c_n=\braket{n\vert\psi},
\end{equation}
where the series converges in the norm of $\mathcal{H}$.

\section{Spectral Theory in Hilbert Spaces}\label{sec:spectral_theory_in_hilbert_spaces}
In this section we introduce the basic elements of spectral theory for operators on Hilbert spaces and connect them to the description of quantum observables and measurements. Throughout, $\mathcal{H}$ denotes a complex Hilbert space with inner product $\langle\cdot,\cdot\rangle$ conjugate-linear in the first argument and linear in the second. We write $\braket{\psi|\phi}:=\langle\psi,\phi\rangle$ in Dirac notation. The associated norm is
\begin{equation}
\lvert\ket{\psi}\rvert := \sqrt{\braket{\psi|\psi}},
\end{equation}
and, for brevity, we will often write $\lvert\psi\rvert$ instead of $\lvert\ket{\psi}\rvert$.
\subsection{Self-adjoint Operators and Observables}
In quantum mechanics, observables are represented by self-adjoint operators on a Hilbert space. Spectral theory describes how such operators can be decomposed into their \emph{eigenvalue parts}, both discrete and continuous, and how this decomposition encodes measurement statistics and time evolution.
\begin{definition}
A linear operator $A$ on $\mathcal{H}$ is called \emph{self-adjoint} if
\begin{equation}
\braket{\phi|A\psi}=\braket{A\phi|\psi}\quad\text{for all }\ket{\phi},\ket{\psi}\in\mathcal{H}
\end{equation}
and the domain of $A$ coincides with the domain of its adjoint.
\end{definition}
In physics terminology, self-adjoint operators are often called Hermitian. In nonrelativistic quantum mechanics, each observable is modeled by a self-adjoint operator $A$ on $\mathcal{H}$ and its spectral properties determine the possible measurement outcomes.
\subsubsection*{Finite-Dimensional Case}
If $\mathcal{H}$ is finite-dimensional, the spectral structure is simple.
\begin{theorem}[Spectral Theorem in Finite Dimensions]
Let $\mathcal{H}$ be finite-dimensional and $A$ self-adjoint. Then there exists an orthonormal basis $\{\ket{n}\}$ of $\mathcal{H}$ such that
\begin{equation}
A\ket{n}=\lambda_n\ket{n},\qquad\lambda_n\in\mathbb{R},
\end{equation}
and
\begin{equation}
A=\sum_n\lambda_n\ket{n}\bra{n}.
\end{equation}
\end{theorem}
The real numbers $\lambda_n$ are the eigenvalues of $A$ and the kets $\ket{n}$ are the corresponding eigenstates. If the system is in a normalized state $\ket{\psi}$, the probability of obtaining the result $\lambda_n$ when measuring $A$ is
\begin{equation}
\mathbb P_A^\psi(\lambda_n)=\lvert\braket{n\vert\psi}\rvert^2,
\end{equation}
and the expectation value of $A$ is
\begin{equation}
\langle A\rangle_\psi=\braket{\psi\vert A\vert\psi}=\sum_n\lambda_n\lvert\braket{n\vert\psi}\rvert^2.
\end{equation}
This is the standard discrete version of the Born rule.
\subsection{Spectral Decomposition in Infinite Dimensions}
In infinite-dimensional Hilbert spaces, self-adjoint operators may have both discrete and continuous parts of the spectrum. The finite sum in the previous subsection is replaced by a combination of sums and integrals.

Heuristically, one can write the spectral decomposition of a self-adjoint operator $A$ as
\begin{equation}\label{eq:heuristic_spectral_decomposition}
A=\sum_n \lambda_n \ket{n}\bra{n}+\int_{\sigma_{\mathrm c}(A)} \lambda\,\ket{\lambda}\bra{\lambda}\,\mathrm{d}\lambda,
\end{equation}
where $\lambda_n$ are discrete eigenvalues with normalizable eigenstates $\ket{n}$ (bound states), $\sigma_{\mathrm c}(A)$ is the continuous part of the spectrum. The generalized kets $\ket{\lambda}$ are not elements of $\mathcal{H}$ (they are not normalizable), but they can be treated formally in Dirac’s notation. More rigorously, the spectral decomposition is described using a projection-valued measure, but the heuristic form in Eq~\eqref{eq:heuristic_spectral_decomposition} is sufficient for most physical applications.

For a normalized state $\ket{\psi}$, the discrete and continuous parts of the spectrum contribute to the normalization as
\begin{equation}
1=\lvert\psi\rvert^2=\sum_n\lvert\braket{n\vert\psi}\rvert^2+\int_{\sigma_{\mathrm c}(A)}\lvert\braket{\lambda\vert\psi}\rvert^2\,\mathrm{d}\lambda,
\end{equation}
and the probability of obtaining a measurement outcome in an interval $\Delta$ is
\begin{equation}
\mathbb P_A^\psi(\Delta)=\sum_{\lambda_n\in\Delta}\lvert\braket{n\vert\psi}\rvert^2+\int_{\Delta\cap\sigma_{\mathrm c}(A)}\lvert\braket{\lambda\vert\psi}\rvert^2\,\mathrm{d}\lambda.
\end{equation}
\subsection{Functions of Observables and Time Evolution}
Once the spectral decomposition of a self-adjoint operator $A$ is known, one can define functions of $A$ by acting on its spectral components.

In the purely discrete case,
\begin{equation}
A=\sum_n\lambda_n\ket{n}\bra{n},\qquad\ket{\psi}=\sum_n c_n\ket{n},
\end{equation}
a function $f(A)$ is defined by
\begin{equation}
f(A):=\sum_n f(\lambda_n)\ket{n}\bra{n},
\end{equation}
so that
\begin{equation}
f(A)\ket{n}=f(\lambda_n)\ket{n}.
\end{equation}

In the general (discrete + continuous) case, the same idea gives
\begin{equation}
f(A)=\sum_n f(\lambda_n)\ket{n}\bra{n}+\int_{\sigma_{\mathrm c}(A)}f(\lambda)\,\ket{\lambda}\bra{\lambda}\,\mathrm{d}\lambda,
\end{equation}
which is the heuristic form of the functional calculus. 

The most important example in quantum mechanics is the time-evolution operator generated by a Hamiltonian $H$. For a time-independent Hamiltonian, the unitary time-evolution operator is
\begin{equation}
U(t)=\mathrm e^{-\mathrm{i} t H},
\end{equation}
which, in spectral form, reads
\begin{equation}
U(t)=\sum_n \mathrm e^{-\mathrm{i} t E_n} \ket{n}\bra{n}+\int_{\sigma_{\mathrm c}(H)} \mathrm e^{-\mathrm{i} t E}\,\ket{E}\bra{E}\,\mathrm{d}E,
\end{equation}
where $E_n$ and $E$ denote discrete and continuous energy values, respectively. This expression shows explicitly how each energy component of the state picks up a phase factor $\mathrm e^{-\mathrm{i} t E}$, while the norm
\begin{equation}
\lvert U(t)\psi\rvert=\lvert\psi\rvert
\end{equation}
is preserved, expressing conservation of total probability under Schr\"odinger time evolution.
\section{Adiabatic vs Diabatic Representation}\label{sec:adiabatic_vs_diabatic_representation}
In molecular quantum mechanics we often describe a system of electrons and nuclei in terms of two sets of coordinates: electronic coordinates $\symbf{r}$ and nuclear coordinates $\symbf{R}$. The total molecular wavefunction depends on both, $\ket{\Psi(\symbf{R},\symbf{r},t)}$, and evolves under the full molecular Hamiltonian. In practice, it is convenient to expand $\ket{\Psi}$ in a suitable electronic basis that depends on the nuclear positions. Two particularly important choices are the \emph{adiabatic} and \emph{diabatic} representations.
\subsection{Born--Oppenheimer Separation}
The starting point is the separation of the Hamiltonian into nuclear and electronic parts. For a molecular system one typically writes
\begin{equation}\label{eq:boa_hamiltonian}
H=T_\mathrm{n}(\symbf{R})+H_\mathrm{el}(\symbf{r};\symbf{R}),
\end{equation}
where $T_\mathrm{n}$ is the nuclear kinetic energy operator and $H_\mathrm{el}$ contains the electronic kinetic energy and all Coulomb interactions, with the nuclei treated as fixed at positions $\symbf{R}$.

For each fixed nuclear configuration $\symbf{R}$ we can solve the electronic Schr\"odinger equation
\begin{equation}
H_\mathrm{el}(\symbf{r};\symbf{R})\ket{\phi_I(\symbf{R})}=E_I(\symbf{R})\ket{\phi_I(\symbf{R})},
\end{equation}
where $\{\ket{\phi_I(\symbf{R})}\}$ are electronic eigenstates and $E_I(\symbf{R})$ is the associated \acrfull{pes} for electronic state $I$.

The total molecular wavefunction can then be expanded as
\begin{equation}\label{eq:boa_expansion}
\ket{\Psi(\symbf{R},\symbf{r},t)}=\sum_I\ket{\chi_I(\symbf{R},t)}\otimes\ket{\phi_I(\symbf{R})},
\end{equation}
where the nuclear wavefunctions $\ket{\chi_I(\symbf{R},t)}$ act as expansion coefficients in the electronic basis.

Different choices of the electronic basis $\{\ket{\phi_I(\symbf{R})}\}$ lead to different representations of the nuclear dynamics. The two most common choices are the adiabatic and diabatic representations.
\subsection{Adiabatic Representation}
In the adiabatic representation the electronic basis is chosen as the eigenstates of the electronic Hamiltonian. By construction,
\begin{equation}
H_\mathrm{el}(\symbf{r};\symbf{R})\ket{\phi_I(\symbf{R})}=E_I(\symbf{R})\ket{\phi_I(\symbf{R})},
\end{equation}
so the electronic Hamiltonian is diagonal in this basis.

Inserting the expansion in Eq.~\eqref{eq:boa_expansion} into the time-dependent Schr\"odinger equation and projecting onto $\bra{\phi_I(\symbf{R})}$ yields coupled equations for the nuclear wavefunctions $\ket{\chi_I(\symbf{R},t)}$. These contain the potential energy surfaces $E_I(\symbf{R})$ and additional terms that arise from the nuclear kinetic energy acting on the $\symbf{R}$-dependent electronic states:
\begin{equation}
\mathrm{i}\frac{\partial}{\partial{t}}\ket{\chi_I(\symbf{R},t)}=\left[T_\mathrm{n}(\symbf{R})+E_I(\symbf{R})\right]\ket{\chi_I(\symbf{R},t)}+\sum_JC_{IJ}(\symbf{R})\,\ket{\chi_J(\symbf{R},t)},
\end{equation}
where operator $C_{IJ}(\symbf{R})$ contains all the derivative coupling terms arising from the application of $T_\mathrm{n}(\symbf{R})$ on the full wavefunction $\ket{\Psi(\symbf{R},\symbf{r},t)}$.

Physically, the adiabatic representation has two key features. The electronic Hamiltonian is diagonal which means that each $E_I(\symbf{R})$ defines a potential energy surface on which the nuclei move. And second, nonadiabatic effects appear as derivative couplings in the nuclear kinetic energy, encoded in $C_{IJ}(\symbf{R})$.

The adiabatic picture is particularly intuitive far from avoided crossings or conical intersections, where the nuclei move approximately on a single surface $E_I(\symbf{R})$. Near regions where surfaces approach or intersect, the derivative couplings become large and the adiabatic representation can become numerically inconvenient.
\subsection{Diabatic Representation}
In the diabatic representation one chooses a different set of electronic basis functions $\{\ket{\tilde\phi_\alpha(\symbf{R})}\}$, related to the adiabatic states by an $\symbf{R}$-dependent unitary transformation
\begin{equation}\label{eq:unitary_transform}
\ket{\tilde\phi_\alpha(\symbf{R})}=\sum_I U_{I\alpha}(\symbf{R})\ket{\phi_I(\symbf{R})},
\end{equation}
where $U(\symbf{R})$ is unitary for each $\symbf{R}$.

The defining property of an ideal diabatic basis is that the derivative couplings vanish:
\begin{equation}
\tilde{\symbf{d}}_{\alpha\beta}(\symbf{R})=\braket{\tilde\phi_\alpha(\symbf{R})\vert\nabla_{\symbf{R}}\tilde\phi_\beta(\symbf{R})}\approx\symbf{0},
\end{equation}
for all $\alpha,\beta$ (or at least for those states of interest). In such a basis, the nuclear kinetic energy operator acts trivially on the electronic functions and there are no derivative (nonadiabatic) couplings.

However, removing the derivative couplings in this way introduces off-diagonal elements into the potential energy matrix. The nuclear Schr\"odinger equation in the diabatic representation then has the form
\begin{equation}
\mathrm{i}\frac{\partial}{\partial{t}}\ket{\tilde\chi_\alpha(\symbf{R},t)}=T_\mathrm{n}\ket{\tilde\chi_\alpha(\symbf{R},t)}+\sum_\beta\tilde{V}_{\alpha\beta}(\symbf{R})\ket{\tilde\chi_\beta(\symbf{R},t)},
\end{equation}
where the couplings between electronic states now appear as off-diagonal potential terms rather than derivative couplings. The $\tilde{V}$ term is equivalent to the electronic Hamiltonian in Eq~\eqref{eq:boa_hamiltonian}, we call it here potential, mainly for historical reasons.

Physically, the diabatic representation has the following features. First, the nuclear kinetic energy is diagonal and free of derivative couplings (or they are at least minimized). Second, the electronic couplings appear as smooth off-diagonal potentials in $\tilde{V}(\symbf{R})$.

This is often advantageous for constructing low-dimensional model Hamiltonians and for simulating nonadiabatic dynamics, since the diabatic potentials can be chosen to vary smoothly through avoided crossings and conical intersections.

In general, a globally exact diabatic basis free of derivative couplings may not exist due to topological obstructions (e.g. around conical intersections). In practice, one works with quasi-diabatic representations where the derivative couplings are small but not exactly zero.
\subsection{Transformation Between Adiabatic and Diabatic Representations}
The nuclear wavefunctions in the two representations are related by the same unitary transformation $U(\symbf{R})$ that relates the electronic bases. Writing the total wavefunction as
\begin{equation}
\ket{\Psi(\symbf{R},\symbf{r},t)}=\sum_I\ket{\chi_I(\symbf{R},t)}\otimes\ket{\phi_I(\symbf{R})}=\sum_\alpha \ket{\tilde\chi_\alpha(\symbf{R},t)}\otimes\ket{\tilde\phi_\alpha(\symbf{R})},
\end{equation}
and inserting Eq~\eqref{eq:unitary_transform} into this equation and comparing the diabatic and adiabatic representation gives
\begin{equation}
\ket{\chi_I(\symbf{R},t)}=\sum_I U_{I\alpha}(\symbf{R})\ket{\tilde\chi_\alpha(\symbf{R},t)}.
\end{equation}
The adiabatic potential matrix is diagonal,
\begin{equation}
V_{IJ}(\symbf{R})=E_I(\symbf{R})\delta_{IJ},
\end{equation}
while the diabatic potential matrix $\tilde{V}(\symbf{R})$ is obtained using the unitary similarity transformation according to the change of basis formula in Eq~\eqref{eq:change_of_basis} as
\begin{equation}
\tilde{V}(\symbf{R})=U^\dagger(\symbf{R})V(\symbf{R})U(\symbf{R}).
\end{equation}
\chapter{Time Evolution in Quantum Mechanics}\label{sec:time_evolution}
Quantum dynamics describes the evolution of quantum systems over time and is fundamental in quantum chemistry for understanding molecular behavior at the atomic level. It is governed by the principles of quantum mechanics, particularly the \acrfull{tdse}, which dictates how wavefunctions change with time. Due to the complex nature of these equations, practical solutions are often limited to small or model systems.

There are two main approaches to quantum dynamics: real time propagation and imaginary time propagation. Real time propagation follows the natural evolution of a system in time, making it crucial for studying dynamic processes such as chemical reactions, electronic excitations, and non-equilibrium phenomena. Imaginary time propagation, on the other hand, is used to find the ground state of a quantum system by evolving it in an artificial imaginary time direction, gradually filtering out higher energy states. While these techniques provide deep insights into molecular quantum behavior, their application is restricted to relatively small or model systems due to the exponential increase in computational cost with system size.
\section{Real Time Propagation}\label{sec:real_time_propagation}
Real time propagation is a computational approach used to study the time evolution of quantum systems according to the \acrshort{tdse}. It is essential for modeling dynamic processes such as electronic excitations, molecular vibrations, and non equilibrium phenomena in quantum chemistry. By solving the wavefunction's evolution step by step in real time, this method captures transient states and ultrafast reactions. However, due to the exponential growth of computational complexity, real time propagation is feasible only for small systems or simplified models.

The time evolution in quantum mechanics is governed by the \acrshort{tdse} in the form
\begin{equation}\label{eq:tdse}
\symrm{i}\hbar\frac{\symrm{d}}{\symrm{d}t}\ket{\Psi\left(t\right)}=\hat{H}\ket{\Psi\left(t\right)},
\end{equation}
where $i$ is the imaginary unit, $\hbar$ is the reduced Planck constant, $\ket{\Psi\left(t\right)}$ is the time dependent wavefunction, $\hat{H}$ is the Hamiltonian operator, which encodes the total energy of the system. In the position representation, one could write $\Psi\left(x,t\right)$, where $x$ is the position coordinate for a quantum particle such as an electron or proton.

A general solution of the \acrshort{tdse} can be written as
\begin{equation}\label{eq:tdse_propagator}
\ket{\Psi\left(t\right)}=\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}\ket{\Psi\left(0\right)}.
\end{equation}
The action of $\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}$ on the initial wavefunction $\ket{\Psi\left(0\right)}$ propagates the state from time $0$ to time $t$, carrying all the information about the time evolution of the system. This exponential operator contains the complete description of how the system evolves with time. In practical applications, however, evaluating $\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}$ analytically is rarely straightforward for nontrivial Hamiltonians, which often necessitates numerical methods for an accurate treatment of quantum dynamics.
\subsection{Space Discretization and the Diabatic Basis}\label{sec:space_discretization_diabatic_basis}
To represent the wavefunction numerically, we sample it on a spatial grid. In practical terms, this means introducing a dependence on the coordinate $x$, so that $\ket{\Psi\left(t\right)}$ becomes $\ket{\Psi\left(x,t\right)}$. More importantly, we expand the wavefunction $\ket{\Psi\left(x,t\right)}$ in a set of basis functions $\lbrace\Phi_i\rbrace_{i=1}^N$, where $N$ is the number of states included in the dynamics. The basis set will be orthonormal and satisfy the condition
\begin{equation}\label{eq:diabatic_condition}
\bra{\Phi_i}\frac{\partial}{\partial x}\ket{\Phi_j}=0,\,i\neq j.
\end{equation}
Because these diabatic basis states are frequently taken simply as the canonical basis in $\mathbb{R}^N$ when defining model potential energy surfaces, their explicit functional forms are not crucial in most model-based simulations. Nonetheless, from a physical standpoint, these states are usually determined through quantum chemical computations that ensure minimal coordinate dependence in the electronic mixing. For a n-state system, the wavefunction in the diabatic basis can be written as
\begin{equation}\label{eq:diabatic_wavefunction}
\ket{\Psi\left(x,t\right)}=\sum_{i=1}^n c_i\left(x,t\right)\ket{\Phi_i}
\end{equation}
where $c_i\left(x,t\right)$ are the expansion coefficients of the wavefunction in the diabatic basis. The Hamiltonian operator $\hat{H}$ in the diabatic basis takes the form
\begin{equation}\label{eq:diabatic_hamiltonian}
\hat{H}=\hat{T}+\hat{V}=-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\hat{I}+\hat{V}
\end{equation}
where $\hat{I}$ is the identity operator, $\hat{T}$ is the kinetic energy operator, and $\hat{V}$ is the potential energy operator.
\subsection{Wavefunction Propagation}\label{sec:wavefunction_propagation}
We aim to reduce the action of the propagator on the diabatic wavefunction~\eqref{eq:diabatic_wavefunction} to a simple matrix multiplication. The main difficulty is the differential form of the kinetic operator in the equation~\eqref{eq:diabatic_hamiltonian}. To handle this, we use a Fourier-based method for applying the kinetic operator $\hat{T}$ on a wavefunction $\ket{\Psi\left(x,t\right)}$. Taking the \acrfull{ft} of $\hat{T}\ket{\Psi\left(x,t\right)}$ yields
\begin{align}\label{eq:kinetic_fourier_method}
\mathcal{F}\left\lbrace\hat{T}\ket{\Psi\left(x,t\right)}\right\rbrace&=\mathcal{F}\left\lbrace-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\ket{\Psi\left(x,t\right)}\right\rbrace=\frac{\hbar^2 k^2}{2m}\mathcal{F}\left\lbrace\ket{\Psi\left(x,t\right)}\right\rbrace \\
\hat{T}\ket{\Psi\left(x,t\right)}&=\frac{\hbar^2}{2m}\mathcal{F}^{-1}\left\lbrace k^2\mathcal{F}\left\lbrace\ket{\Psi\left(x,t\right)}\right\rbrace\right\rbrace,
\end{align}
where $k$ is the coordinate in the Fourier space. In other words, to apply the kinetic operator on the wavefunction, we need to \acrshort{ft} the wavefunction, multiply it by $\frac{\hbar^2k^2}{2m}$, and then apply the \acrfull{ift}. This method is computationally efficient, since the \acrshort{ft} can be done using the \acrfull{fft} algorithm, which has a complexity of $\mathcal{O}(N\log N)$, where $N$ is the number of grid points (contrary to the straightforward \acrfull{dft} with $\mathcal{O}(N^2)$ complexity).

A remaining challenge is the non-commutativity of the potential and kinetic operators. Because $\hat{V}$ and $\hat{T}$ do not commute we cannot factorize the propagator as
\begin{equation}
\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}\neq\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{V}t}\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{T}t}.
\end{equation}
If such a factorization were possible, we could apply the exponential of $\hat{V}$ in the position space and the exponential of $\hat{T}$ momentum space separately. To address this, we split the total propagation time $t$ into $n$ short steps $\Delta t$, so the full propagator becomes a product of short-time propagators as
\begin{equation}
\hat{U}(t)=\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}=\symbf{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t}\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t} \cdots\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t},
\end{equation}
where $\Delta t$ is a fixed time step. Now, each individual short-time propagator acts on the wave function and causes a small evolution. We can then use the symmetric splitting approximation
\begin{equation}
\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t}=\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}\hat{V}}\symrm{e}^{-\frac{\symrm{i}\Delta t}{\hbar}\hat{T}}\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}\hat{V}}+\mathcal{O}(\Delta t^3),
\end{equation}
which is valid for small $\Delta t$. Now we have everything we need to propagate the wavefunction in time. The formula for propagating the wavefunction from time $t$ to time $t+\Delta t$ is
\begin{equation}
\ket{\Psi\left(x,t+\Delta t\right)}=\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}V}\mathcal{F}^{-1}\left\lbrace\symrm{e}^{-\frac{\symrm{i}\Delta t}{\hbar}T}\mathcal{F}\left\lbrace\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}V}\ket{\Psi\left(x,t\right)}\right\rbrace\right\rbrace,
\end{equation}
where $\hat{T}$ is the kinetic matrix in the momentum space, and $\hat{V}$ is the potential matrix in the position space. The matrix exponential is now simply a mathematical problem and is described .

Now that we have the wavefunction an any given time, we can calculate observables, such as the density matrix, position, momentum, etc. The expectation value of position is trivial to calculate, since the wavefunction is already in the position space. The expectation value of momentum can be calculated by applying \acrshort{ft} on the wavefunction and multiplying by the momentum operator. The density matrix can be calculated by taking the outer product of the wavefunction with itself.
\subsection{The Adiabatic Transform}\label{sec:the_adiabatic_transform}
Sometimes, we would like to see the results from the dynamics in the adiabatic basis (i.e., the basis where the potential matrix $V$ is diagonal). Most of the quantum chemistry software for molecular dynamics use the adiabatic basis, since the diabatic basis is not known. For that reason, most of the surface hopping algorithms also work in the adiabatic basis so it is convenient to know how to transform the wavefunction from the diabatic basis to the adiabatic basis to compare the results. To find the transformation from the diabatic basis to the adiabatic basis, we need to find a matrix $\symbf{U}$ that diagonalizes the potential matrix $V$. To do that, we solve the eigenvalue problem
\begin{equation}
V\symbf{U}=\symbf{U}\symbf{E},
\end{equation}
for each coordinate of the grid, where $\symbf{E}$ is a diagonal matrix with the eigenvalues of $V$ on the diagonal. The columns of $\symbf{U}$ are the eigenvectors of $V$. The matrix $\symbf{U}$ is the transformation matrix from the diabatic basis to the adiabatic basis. To transform the wavefunction from the diabatic basis to the adiabatic basis, we multiply the wavefunction by the transformation matrix $\symbf{U}^\dagger$ as
\begin{equation}
\ket{\Psi_{\text{adiabatic}}\left(x,t\right)}=\symbf{U}^\dagger\ket{\Psi_{\text{diabatic}}\left(x,t\right)}.
\end{equation}
The transformation matrix $\symbf{U}$ can be used to transform matrix representations of any operator from the diabatic basis to the adiabatic basis.
\section{Imaginary Time Propagation}\label{sec:imaginary_time_propagation}
Imaginary time propagation is a numerical technique used to find the ground state of a quantum system by evolving it in imaginary time instead of real time. By replacing time with an imaginary variable, higher energy states decay exponentially, leaving only the lowest-energy state. This method is widely used in quantum chemistry and condensed matter physics to determine electronic and nuclear ground states. However, like real time propagation, its applicability is limited to small systems due to the high computational cost of solving the underlying equations.

Similar to real time propagation, we start with the \acrshort{tdse}, but this time we perform a suspicious substitution $\tau\rightarrow\symrm{i}t$ to obtain the equation
\begin{equation}\label{eq:tdse_it}
\hbar\frac{\symrm{d}}{\symrm{d}\tau}\ket{\Psi\left(\tau\right)}=-\hat{H}\ket{\Psi\left(\tau\right)},
\end{equation}
which has a formal solution given by
\begin{equation}\label{eq:wf_it}
\ket{\Psi\left(\tau\right)}=\symrm{e}^{-\frac{\hat{H}\tau}{\hbar}}\ket{\Psi\left(0\right)}.
\end{equation}
We now expand the wavefunction in the basis of the eigenstates of the Hamiltonian $\hat{H}$ as
\begin{equation}\label{eq:wf_expansion_eigenstates}
\ket{\Psi\left(\tau\right)}=\sum_{n}c_{n}\left(\tau\right)\ket{\phi_{n}},
\end{equation}
where $\ket{\phi_{n}}$ are the eigenstates of the Hamiltonian $\hat{H}$ and $c_{n}\left(\tau\right)$ are the expansion coefficients. Substituting the wavefunction expression~\eqref{eq:wf_expansion_eigenstates} into the solution of \acrshort{tdse} in imaginary time~\eqref{eq:wf_it} we obtain
\begin{equation}\label{eq:tdse_it_expansion}
\ket{\Psi\left(\tau\right)}=\sum_{n}c_{n}\left(\tau\right)\symrm{e}^{-\frac{E_{n}\tau}{\hbar}}\ket{\phi_{n}},
\end{equation}
where $E_{n}$ are the eigenvalues of the Hamiltonian $\hat{H}$. We now see, that the bigger the value of $E_n$, the faster the corresponding term in the sum decays. This means that, in the limit $\tau\rightarrow\infty$, only the term corresponding to the smallest eigenvalue $E_{0}$ will survive, and the wavefunction will converge to the ground state of the system. Note that we need to normalize the wavefunction at each step of the imaginary time propagation to ensure that the wavefunction remains normalized, otherwise the wavefunction will decay to zero.
\chapter{Mixed Quantum-Classical Dynamics}\label{sec:mixed_quantum_classical_dynamics}
\section{Quantum Amplitude Propagation in Mixed Schemes}\label{sec:quantum_amplitude_propagation_in_mixed_themes}
Under the \acrfull{boa}, the total molecular wavefunction, $\ket{\Psi(t)}$, is formally separable into electronic and nuclear parts because the nuclear masses greatly exceed those of the electrons. However, when two or more electronic states approach degeneracy, the \acrshort{boa} separation fails and nonadiabatic effects become significant. \acrfull{tsh} is a general framework that treats nuclei and electrons separately: nuclei are propagated classically on an adiabatic \acrfull{pes}, while electrons are treated quantum mechanically.

In \acrshort{tsh}, the nuclei follow Newton's equations of motion on a single adiabatic \acrshort{pes} at any instant:
\begin{equation}
\symbf{M}\ddot{\symbf{R}}(t)=-\nabla_{\symbf{R}}E_j,
\end{equation}
where $\symbf{M}$ is the diagonal matrix with masses for each coordinate, $\symbf{R}(t)$ is nuclear geometry at time $t$, and $E_j$ is the electronic energy of the state $j$ at the nuclear configuration $\symbf{R}(t)$. Meanwhile, the electrons evolve according to the \acrshort{tdse}, with the electronic wavefunction $\ket{\Phi(t)}$ expanded in the instantaneous adiabatic eigenvectors of the electronic Hamiltonian $\hat{H}^{\symrm{el}}$ as
\begin{equation}
\Phi(t)=\sum_j c_j(t)\ket{\phi_j},
\end{equation}
where $\phi_j$ satisfies the \acrfull{tise}
\begin{equation}
\hat{H}^{\symrm{el}}\ket{\phi_j}=E_j\ket{\phi_j},
\end{equation}
and the complex coefficients $c_j$ encode the instantaneous probability amplitudes for occupying each adiabatic state. For notational brevity, we will often suppress explicit dependence on the variables where no ambiguity arises.

Substituting this ansatz into the full electronic \acrshort{tdse},
\begin{equation}
i\hbar\frac{\partial}{\partial t}\ket{\Phi(t)}=\hat{H}^{\symrm{el}}\ket{\Phi(t)},
\end{equation}
and projecting onto a particular adiabatic state $\ket{\phi_j}$, we obtain a set of coupled equations for the coefficients $c_j$:
\begin{equation}\label{eq:tsh_eom}
i\hbar\frac{\symrm{d}c_j}{\symrm{d}t}=\sum_k\left(\hat{H}_{jk}^{\symrm{el}}-i\hbar\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial t}}\right)c_k,
\end{equation}
where $\sigma_{jk}=\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial t}}$ is the \acrfull{tdc}. In the purely adiabatic basis, $\hat{H}^{\symrm{el}}$ is diagonal, so electronic population transfer between states is mediated entirely by $\sigma_{jk}$:
\begin{equation}
i\hbar\frac{\partial c_j}{\partial t}=E_j c_j-i\hbar\sum_k\sigma_{jk}c_k.
\end{equation}
To evaluate $\sigma_{jk}$ in practice, note that each adiabatic eigenfunction depends parametrically on the instantaneous nuclear coordinates $\symbf{R}(t)$. By the chain rule,
\begin{equation}
\sigma_{jk}=\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial t}}=\dot{\symbf{R}}\cdot\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial\symbf{R}}}=\symbf{v}\cdot\symbf{d}_{jk},
\end{equation}
where $\symbf{v}=\dot{\symbf{R}}$ is the nuclear velocity and $\symbf{d}_{jk}$ is the \acrfull{nacv}. Although many quantum‐chemistry packages provide \acrshort{nacv} and thus allow computation of the \acrshort{tdc} via $\sigma_{jk}=\symbf{v}\cdot\symbf{d}_{jk}$, \acrshort{nacv} can become ill-defined or numerically unstable near conical intersections (e.g., in multi-reference methods). In cases where a reliable \acrshort{nacv} is not available, one may instead employ a direct \acrshort{tdc} approximation using wavefunction overlaps.
\section{The Time Derivative Coupling}\label{sec:time_derivative_coupling}
\acrshort{tdc} is a pivotal quantity in \acrshort{tsh} simulations because it enters the electronic equations of motion given in Eq.~\eqref{eq:tsh_eom}. In \emph{ab initio} molecular dynamics, the \acrshort{tdc} is commonly evaluated using the full \acrshort{nacv} as $\sigma_{jk}=\symbf{v}\cdot\symbf{d}_{jk}$, where $\symbf{v}$ is the nuclear velocity. When the dynamics evolves on analytic \acrshort{pes} or the \acrshort{nacv} is for any reason not available, it is often preferable to calculate the \acrshort{tdc} directly. The most straightforward strategy applies a finite-difference formula to the adiabatic wavefunctions
\begin{equation}\label{eq:tdc_fd1}
\sigma_{jk}(t)=\frac{1}{\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\Braket{\phi_j(t)\vert\phi_k(t)}\right)=\frac{1}{\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\delta_{jk}\right),
\end{equation}
with $\Delta t$ a small time step. When analytic \acrshort{pes} is available, the finite-difference estimate of the \acrshort{tdc} is straightforward to implement. In \emph{ab initio} molecular dynamics, however, it is usually more practical to recover this quantity from the non-adiabatic coupling vector, which most electronic-structure packages provide natively. The naïve first-order finite-difference formula suffers from well-known deficiencies: its truncation error scales linearly with the time step and it is highly sensitive to numerical noise. A simple remedy is to adopt the second-order approximation
\begin{equation}\label{eq:tdc_fd2}
\sigma_{jk}(t)=\frac{1}{2\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\Braket{\phi_j(t)\vert\phi_k(t-\Delta t)}\right).
\end{equation}
Although nominally second-order accurate, the approximation still retains significant shortcomings. In particular, it still violates the fundamental antisymmetry requirement, $\sigma_{jk}\neq-\sigma_{kj}$, undermining the Hermiticity of the electronic Hamiltonian. The following sections tackle these deficiencies and introduce more accurate, numerically robust strategies for evaluating the \acrshort{tdc}.
\subsection{The Hammes-Schiffer Tully Scheme}\label{sec:hammes_schiffer_tully_scheme}
The \acrfull{hst} scheme remedies the antisymmetry defect of Eq.~\eqref{eq:tdc_fd1} and Eq.~\eqref{eq:tdc_fd2}, making the electronic Hamiltonian Hermitian. To that end we first introduce a midpoint approximation for each adiabatic eigenstate $\phi_j$, obtained by linear interpolation between its values at the bracketing time slices
\begin{equation}\label{eq:hst_wfn}
\phi_j\left(t+\frac{\Delta t}{2}\right)=\frac{1}{2}\left(\phi_j(t+\Delta t)+\phi_j(t)\right).
\end{equation}
This midpoint state is not strictly normalized, a fully norm-preserving variant will be introduced in the next section. The time derivative at the same midpoint is obtained by combining a backward difference for $\phi_j(t+\Delta t)$ with a forward difference for $\phi_j(t)$, yielding
\begin{equation}\label{eq:hst_dwfn}
\frac{\partial \phi_j}{\partial t}\left(t+\frac{\Delta t}{2}\right)=\frac{1}{\Delta t}\left(\phi_j(t+\Delta t)-\phi_j(t)\right).
\end{equation}
The \acrshort{tdc} $\sigma_{jk}=\Braket{\phi_j\vert\frac{\partial \phi_k}{\partial t}}$ at a time $t+\frac{\Delta t}{2}$ is then formed as an overlap beween the expressions in Eq.~\eqref{eq:hst_wfn} and Eq.~\eqref{eq:hst_dwfn}:
\begin{equation}\label{eq:hst_tdc}
\sigma_{jk}\left(t+\frac{\Delta t}{2}\right)=\frac{1}{2\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\Braket{\phi_j(t+\Delta t)\vert\phi_k(t)}\right).
\end{equation}
The \acrshort{hst} scheme in Eq.~\eqref{eq:hst_tdc} is a finite-difference approximation of the \acrshort{tdc}, which is also antisymmetric, i.e., $\sigma_{jk}=-\sigma_{kj}$. The scheme is numerically stable, but it does not preserve the norm of the adiabatic wavefunction in between the timesteps. Any rapidly varying wavefunction will suffer from a significant normalization error, which can lead to unphysical results. Such a case occurs for two diabatic surfaces coupled by zero diabatic interaction: in the adiabatic basis the exact \acrshort{tdc} diverges, yet the \acrshort{hst} formula collapses to zero, predicting no population transfer. To address this issue, we can apply a \acrfull{npi} to the adiabatic wavefunction, which is discussed in the next section.
\subsection{The Norm-Preserving Interpolation}\label{sec:norm_preserving_interpolation}
\subsection{The Baeck--An Scheme}\label{sec:baeck_an}
\section{Ehrenfest Dynamics}\label{sec:ehrenfest_dynamics}
\section{Fewest Switches Surface Hopping}\label{sec:fewest_switches}
\section{Landau--Zener Surface Hopping}\label{sec:landau_zener}
\section{Mapping Approach to Surface Hopping}\label{sec:mapping_approach}
\section{Beyond Surface Hopping}\label{sec:beyond_surface_hopping}
\begin{appendices}
\chapter{Mathematical Methods}\label{sec:mathematical_methods}
\section{Matrix Exponential}\label{sec:matrix_exponential}
\section{Generalized Eigenvalue Problem}\label{sec:generalized_eigenvalue_problem}
The generalized eigenvalue problem is a mathematical problem that arises in many areas of science and engineering. It is a generalization of the standard eigenvalue problem, where we seek the eigenvalues and eigenvectors of a square matrix. In the generalized eigenvalue problem, we consider two square matrices $\symbf{A}$ and $\symbf{B}$, and we seek the eigenvalues $\lambda$ and eigenvectors $\symbf{x}$ that satisfy the equation
\begin{equation}\label{eq:gen_eig}
\symbf{A}\symbf{C}=\symbf{B}\symbf{C}\symbf{\Lambda},
\end{equation}
where $\symbf{C}$ is a matrix of eigenvectors and $\symbf{\Lambda}$ is a diagonal matrix of eigenvalues. The quick way to solve the generalized eigenvalue problem is to transform it into a standard eigenvalue problem by multiplying both sides of the equation by the inverse of $\symbf{B}$ as
\begin{equation}
\symbf{B}^{-1}\symbf{A}\symbf{C}=\symbf{C}\symbf{\Lambda}.
\end{equation}
This method is not always numerically stable, especially when the matrices $\symbf{A}$ and $\symbf{B}$ are ill-conditioned. Should you try to use this method for the Roothaan equations in the \acrshort{hf} method, you would find that the solution is is not correct. A more stable approach is to modify the equation~\eqref{eq:gen_eig} as
\begin{align}
\symbf{A}\symbf{C}&=\symbf{B}\symbf{C}\symbf{\Lambda}\nonumber \\
\symbf{B}^{-\frac{1}{2}}\symbf{A}\symbf{C}&=\symbf{B}^{\frac{1}{2}}\symbf{C}\symbf{\Lambda}\nonumber \\
\symbf{B}^{-\frac{1}{2}}\symbf{A}\symbf{B}^{-\frac{1}{2}}\symbf{B}^{\frac{1}{2}}\symbf{C}&=\symbf{B}^{\frac{1}{2}}\symbf{C}\symbf{\Lambda},
\end{align}
where we solve the standard eigenvalue problem for the matrix $\symbf{B}^{-\frac{1}{2}}\symbf{A}\symbf{B}^{-\frac{1}{2}}$ and obtain the eigenvectors $\symbf{B}^{\frac{1}{2}}\symbf{C}$ and eigenvalues $\symbf{\Lambda}$.\supercite{10.48550/arXiv.1903.11240} The eigenvectors of the original problem are then given by $\symbf{C}=\symbf{B}^{-\frac{1}{2}}\symbf{B}^{\frac{1}{2}}\symbf{C}$ and the eigenvalues are the same.
\end{appendices}
\printglossary[type=\acronymtype]
\printbibliography
\end{document}
