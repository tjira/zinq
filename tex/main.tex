% Compile with the "lualatex main && biber main && makeglossaries main && lualatex main && lualatex main" command.

\IfFileExists{/dev/null}{
    \DocumentMetadata{
        lang = en,
        pdfversion = 1.7,
        pdfstandard = A-3b
    }
}

\documentclass[headsepline=true,open=right,parskip=half,twoside=true,12pt]{scrbook}\raggedbottom

\title{Algorithms of Quantum Chemistry}\author{Tomáš \textsc{Jíra}}

\usepackage[onehalfspacing]{setspace} % setting the line spacing
\usepackage[top=25mm,bottom=30mm,inner=20mm,outer=15mm,bindingoffset=10mm]{geometry} % page composition
\usepackage[hidelinks]{hyperref} % hyperlinks and metadata
\usepackage[backend=biber,style=chem-acs]{biblatex} % bibliography package
\usepackage{braket} % braket notation
\usepackage[acronym,nogroupskip,nomain,toc]{glossaries} % glossary package
\usepackage[automark]{scrlayer-scrpage} % page styles
\usepackage{unicode-math} % unicode math support
\usepackage{microtype} % better typesetting
\usepackage{tikz} % graphics

\makeatletter\hypersetup{pdfauthor={\@author},pdftitle={\@title}}\makeatother % set metadata

\addbibresource{library.bib} % set the library file
\makeglossaries % make the glossary

\clearpairofpagestyles % clear the default page styles
\ihead{\pagemark} % add page number to the inner header
\ohead{\headmark} % add chapter name to the outer header

\setmainfont{Libertinus Serif} % set the main font
\setmathfont{Libertinus Math} % set the math font
\setsansfont{Libertinus Sans} % set the sans-serif font

\newtheorem{definition}{Definition}\numberwithin{definition}{section}
\newtheorem{theorem}{Theorem}\numberwithin{theorem}{section}

\newacronym{rhf}{RHF}{Restricted Hartree--Fock}
\newacronym{post-hf}{post-HF}{post-Hartree--Fock}
\newacronym{hf}{HF}{Hartree--Fock}
\newacronym{mp2}{MP2}{Møller--Plesset Perturbation Theory of 2nd Order}
\newacronym{mp3}{MP3}{Møller--Plesset Perturbation Theory of 3rd Order}
\newacronym{mppt}{MPPT}{Møller--Plesset Perturbation Theory}
\newacronym{cisdt}{CISDT}{Configuration Interaction Singles, Doubles and Triples}
\newacronym{cisd}{CISD}{Configuration Interaction Singles and Doubles}
\newacronym{fci}{FCI}{Full Configuration Interaction}
\newacronym{ci}{CI}{Configuration Interaction}
\newacronym{ccsd}{CCSD}{Coupled Cluster Singles and Doubles}
\newacronym{ccd}{CCD}{Coupled Cluster Doubles}
\newacronym{cc}{CC}{Coupled Cluster}
\newacronym{tdse}{TDSE}{Time-Dependent Schr\"dinger Equation}
\newacronym{tise}{TISE}{Time-Independent Schr\"dinger Equation}
\newacronym{diis}{DIIS}{Direct Inversion in the Iterative Subspace}
\newacronym{ms}{MS}{Molecular Spinorbital}
\newacronym{fft}{FFT}{Fast Fourier Transform}
\newacronym{ift}{IFT}{Inverse Fourier Transform}
\newacronym{dft}{DFT}{Discrete Fourier Transform}
\newacronym{ft}{FT}{Fourier Transform}
\newacronym{scf}{SCF}{Self-Consistent Field}
\newacronym{tdc}{TDC}{Time Derivative Coupling}
\newacronym{nacv}{NACV}{Nonadiabatic Coupling Vector}
\newacronym{tsh}{TSH}{Trajectory Surface Hopping}
\newacronym{boa}{BOA}{Born--Oppenheimer Approximation}
\newacronym{pes}{PES}{Potential Energy Surface}
\newacronym{hst}{HST}{Hammes-Schiffer Tully}
\newacronym{npi}{NPI}{Norm-Preserving Interpolation}

\begin{document}
\makeatletter\begin{titlepage}
    \center
    \textsc{\LARGE University of Chemistry and Technology, Prague}\\[1.5cm]
    \rule{\linewidth}{0.5mm}\\[0.4cm]
    {\huge\bfseries\@title}\\[0.4cm]
    \rule{\linewidth}{0.5mm}\\[1.5cm]
    {\large\textit{Author}}\\\@author
    \vfill\vfill\vfill
    {\large\today}
    \vfill
\end{titlepage}\makeatother
\tableofcontents
\chapter{Mathematical Background}\label{sec:mathematical_background}
\section{Vector Spaces and Linear Operators}\label{sec:vector_spaces_and_linear_operators}
Many concepts in quantum mechanics rest on the algebra of vector spaces and the action of linear operators. We review what is needed here. More complete accounts can be found in standard linear algebra and functional analysis texts. Throughout, the scalar field is a general field $F$, but in quantum mechanics we take $F=\mathbb{C}$ unless stated otherwise.
\subsection{Vector Spaces and Basic Properties}\label{sec:vector_spaces_basic_properties}
The object of interest is a vector space: a set with addition and scalar multiplication satisfying compatibility rules. The following definition states these axioms explicitly.
\begin{definition}[Vector Space]
A vector space $(V,+,\cdot)$ over a field $F$ is a non-empty set that is closed under vector addition and scalar multiplication. Every vector space must satisfy the following axioms for all $\symbf{u},\symbf{v},\symbf{w}\in V$ and all scalars $a,b\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Associativity of vector addition: $(\symbf{u}+\symbf{v})+\symbf{w}=\symbf{u}+(\symbf{v}+\symbf{w})$
\item Commutativity of vector addition: $\symbf{u}+\symbf{v}=\symbf{v}+\symbf{u}$
\item Identity element of vector addition: $\exists\,\symbf{0}\in V$ such that $\symbf{u}+\symbf{0}=\symbf{u}$
\item Inverse element of vector addition: $\exists\,(-\symbf{u})\in V$ such that $\symbf{u}+(-\symbf{u})=\symbf{0}$
\item Compatibility of scalar multiplication with field multiplication: $a(b\symbf{u})=(ab)\symbf{u}$
\item Identity element of scalar multiplication: $1\symbf{u}=\symbf{u}$, where $1$ is the multiplicative identity in $F$
\item Distributivity of scalar multiplication with respect to vector addition: $a(\symbf{u}+\symbf{v})=a\symbf{u}+a\symbf{v}$
\item Distributivity of scalar multiplication with respect to field addition: $(a+b)\symbf{u}=a\symbf{u}+b\symbf{u}$
\end{enumerate}
\end{definition}
These axioms are necessary for a set to be considered a vector space. Examples of vector spaces include the Euclidean space $\mathbb{R}^n$, the set of all functions with continuous $m$-th derivatives $C^m(\mathbb{R}^n)$ or $C^\infty(\mathbb{R}^n)$, which is the set of all infinitely differentiable functions. In quantum mechanics we usually work in Hilbert spaces, which will be described in more detail in the next section.

Before we can use vector spaces effectively, we must identify and characterize smaller structures that inherit their properties. These are known as subspaces.
\begin{definition}[Subspace]
A subset $W\subset V$ is a subspace if it is non-empty and closed under addition and scalar multiplication. For all $\symbf{u},\symbf{v}\in W$ and $a\in F$, one has $\symbf{u}+\symbf{v}\in W$ and $a\symbf{u}\in W$. Equivalently, $W$ is a subspace iff $\symbf{0}\in W$, $\symbf{u}+\symbf{v}\in W$ and $a\symbf{u}\in W$.
\end{definition}
Every subspace necessarily contains the zero vector and is closed under both addition and scalar multiplication so it is itself a vector space. Subspaces are important because they capture smaller, self-contained portions of a larger space where similar operations remain valid.

We now proceed to the concept of linear combinations, which provides a mechanism for generating new vectors from existing ones, and the related concept of the span, which describes the set of all such combinations.
\begin{definition}[Linear Combination]
Given a vector space $V$ over a field $F$, a linear combination of vectors $\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\in V$ is a vector
\begin{equation}
\symbf{u}=a_1\symbf{v}_1+a_2\symbf{v}_2+\ldots+a_n\symbf{v}_n,
\end{equation}
where $a_1,a_2,\ldots,a_n\in F$. The set of all linear combinations of ${\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n}$ is called the span of these vectors
\begin{equation}
\text{span}\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace=\left\lbrace\sum_{i=1}^n a_i\symbf{v}_i\middle\vert a_i\in F\right\rbrace.
\end{equation}
The span of a set of vectors is the smallest subspace of $V$ that contains all the vectors in the set.
\end{definition}
The span formalizes the idea of building new vectors from known ones. In finite-dimensional spaces, the span of a finite set can fill the entire space (as in the case of a basis) or a lower-dimensional subspace.

To understand how vectors relate to one another within a span, we must determine whether some vectors can be expressed as combinations of others. This leads us to the notion of linear independence.
\begin{definition}[Linear Independence]
A set of vectors $\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace$ in a vector space $V$ over a field $F$ is said to be linearly independent if the only solution to the equation
\begin{equation}
a_1\symbf{v}_1+a_2\symbf{v}_2+\ldots+a_n\symbf{v}_n=\symbf{0}
\end{equation}
is $a_1=a_2=\ldots=a_n=0$, where $a_1,a_2,\ldots,a_n\in F$. If there exists a non-trivial solution (i.e., not all $a_i$ are zero), then the set is said to be linearly dependent.
\end{definition}
Linear independence ensures that no vector in the set can be constructed from others. This property is essential when defining a minimal generating set for the entire space. For linear operators (defined later), eigenvectors associated with distinct eigenvalues are linearly independent. No orthogonality is assumed here.

Finally, the concepts of basis and dimension summarize the structure of a vector space in a concise form. A basis provides coordinates for states and a matrix representation for operators.
\begin{definition}[Basis]
A basis of a vector space $V$ over a field $F$ is a set of vectors $\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace$ in $V$ that is linearly independent and spans $V$. The dimension of $V$, denoted as $\text{dim}(V)$, is the number of vectors in any basis of $V$.
\end{definition}
Every vector in a vector space can be written uniquely as a linear combination of basis vectors. The dimension quantifies the minimal number of coordinates required to specify any element of the space, which is a fundamental concept underlying state representation in quantum mechanics.
\subsection{Normed, Inner Product and Complete Vector Space}\label{sec:normed_inner_product_complete_vector_space}
To introduce geometric concepts into vector spaces, we first define the normed vector space.
\begin{definition}[Normed Vector Space]
A normed vector space $(V,\lvert\cdot\rvert)$ is a vector space $V$ over a field $F$ (here, $F$ is $\mathbb{R}$ or $\mathbb{C}$) equipped with a norm, which is a function $\lvert\cdot\rvert:V\to\left[0,\infty\right)\subset\mathbb{R}$ that satisfies the following properties for all $\symbf{u},\symbf{v}\in V$ and all scalars $a\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Non-negativity: $\lvert\symbf{u}\rvert\geq 0$, with equality iff $\symbf{u}=\symbf{0}$
\item Absolute scalability: $\lvert a\symbf{u}\rvert=\lvert a\rvert\lvert\symbf{u}\rvert$
\item Triangle inequality: $\lvert\symbf{u}+\symbf{v}\rvert\leq\lvert\symbf{u}\rvert+\lvert\symbf{v}\rvert$
\end{enumerate}
\end{definition}
A norm provides a measure of the length or magnitude of vectors in the space, allowing us to discuss concepts such as convergence and continuity. The norm induces a metric (or distance function) on the vector space defined briefly as
\begin{equation}
d(\symbf{u},\symbf{v})=\lvert\symbf{u}-\symbf{v}\rvert,
\end{equation}
which satisfies all the properties of a metric, thereby enabling the study of geometric properties within the vector space. More specialized normed vector space emerges after we define an inner product.

The inner product generalizes the familiar Euclidean dot product to abstract vector spaces.
\begin{definition}[Inner Product]\label{def:inner_product}
An inner product on a vector space $V$ over a field $F$ is a function $\langle\cdot,\cdot\rangle:V\times V\to F$ that satisfies the following properties for all $\symbf{u},\symbf{v},\symbf{w}\in V$ and all scalars $a\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Conjugate symmetry: $\langle\symbf{u},\symbf{v}\rangle=\overline{\langle\symbf{v},\symbf{u}\rangle}$
\item Linearity in the second argument: $\langle \symbf{u},a\symbf{v}+\symbf{w}\rangle=a\langle\symbf{u},\symbf{v}\rangle+\langle\symbf{u},\symbf{w}\rangle$
\item Positive-definiteness: $\langle\symbf{u},\symbf{u}\rangle\geq 0$, with equality if and only if $\symbf{u}=\symbf{0}$
\end{enumerate}
\end{definition}
The linearity in second argument and conjugate symmetry together imply conjugate-linearity in the first argument. We note here that we use the definition that is commonly used un quantum mechanics. Mathematicians often require linearity in the first argument and conjugate-linearity in the second argument. For real vector spaces, the conjugate symmetry reduces to ordinary symmetry. The inner product induces a norm (or length) of a vector $\symbf{u}$ defined by
\begin{equation}
\lvert\symbf{u}\rvert=\sqrt{\langle\symbf{u},\symbf{u}\rangle}.
\end{equation}
We can also generalize the concept of perpendicularity to abstract vector spaces using the inner product. Two vectors $\symbf{u}$ and $\symbf{v}$ are said to be orthogonal if their inner product is zero, i.e., $\langle\symbf{u},\symbf{v}\rangle=0$. If their lengths are both one, they are called orthonormal.

The inner product gives rise to two key inequalities that determine much of the geometric structure of the space. The Cauchy-Schwarz inequality states that for any vectors $\symbf{u},\symbf{v}\in V$,
\begin{equation}
\lvert\langle\symbf{u},\symbf{v}\rangle\rvert\leq\lvert\symbf{u}\rvert\lvert\symbf{v}\rvert.
\end{equation}
This inequality ensures that the angle between two vectors is well-defined and implies the triangle inequality for the induced norm. A vector space equipped with an inner product is called an inner product space. Together, these results ensure that the norm induced by an inner product satisfies all metric axioms, endowing the space with a well-defined notion of distance. Thus, every inner product space is a normed vector space, although the converse is not generally true.

Before we get to linear operators, we add one final definition that will become important when we discuss infinite-dimensional spaces.
\begin{definition}[Complete Vector Space]
Let $(V,\lvert\cdot\rvert)$ be a normed vector space. A sequence $\lbrace\symbf{v}_n\rbrace$ in $V$ is called a Cauchy sequence if for every $\epsilon>0$, there exists an integer $N$ such that $\lvert\symbf{v}_m-\symbf{v}_n\rvert<\epsilon$ for all $m,n>N$. The space $V$ is said to be complete if every Cauchy sequence in $V$ converges to a limit that is also in $V$. A complete normed vector space is called a Banach space.
\end{definition}
\subsection{Linear Operators}\label{sec:linear_operators}
Linear operators lie at the core of quantum mechanics, representing physical observables and the evolution of quantum states. We now define linear operators and explore their properties.
\begin{definition}[Linear Operator]
A linear operator on a vector space $V$ over a field $F$ is a map $A:V\to V$ with $A(\symbf{u}+\symbf{v})=A\symbf{u}+A\symbf{v}$ and $A(a\symbf{u})=aA\symbf{u}$ for all $\symbf{u},\symbf{v}\in V$, $a\in F$.
\end{definition}
Using a basis of a vector space, we can represent linear operators as matrices. If $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ is a basis of an $n$-dimensional vector space $V$, an operator $A$ can be represented by a matrix $[A]_{ij}=a_{ij}$ with the coefficients defined by
\begin{equation}
A\symbf{e}_j=\sum_{i=1}^n a_{ij}\symbf{e}_i.
\end{equation}
The action of $A$ on any vector $\symbf{v}=\sum_{j=1}^n v_j\symbf{e}_j$ can then be computed as
\begin{equation}
A\symbf{v}=\sum_{i=1}^n\left(\sum_{j=1}^n a_{ij}v_j\right)\symbf{e}_i.
\end{equation}
If the vector space $V$ is equipped with an inner product and the basis $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ is orthonormal, the matrix elements can also be expressed as
\begin{equation}
a_{ij}=\langle\symbf{e}_i,A\symbf{e}_j\rangle.
\end{equation}
This equation is consistent with Def~\ref{def:inner_product}, where we adopt the physics convention for the inner product. Note that the matrix representation of an operator depends on the choice of basis. If $U$ is a change of basis matrix from a new basis $\lbrace\symbf{f}_i\rbrace_{i=1}^n$ to the old basis $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ (i.e., $\symbf{e}_i=\sum_{j=1}^n U_{ji}\symbf{f}_j$), the matrix representation of $A$ in the new basis is given by
\begin{equation}\label{eq:change_of_basis}
[A]_{\symbf{e}}=U^{-1}[A]_{\symbf{f}}U.
\end{equation}
Let's now define some additional properties of linear operators.
\begin{definition}[Null Space]
For $A:V\to V$, the kernel (or null space) of $A$ is the set of vectors $\symbf{v}\in V$ such that $A\symbf{v}=\symbf{0}$. The image (or range) of $A$ is the set of vectors $\symbf{w}\in V$ such that $\symbf{w}=A\symbf{v}$ for some $\symbf{v}\in V$. If $\text{dim}(V)$ is finite, the rank-nullity theorem states that $\text{dim}(\text{ker}(A))+\text{dim}(\text{im}(A))=\text{dim}(V)$.
\end{definition}
Another important concept is the concept of eigenvalues and eigenvectors.
\begin{definition}[Eigenvalues and Eigenvectors]
A scalar $\lambda\in F$ is called an eigenvalue of a linear operator $A:V\to V$ if there exists a non-zero vector $\symbf{v}\in V$ such that $A\symbf{v}=\lambda\symbf{v}$. The vector $\symbf{v}$ is called an eigenvector associated with the eigenvalue $\lambda$. For a finite-dimensional vector space, the set of all eigenvalues of $A$ is called the spectrum of $A$. The subspace $E_\lambda=\symrm{ker}(A-\lambda I)$ is called the eigenspace associated with the eigenvalue $\lambda$, where $I$ is the identity operator on $V$.
\end{definition}
If $V$ is finite-dimensional, the eigenvalues of $A$ can be found by solving the characteristic polynomial equation $\text{det}(A-\lambda I)=0$.
\section{Hilbert Spaces and Quantum States}\label{sec:hilbert_spaces_and_quantum_states}
Hilbert spaces are fundamental concepts in quantum mechanics, providing the mathematical framework for describing quantum states and their evolution. In this section, we will explore the definition of Hilbert spaces, their properties, and how they relate to quantum states. We will start with the definition.
\begin{definition}[Hilbert Space]
A Hilbert space $\mathcal{H}$ is a complete inner product space, which means it is a vector space equipped with an inner product that allows for the definition of length and angle, and it is complete with respect to the norm induced by the inner product.
\end{definition}
In the context of quantum mechanics we will always take $\mathcal{H}$ to be a complex vector space, and we adopt the physics convention for the inner product, where $\langle\cdot,\cdot\rangle$ is conjugate-linear in its first argument and linear in its second, consistent with Def~\ref{def:inner_product}. The norm induced by the inner product is
\begin{equation}
\lvert\psi\rvert = \sqrt{\langle\psi,\psi\rangle}.
\end{equation}
Some examples of Hilbert spaces include the finite-dimensional vector spaces $\mathbb{R}^n$ or $\mathbb{C}^n$ or, as is often the case in quantum mechanics, the space of all square-integrable functions $L^2(\mathbb{R}^n)$. Formally\footnote{We will skip here the talk about the Lebesgue measure, since we do not want to complicate our lives with edge cases.}
\begin{equation}
L^2(\mathbb{R}^n)=\left\lbrace\psi :\mathbb{R}^n\to\mathbb{C}\middle\vert\int_{\mathbb{R}^n}\psi^*(\symbf{r})\psi(\symbf{r})\mathrm{d}\symbf{r}<\infty\right\rbrace
\end{equation}
where $\psi^*(\symbf{r})$ is the complex conjugate of $\psi(\symbf{r})$. In Dirac notation, elements of $\mathcal{H}$ are written as kets $\ket{\psi}$. The inner product of $\psi,\phi\in\mathcal{H}$ is then denoted by
\begin{equation}
\braket{\psi\vert\phi}:=\langle\psi,\phi\rangle.
\end{equation}
To each ket $\ket{\psi}$ there corresponds a bra $\bra{\psi}$, which is a linear functional
\begin{equation}
\bra{\psi} : \mathcal{H}\to\mathbb{C},\qquad\bra{\psi}\left(\ket{\phi}\right):=\braket{\psi|\phi}.
\end{equation}
This correspondence is guaranteed by the Riesz representation theorem.

In quantum mechanics, physical (pure) states are represented by rays in a Hilbert space, i.e. one-dimensional subspaces of $\mathcal{H}$. Vectors that differ only by a nonzero complex scalar multiplication represent the same physical state:
\begin{equation}
\ket{\psi}\sim\ket{\phi}\Longleftrightarrow\exists\,c\in\mathbb{C}\setminus\lbrace 0\rbrace:\ket{\phi}=c\ket{\psi}.
\end{equation}
It is customary to choose a normalized representative $\ket{\psi}$ with
\begin{equation}
\braket{\psi\vert\psi}=1,
\end{equation}
in which case $\ket{\psi}$ is called a (normalized) state vector. The overall phase $e^{i\theta}\ket{\psi}$ has no observable physical effect.

In most physical applications one assumes that $\mathcal{H}$ is separable, i.e. it possesses a countable orthonormal basis $\lbrace\ket{n}\rbrace_{n\in\mathbb{N}_0}$. Any vector $\ket{\psi}\in\mathcal{H}$ can then be then expanded as
\begin{equation}
\ket{\psi}=\sum_{n}c_n\ket{n},\qquad c_n=\braket{n\vert\psi},
\end{equation}
where the series converges in the norm of $\mathcal{H}$.

\section{Spectral Theory in Hilbert Spaces}\label{sec:spectral_theory_in_hilbert_spaces}
In this section we introduce the basic elements of spectral theory for operators on Hilbert spaces and connect them to the description of quantum observables and measurements. Throughout, $\mathcal{H}$ denotes a complex Hilbert space with inner product $\langle\cdot,\cdot\rangle$ conjugate-linear in the first argument and linear in the second. We write $\braket{\psi|\phi}:=\langle\psi,\phi\rangle$ in Dirac notation. The associated norm is
\begin{equation}
\lvert\ket{\psi}\rvert := \sqrt{\braket{\psi|\psi}},
\end{equation}
and, for brevity, we will often write $\lvert\psi\rvert$ instead of $\lvert\ket{\psi}\rvert$.
\subsection{Self-adjoint Operators and Observables}\label{sec:self_adjoint_operators_and_observables}
In quantum mechanics, observables are represented by self-adjoint operators on a Hilbert space. Spectral theory describes how such operators can be decomposed into their eigenvalue parts, both discrete and continuous, and how this decomposition encodes measurement statistics and time evolution.
\begin{definition}[Self-Adjoint Operator]
A linear operator $A$ on $\mathcal{H}$ is called self-adjoint if
\begin{equation}
\braket{\phi|A\psi}=\braket{A\phi|\psi}\quad\text{for all }\ket{\phi},\ket{\psi}\in\mathcal{H}
\end{equation}
and the domain of $A$ coincides with the domain of its adjoint.
\end{definition}
In physics terminology, self-adjoint operators are often called Hermitian. In nonrelativistic quantum mechanics, each observable is modeled by a self-adjoint operator $A$ on $\mathcal{H}$ and its spectral properties determine the possible measurement outcomes.

Since a lot of operators in quantum mechanics require unitarity, we also define it here.
\begin{definition}[Unitary Operator]
A linear operator $U$ on a Hilbert space $\mathcal{H}$ is called unitary if
\begin{equation}
U^\dagger U=U U^\dagger=I,
\end{equation}
where $U^\dagger$ is the adjoint of $U$ and $I$ is the identity on $\mathcal{H}$.
\end{definition}
Unitary operators preserve the inner product and hence the norm
\begin{equation}
\braket{U\psi\vert U\phi}=\braket{\psi\vert\phi}\quad\text{for all }\psi,\phi\in\mathcal{H},
\end{equation}
so that $\lvert U\psi\rvert=\lvert\psi\rvert$. In quantum mechanics they represent symmetry
transformations and the time evolution of isolated systems.

\subsubsection*{Finite-Dimensional Case}\label{sec:finite_dimensional_case}
If $\mathcal{H}$ is finite-dimensional, the spectral structure is simple.
\begin{theorem}[Spectral Theorem]
Let $\mathcal{H}$ be a finite-dimensional Hilbert space and $A$ self-adjoint. Then there exists an orthonormal basis $\{\ket{n}\}$ of $\mathcal{H}$ such that
\begin{equation}
A\ket{n}=\lambda_n\ket{n},\qquad\lambda_n\in\mathbb{R},
\end{equation}
and
\begin{equation}
A=\sum_n\lambda_n\ket{n}\bra{n}.
\end{equation}
\end{theorem}
The real numbers $\lambda_n$ are the eigenvalues of $A$ and the kets $\ket{n}$ are the corresponding eigenstates. If the system is in a normalized state $\ket{\psi}$, the probability of obtaining the result $\lambda_n$ when measuring $A$ is
\begin{equation}
\mathbb P_A^\psi(\lambda_n)=\lvert\braket{n\vert\psi}\rvert^2,
\end{equation}
and the expectation value of $A$ is
\begin{equation}
\langle A\rangle_\psi=\braket{\psi\vert A\vert\psi}=\sum_n\lambda_n\lvert\braket{n\vert\psi}\rvert^2.
\end{equation}
This is the standard discrete version of the Born rule.
\subsection{Spectral Decomposition in Infinite Dimensions}\label{sec:spectral_decompositions_in_infinite_dimenstions}
In infinite-dimensional Hilbert spaces, self-adjoint operators may have both discrete and continuous parts of the spectrum. The finite sum in the previous subsection is replaced by a combination of sums and integrals.

Heuristically, one can write the spectral decomposition of a self-adjoint operator $A$ as
\begin{equation}\label{eq:heuristic_spectral_decomposition}
A=\sum_n \lambda_n \ket{n}\bra{n}+\int_{\sigma_{\symrm{c}}(A)} \lambda\,\ket{\lambda}\bra{\lambda}\,\symrm{d}\lambda,
\end{equation}
where $\lambda_n$ are discrete eigenvalues with normalizable eigenstates $\ket{n}$ (bound states), $\sigma_{\symrm{c}}(A)$ is the continuous part of the spectrum. The generalized kets $\ket{\lambda}$ are not elements of $\mathcal{H}$ (they are not normalizable), but they can be treated formally in Dirac’s notation. More rigorously, the spectral decomposition is described using a projection-valued measure, but the heuristic form in Eq~\eqref{eq:heuristic_spectral_decomposition} is sufficient for most physical applications.

For a normalized state $\ket{\psi}$, the discrete and continuous parts of the spectrum contribute to the normalization as
\begin{equation}
1=\lvert\psi\rvert^2=\sum_n\lvert\braket{n\vert\psi}\rvert^2+\int_{\sigma_{\symrm{c}}(A)}\lvert\braket{\lambda\vert\psi}\rvert^2\,\symrm{d}\lambda,
\end{equation}
and the probability of obtaining a measurement outcome in an interval $\Delta$ is
\begin{equation}
\mathbb P_A^\psi(\Delta)=\sum_{\lambda_n\in\Delta}\lvert\braket{n\vert\psi}\rvert^2+\int_{\Delta\cap\sigma_{\symrm{c}}(A)}\lvert\braket{\lambda\vert\psi}\rvert^2\,\symrm{d}\lambda.
\end{equation}
\subsection{Functions of Observables and Time Evolution}\label{sec:functions_of_observables_and_time_evolution}
Once the spectral decomposition of a self-adjoint operator $A$ is known, one can define functions of $A$ by acting on its spectral components.

In the purely discrete case,
\begin{equation}
A=\sum_n\lambda_n\ket{n}\bra{n},\qquad\ket{\psi}=\sum_n c_n\ket{n},
\end{equation}
a function $f(A)$ is defined by
\begin{equation}
f(A):=\sum_n f(\lambda_n)\ket{n}\bra{n},
\end{equation}
so that
\begin{equation}
f(A)\ket{n}=f(\lambda_n)\ket{n}.
\end{equation}

In the general (discrete + continuous) case, the same idea gives
\begin{equation}
f(A)=\sum_n f(\lambda_n)\ket{n}\bra{n}+\int_{\sigma_{\symrm{c}}(A)}f(\lambda)\ket{\lambda}\bra{\lambda}\,\symrm{d}\lambda,
\end{equation}
which is the heuristic form of the functional calculus. 

The most important example in quantum mechanics is the time-evolution operator generated by a Hamiltonian $H$. For a time-independent Hamiltonian, the unitary time-evolution operator is
\begin{equation}
U(t)=e^{-i t H},
\end{equation}
which, in spectral form, reads
\begin{equation}
U(t)=\sum_n e^{-i t E_n} \ket{n}\bra{n}+\int_{\sigma_{\symrm{c}}(H)}e^{-i t E}\ket{E}\bra{E}\,\symrm{d}E,
\end{equation}
where $E_n$ and $E$ denote discrete and continuous energy values, respectively. This expression shows explicitly how each energy component of the state picks up a phase factor $e^{-i t E}$, while the norm
\begin{equation}
\lvert U(t)\psi\rvert=\lvert\psi\rvert
\end{equation}
is preserved, expressing conservation of total probability under Schr\"odinger time evolution.
\section{Adiabatic vs Diabatic Representation}\label{sec:adiabatic_vs_diabatic_representation}
In molecular quantum mechanics we often describe a system of electrons and nuclei in terms of two sets of coordinates: electronic coordinates $\symbf{r}$ and nuclear coordinates $\symbf{R}$. The total molecular wavefunction depends on both, $\ket{\Psi(\symbf{R},\symbf{r},t)}$, and evolves under the full molecular Hamiltonian. In practice, it is convenient to expand $\ket{\Psi}$ in a suitable electronic basis that depends on the nuclear positions. Two particularly important choices are the adiabatic and diabatic representations.
\subsection{Born--Oppenheimer Separation}\label{sec:born_oppenheimer_separation}
The starting point is the separation of the Hamiltonian into nuclear and electronic parts. For a molecular system one typically writes
\begin{equation}\label{eq:boa_hamiltonian}
H=T_{\symrm{n}}(\symbf{R})+H_{\symrm{el}}(\symbf{r};\symbf{R}),
\end{equation}
where $T_{\symrm{n}}$ is the nuclear kinetic energy operator and $H_{\symrm{el}}$ contains the electronic kinetic energy and all Coulomb interactions, with the nuclei treated as fixed at positions $\symbf{R}$.

For each fixed nuclear configuration $\symbf{R}$ we can solve the electronic Schr\"odinger equation
\begin{equation}
H_{\symrm{el}}(\symbf{r};\symbf{R})\ket{\phi_I(\symbf{R})}=E_I(\symbf{R})\ket{\phi_I(\symbf{R})},
\end{equation}
where $\{\ket{\phi_I(\symbf{R})}\}$ are electronic eigenstates and $E_I(\symbf{R})$ is the associated \acrfull{pes} for electronic state $I$.

The total molecular wavefunction can then be expanded as
\begin{equation}\label{eq:boa_expansion}
\ket{\Psi(\symbf{R},\symbf{r},t)}=\sum_I\ket{\chi_I(\symbf{R},t)}\otimes\ket{\phi_I(\symbf{R})},
\end{equation}
where the nuclear wavefunctions $\ket{\chi_I(\symbf{R},t)}$ act as expansion coefficients in the electronic basis.

Different choices of the electronic basis $\{\ket{\phi_I(\symbf{R})}\}$ lead to different representations of the nuclear dynamics. The two most common choices are the adiabatic and diabatic representations.
\subsection{Adiabatic Representation}\label{sec:adiabatic_representation}
In the adiabatic representation the electronic basis is chosen as the eigenstates of the electronic Hamiltonian. By construction,
\begin{equation}
H_{\symrm{el}}(\symbf{r};\symbf{R})\ket{\phi_I(\symbf{R})}=E_I(\symbf{R})\ket{\phi_I(\symbf{R})},
\end{equation}
so the electronic Hamiltonian is diagonal in this basis.

Inserting the expansion in Eq.~\eqref{eq:boa_expansion} into the time-dependent Schr\"odinger equation and projecting onto $\bra{\phi_I(\symbf{R})}$ yields coupled equations for the nuclear wavefunctions $\ket{\chi_I(\symbf{R},t)}$. These contain the potential energy surfaces $E_I(\symbf{R})$ and additional terms that arise from the nuclear kinetic energy acting on the $\symbf{R}$-dependent electronic states:
\begin{equation}
i\frac{\partial}{\partial{t}}\ket{\chi_I(\symbf{R},t)}=\left[T_{\symrm{n}}(\symbf{R})+E_I(\symbf{R})\right]\ket{\chi_I(\symbf{R},t)}+\sum_JC_{IJ}(\symbf{R})\,\ket{\chi_J(\symbf{R},t)},
\end{equation}
where operator $C_{IJ}(\symbf{R})$ contains all the derivative coupling terms arising from the application of $T_{\symrm{n}}(\symbf{R})$ on the full wavefunction $\ket{\Psi(\symbf{R},\symbf{r},t)}$.

Physically, the adiabatic representation has two key features. The electronic Hamiltonian is diagonal which means that each $E_I(\symbf{R})$ defines a potential energy surface on which the nuclei move. And second, nonadiabatic effects appear as derivative couplings in the nuclear kinetic energy, encoded in $C_{IJ}(\symbf{R})$.

The adiabatic picture is particularly intuitive far from avoided crossings or conical intersections, where the nuclei move approximately on a single surface $E_I(\symbf{R})$. Near regions where surfaces approach or intersect, the derivative couplings become large and the adiabatic representation can become numerically inconvenient.
\subsection{Diabatic Representation}\label{sec:diabatic_representation}
In the diabatic representation one chooses a different set of electronic basis functions $\{\ket{\tilde\phi_\alpha(\symbf{R})}\}$, related to the adiabatic states by an $\symbf{R}$-dependent unitary transformation
\begin{equation}\label{eq:unitary_transform}
\ket{\phi_I(\symbf{R})}=\sum_\alpha U_{\alpha I}(\symbf{R})\ket{\tilde\phi_\alpha(\symbf{R})},
\end{equation}
where $U(\symbf{R})$ is unitary for each $\symbf{R}$.

The defining property of an ideal diabatic basis is that the derivative couplings vanish:
\begin{equation}
\tilde{\symbf{d}}_{\alpha\beta}(\symbf{R})=\braket{\tilde\phi_\alpha(\symbf{R})\vert\nabla_{\symbf{R}}\tilde\phi_\beta(\symbf{R})}\approx\symbf{0},
\end{equation}
for all $\alpha,\beta$ (or at least for those states of interest). In such a basis, the nuclear kinetic energy operator acts trivially on the electronic functions and there are no derivative (nonadiabatic) couplings.

However, removing the derivative couplings in this way introduces off-diagonal elements into the potential energy matrix. The nuclear Schr\"odinger equation in the diabatic representation then has the form
\begin{equation}
i\frac{\partial}{\partial{t}}\ket{\tilde\chi_\alpha(\symbf{R},t)}=\tilde{T}_{\symrm{n}}\ket{\tilde\chi_\alpha(\symbf{R},t)}+\sum_\beta\tilde{V}_{\alpha\beta}(\symbf{R})\ket{\tilde\chi_\beta(\symbf{R},t)},
\end{equation}
where the couplings between electronic states now appear as off-diagonal potential terms rather than derivative couplings. The $\tilde{V}$ term is equivalent to the electronic Hamiltonian in Eq~\eqref{eq:boa_hamiltonian} in diabatic basis, we call it here potential, mainly for historical reasons.

Physically, the diabatic representation has the following features. First, the nuclear kinetic energy is diagonal and free of derivative couplings (or they are at least minimized). Second, the electronic couplings appear as smooth off-diagonal potentials in $\tilde{V}(\symbf{R})$.

This is often advantageous for constructing low-dimensional model Hamiltonians and for simulating nonadiabatic dynamics, since the diabatic potentials can be chosen to vary smoothly through avoided crossings and conical intersections.

In general, a globally exact diabatic basis free of derivative couplings may not exist due to topological obstructions (e.g. around conical intersections). In practice, one works with quasi-diabatic representations where the derivative couplings are small but not exactly zero.
\subsection{Adiabatic Transform}\label{sec:adiabatic_transform}
The nuclear wavefunctions in the two representations are related by the same unitary transformation $U(\symbf{R})$ that relates the electronic bases. Writing the total wavefunction as
\begin{equation}
\ket{\Psi(\symbf{R},\symbf{r},t)}=\sum_I\ket{\chi_I(\symbf{R},t)}\otimes\ket{\phi_I(\symbf{R})}=\sum_\alpha \ket{\tilde\chi_\alpha(\symbf{R},t)}\otimes\ket{\tilde\phi_\alpha(\symbf{R})},
\end{equation}
and inserting Eq~\eqref{eq:unitary_transform} into this equation and comparing the diabatic and adiabatic representation gives
\begin{equation}
\ket{\chi_I(\symbf{R},t)}=\sum_\alpha U_{\alpha I}^\ast(\symbf{R})\ket{\tilde\chi_\alpha(\symbf{R},t)}.
\end{equation}
The adiabatic potential matrix is diagonal,
\begin{equation}
V_{IJ}(\symbf{R})=E_I(\symbf{R})\delta_{IJ},
\end{equation}
while the adiabatic potential matrix $\tilde{V}(\symbf{R})$ is obtained using the unitary similarity transformation according to the change of basis formula in Eq~\eqref{eq:change_of_basis} as
\begin{equation}
V(\symbf{R})=U^\dagger(\symbf{R})\tilde{V}(\symbf{R})U(\symbf{R}).
\end{equation}
\chapter{Time Evolution in Quantum Mechanics}\label{sec:time_evolution}
Quantum dynamics describes the evolution of quantum states $\ket{\Psi(t)}\in\mathcal{H}$ over time and is fundamental in quantum chemistry for understanding molecular behavior at the atomic level. It is governed by the principles of quantum mechanics, particularly the \acrfull{tdse}
\begin{equation}\label{eq:tdse}
i\frac{\symrm{d}}{\symrm{d}t}\ket{\Psi\left(t\right)}=H\ket{\Psi\left(t\right)},
\end{equation}
which dictates how wavefunctions change with time. Due to the complex nature of these equations, practical solutions are often limited to small or model systems.

This section describes, how we can practically obtain the solution to the \acrshort{tdse}. We work in the complex separable Hilbert space $\mathcal{H}$ introduced in Sec~\ref{sec:hilbert_spaces_and_quantum_states}, with self-adjoint Hamiltonian $H$ (Sec~\ref{sec:self_adjoint_operators_and_observables}), so that the time-evolution operators described in this chapter are consistent with functional calculus treatment in Sec~\ref{sec:spectral_theory_in_hilbert_spaces}.
\section{Numerical Schemes for TDSE}\label{sec:numerical_schemes_for_time_dependent_schrodinger_equation}
This section focuses on numerical solutions to \acrshort{tdse}. In the described methods, we work in a position representation $\Psi\left(\symbf{R},t\right):=\braket{\symbf{R}\vert\Psi\left(t\right)}$. More importantly, we expand the wavefunction $\ket{\Psi\left(t\right)}$ in a set of diabatic basis functions $\lbrace\tilde\phi_I\rbrace_{I=1}^N$ defined in Sec~\ref{sec:adiabatic_vs_diabatic_representation}, so that $\Psi\left(\symbf{R},t\right)$ is an N-component vector in a electronic diabatic subspace.
\subsection{Split-Operator Method}\label{sec:split_operator_method}
For a time-independent Hamiltonian, the solution to the \acrshort{tdse} can be written as
\begin{equation}\label{eq:tdse_propagator}
\ket{\Psi\left(t\right)}=e^{-i t H}\ket{\Psi\left(0\right)}.
\end{equation}
The action of the operator $e^{-i t H}$ on the initial wavefunction $\ket{\Psi\left(0\right)}$ propagates the state from time $0$ to time $t$, carrying all the information about the time evolution of the system. Since $H$ is self-adjoint, the spectral theorem yields a unitary propagator, ensuring $\lvert\Psi(t)\rvert=\lvert\Psi(0)\rvert$. This exponential operator contains the complete description of how the system evolves with time. In practical applications, however, evaluating $e^{-i t H}$ analytically is rarely straightforward for nontrivial Hamiltonians, which is exactly the problem the Split-Operator method solves.

For an N-state system, the wavefunction in the diabatic basis can be written as
\begin{equation}\label{eq:diabatic_wavefunction}
\Psi\left(\symbf{R},t\right)=\sum_{I=1}^N c_I\left(\symbf{R},t\right)\ket{\tilde\phi_I}
\end{equation}
where $c_I\left(\symbf{R},t\right)$ are the expansion coefficients of the wavefunction in the diabatic basis. The Hamiltonian operator $H$ in the diabatic basis takes the form
\begin{equation}\label{eq:diabatic_hamiltonian}
H=\tilde{T}_{\symrm{n}}+\tilde{V}=-\frac{1}{2m}\nabla_\mathbf{R}^2\otimes I_N+\tilde{V}
\end{equation}
where $I_N$ is the identity operator on the $N$-dimensional diabatic electronic space,
$\tilde{T}_{\symrm{n}}$ is the nuclear kinetic energy operator, and $\tilde{V}$ is the
potential energy operator represented by an $N\times N$ matrix in the diabatic electronic basis. We aim to reduce the action of the propagator on the diabatic wavefunction in Eq~\eqref{eq:diabatic_wavefunction} to a simple matrix multiplication. The main difficulty is the differential form of the kinetic operator in the equation~\eqref{eq:diabatic_hamiltonian}. To handle this, we use a Fourier-based method for applying the kinetic operator $\tilde{T}_{\symrm{n}}$ on a wavefunction $\Psi\left(\symbf{R},t\right)$. Taking the \acrfull{ft} of $\tilde{T}_{\symrm{n}}\Psi\left(\symbf{R},t\right)$ yields
\begin{equation}\label{eq:kinetic_fourier_method}
\mathcal{F}\left\lbrace \tilde{T}_{\symrm{n}}\Psi\left(\symbf{R},t\right)\right\rbrace=\mathcal{F}\left\lbrace-\frac{1}{2m}\frac{\partial^2}{\partial \symbf{R}^2}\Psi\left(\symbf{R},t\right)\right\rbrace=\frac{k^2}{2m}\mathcal{F}\left\lbrace\Psi\left(\symbf{R},t\right)\right\rbrace,
\end{equation}
where $k$ is the coordinate in the Fourier space. In other words, to apply the kinetic operator on the wavefunction, we need to \acrshort{ft} the wavefunction, multiply it by $\frac{k^2}{2m}$, and then apply the \acrfull{ift}. In Fourier space, $\tilde{T}_{\symrm{n}}$ becomes diagonal multiplication operator $\frac{k^2}{2m}$. For notational simplicity we denote both the coordinate-space operator and its Fourier-space representation by the same symbol. This method is computationally efficient, since the \acrshort{ft} can be done using the \acrfull{fft} algorithm, which has a complexity of $\mathcal{O}(N\log N)$, where $N$ is the number of grid points (contrary to the straightforward \acrfull{dft} with $\mathcal{O}(N^2)$ complexity).

A remaining challenge is the non-commutativity of the potential and kinetic operators. Because $\tilde{V}$ and $\tilde{T}_{\symrm{n}}$ do not commute we cannot factorize the propagator as
\begin{equation}
e^{-i t H}\neq e^{-i t\tilde{V}}e^{-i t\tilde{T}_{\symrm{n}}}.
\end{equation}
If such a factorization were possible, we could apply the exponential of $\tilde{V}$ in the position space and the exponential of $\tilde{T}_{\symrm{n}}$ momentum space separately. To address this, we split the total propagation time $t$ into $n$ short steps $\Delta t$, so the full propagator becomes a product of short-time propagators as
\begin{equation}
U(t)=e^{-i t H}=\left(e^{-i\Delta t H}\right)^n,\qquad t=n\Delta t
\end{equation}
where $\Delta t$ is a fixed time step. Now, each individual short-time propagator acts on the wave function and causes a small evolution. We can then use the symmetric splitting approximation
\begin{equation}
U(\Delta t)=e^{-i\frac{\Delta t}{2}\tilde{V}}e^{-i\Delta t\tilde{T}_{\symrm{n}}}e^{-i\frac{\Delta t}{2}\tilde{V}}+\mathcal{O}(\Delta t^3),
\end{equation}
which is valid for small $\Delta t$. Now we have everything we need to propagate the wavefunction in time. The formula for propagating the wavefunction from time $t$ to time $t+\Delta t$ is
\begin{equation}
\Psi\left(\symbf{R},t+\Delta t\right)=e^{-i\frac{\Delta t}{2}\tilde V}\mathcal{F}^{-1}\left\lbrace e^{-i\Delta t\tilde{T}_{\symrm{n}}}\mathcal{F}\left\lbrace e^{-i\frac{\Delta t}{2}\tilde V}\Psi\left(\symbf{R},t\right)\right\rbrace\right\rbrace,
\end{equation}
where $\tilde{T}_{\symrm{n}}$ is the kinetic operator in the momentum space, and $\tilde{V}$ is the potential operator in the position space. In a discrete grid representation these become matrices acting on the sampled wavefunction.
\subsection{Crank--Nicolson Scheme}\label{sec:crank_nicolson_scheme}
The Split-Operator method in Sec.~\ref{sec:split_operator_method} exploits the simple action
of the kinetic and potential operators in Fourier and position representation, respectively,
and approximates the short-time propagator by a symmetric factorization of
$e^{-i\Delta t H}$. An alternative widely used time-integration scheme is the
Crank--Nicolson method, which can be viewed as applying a rational (Padé) approximation to
the propagator.

We again consider the \acrshort{tdse} for a time-independent, self-adjoint Hamiltonian $H$,
\begin{equation}\label{eq:tdse_cn}
i\frac{\symrm{d}}{\symrm{d}t}\ket{\Psi(t)}=H\ket{\Psi(t)},
\end{equation}
acting on the separable Hilbert space $\mathcal{H}$ introduced in Sec.~\ref{sec:hilbert_spaces_and_quantum_states}. Let $t_n=n\Delta t$ denote a uniform time grid with step $\Delta t>0$ and define
\begin{equation}
\ket{\Psi^n}:=\ket{\Psi(t_n)}.
\end{equation}
The Crank–Nicolson scheme is obtained by approximating the time derivative by a centered second-order accurate finite difference at the midpoint $t_{n+\frac12}=t_n+\Delta t/2$ and evaluating the right-hand side via the trapezoidal rule (midpoint average) as
\begin{equation}\label{eq:cn_trapezoidal}
\frac{\ket{\Psi^{n+1}}-\ket{\Psi^n}}{\Delta t}=-\frac{i}{2}\left(H\ket{\Psi^{n+1}}+H\ket{\Psi^n}\right).
\end{equation}
Both sides of Eq.~\eqref{eq:cn_trapezoidal} approximate the exact quantities at the midpoint
with an $\mathcal{O}(\Delta t^2)$ error. Rearranging Eq.~\eqref{eq:cn_trapezoidal} yields the linear system
\begin{equation}\label{eq:cn_linear_system}
\left(I+\frac{i\Delta t}{2}H\right)\ket{\Psi^{n+1}}=\left(I-\frac{i\Delta t}{2}H\right)\ket{\Psi^n},
\end{equation}
where $I$ is the identity operator on $\mathcal{H}$. For each time step one solves Eq.~\eqref{eq:cn_linear_system} for $\ket{\Psi^{n+1}}$ given $\ket{\Psi^n}$.

Formally, Eq.~\eqref{eq:cn_linear_system} defines a one-step propagation
\begin{equation}\label{eq:cn_propagator_operator}
\ket{\Psi^{n+1}}=U_{\symrm{CN}}(\Delta t)\ket{\Psi^n},\qquad U_{\symrm{CN}}(\Delta t)=\left(I+\frac{i\Delta t}{2}H\right)^{-1}\left(I-\frac{i\Delta t}{2}H\right).
\end{equation}
The operator $U_{\symrm{CN}}(\Delta t)$ is equivalent to the $[1,1]$ Padé approximation to the exact propagator $e^{-i\Delta t H}$:
\begin{equation}
U_{\symrm{CN}}(\Delta t)=e^{-i\Delta t H}+\mathcal{O}(\Delta t^3).
\end{equation}
Because $H$ is self-adjoint, $U_{\symrm{CN}}(\Delta t)$ is unitary so that $\lvert\Psi^{n+1}\rvert=\lvert\Psi^n\rvert$ and the total probability is conserved, as required by the unitary time evolution discussed in Sec.~\ref{sec:spectral_theory_in_hilbert_spaces}.
\subsection{Imaginary Time Propagation}\label{sec:imaginary_time_propagation}
Imaginary time propagation is a numerical technique for obtaining the ground state of a
quantum system by evolving it in imaginary time instead of real time. By replacing the real
time variable by an imaginary one, contributions from higher-energy eigenstates are
suppressed exponentially faster than that of the ground state, so that repeated
propagation and renormalization project an initial state onto the ground state. As with real-time propagation, its practical applicability is restricted by the cost of repeatedly applying the Hamiltonian to the wavefunction.

We again consider the \acrshort{tdse} for a time-independent, self-adjoint Hamiltonian
$H$ acting on a vector from Hilbert space $\mathcal{H}$,
\begin{equation}\label{eq:tdse_it_real}
i\frac{\symrm{d}}{\symrm{d}t}\ket{\Psi(t)}=H\ket{\Psi(t)}.
\end{equation}
Introducing imaginary time $\tau$ via $\tau = i t$ (equivalently $t=-i\tau$) and writing
$\ket{\Psi(\tau)}:=\ket{\Psi(t=-i\tau)}$ gives the imaginary-time evolution equation
\begin{equation}\label{eq:tdse_it}
\frac{\symrm{d}}{\symrm{d}\tau}\ket{\Psi(\tau)}=-H\ket{\Psi(\tau)},
\end{equation}
with formal solution
\begin{equation}\label{eq:wf_it}
\ket{\Psi(\tau)}=e^{-\tau H}\ket{\Psi(0)}.
\end{equation}
Let $\{\ket{\phi_I}\}$ denote a complete orthonormal set of eigenstates of $H$,
\begin{equation}
H\ket{\phi_I}=E_I\ket{\phi_I},
\end{equation}
with real eigenvalues $E_I$ bounded from below. Expanding the initial state in this eigenbasis,
\begin{equation}\label{eq:wf_expansion_eigenstates}
\ket{\Psi(0)}=\sum_I c_I(0)\ket{\phi_I},
\end{equation}
and substituting into Eq.~\eqref{eq:wf_it} yields
\begin{equation}\label{eq:tdse_it_expansion}
\ket{\Psi(\tau)}=\sum_I c_I(0)e^{-\tau E_I}\ket{\phi_I}.
\end{equation}
For $\tau>0$, components with larger eigenvalues $E_I$ decay faster than those with
smaller $E_I$. Assuming the ground-state coefficient $c_0(0)\neq 0$, the normalized state
$\frac{\ket{\Psi(\tau)}}{\lvert\Psi(\tau)\rvert}$ converges to the ground state $\ket{\phi_0}$ in the limit
$\tau\to\infty$. In numerical applications one therefore propagates in small imaginary-time
steps and renormalizes the wavefunction after each step to prevent decay to the zero
vector and to extract the ground state.
\chapter{Mixed Quantum-Classical Dynamics}\label{sec:mixed_quantum_classical_dynamics}
\section{Quantum Amplitude Propagation in Mixed Schemes}\label{sec:quantum_amplitude_propagation_in_mixed_themes}
Under the \acrfull{boa}, the total molecular wavefunction, $\ket{\Psi(t)}$, is formally separable into electronic and nuclear parts because the nuclear masses greatly exceed those of the electrons. However, when two or more electronic states approach degeneracy, the \acrshort{boa} separation fails and nonadiabatic effects become significant. \acrfull{tsh} is a general framework that treats nuclei and electrons separately: nuclei are propagated classically on an adiabatic \acrfull{pes}, while electrons are treated quantum mechanically.

In \acrshort{tsh}, the nuclei follow Newton's equations of motion on a single adiabatic \acrshort{pes} at any instant:
\begin{equation}
\symbf{M}\ddot{\symbf{R}}(t)=-\nabla_{\symbf{R}}E_I,
\end{equation}
where $\symbf{M}$ is the diagonal matrix with masses for each coordinate, $\symbf{R}(t)$ is nuclear geometry at time $t$, and $E_I$ is the electronic energy of the state $I$ at the nuclear configuration $\symbf{R}(t)$. Meanwhile, the electrons evolve according to the \acrshort{tdse}, with the electronic wavefunction $\ket{\Phi(t)}$ expanded in the instantaneous adiabatic eigenvectors of the electronic Hamiltonian $H_{\symrm{el}}$ as
\begin{equation}
\ket{\Phi(t)}=\sum_I c_I(t)\ket{\phi_I},
\end{equation}
where $\phi_I$ satisfies the \acrfull{tise}
\begin{equation}
H_{\symrm{el}}\ket{\phi_I}=E_I\ket{\phi_I},
\end{equation}
and the complex coefficients $c_I$ encode the instantaneous probability amplitudes for occupying each adiabatic state. For notational brevity, we will often suppress explicit dependence on the variables where no ambiguity arises.

Substituting this ansatz into the full electronic \acrshort{tdse},
\begin{equation}
i\frac{\partial}{\partial t}\ket{\Phi(t)}=H_{\symrm{el}}\ket{\Phi(t)},
\end{equation}
and projecting onto a particular adiabatic state $\ket{\phi_I}$, we obtain a set of coupled equations for the coefficients $c_I$:
\begin{equation}\label{eq:tsh_eom}
i\frac{\symrm{d}c_I}{\symrm{d}t}=\sum_J\left(H_{IJ}^{\symrm{el}}-i\Braket{\phi_I\vert\frac{\partial\phi_J}{\partial t}}\right)c_J,
\end{equation}
where $\sigma_{IJ}=\Braket{\phi_I\vert\frac{\partial\phi_J}{\partial t}}$ is the \acrfull{tdc}. In the purely adiabatic basis, $H_{\symrm{el}}$ is diagonal, so electronic population transfer between states is mediated entirely by $\sigma_{IJ}$:
\begin{equation}\label{eq:sh_tdse_coefs}
i\frac{\partial c_I}{\partial t}=E_I c_I-i\sum_J\sigma_{IJ}c_J.
\end{equation}
With $\symbf{c}(t)=\left(c_1(t),c_2(t),\dots,c_N(t)\right)^{\symrm{T}}$ the Eq~\eqref{eq:sh_tdse_coefs} is a Schr\"odinger equation in a finite-dimensional Hilbert space $\mathbb{C}^N$ and can be rewritten as
\begin{equation}
i\symbf{\dot{c}}(t)=H_{\symrm{eff}}\symbf{c}(t),\qquad H_{IJ}^{\symrm{eff}}=E_I\delta_{IJ}-i\sigma_{IJ}
\end{equation}
For electronic dynamics in Eq~\eqref{eq:sh_tdse_coefs} to be unitary, the effective Hamiltonian $H_{\symrm{eff}}$ must be self-adjoint, which in turn requires $\sigma_{IJ}$ to be antisymmetric, $\sigma^\dagger=-\sigma$, in accordance with Sec~\ref{sec:self_adjoint_operators_and_observables}. To evaluate $\sigma_{IJ}$ in practice, note that each adiabatic eigenfunction depends parametrically on the instantaneous nuclear coordinates $\symbf{R}(t)$. By the chain rule,
\begin{equation}
\sigma_{IJ}=\Braket{\phi_I\vert\frac{\partial\phi_J}{\partial t}}=\dot{\symbf{R}}\cdot\Braket{\phi_I\vert\frac{\partial\phi_J}{\partial\symbf{R}}}=\symbf{v}\cdot\symbf{d}_{IJ},
\end{equation}
where $\symbf{v}=\dot{\symbf{R}}$ is the nuclear velocity and $\symbf{d}_{IJ}$ is the \acrfull{nacv}. Although many quantum‐chemistry packages provide \acrshort{nacv} and thus allow computation of the \acrshort{tdc} via $\sigma_{IJ}=\symbf{v}\cdot\symbf{d}_{IJ}$, \acrshort{nacv} can become ill-defined or numerically unstable near conical intersections (e.g., in multi-reference methods). In cases where a reliable \acrshort{nacv} is not available, one may instead employ a direct \acrshort{tdc} approximation using wavefunction overlaps.
\section{The Time Derivative Coupling}\label{sec:time_derivative_coupling}
\acrshort{tdc} is a pivotal quantity in \acrshort{tsh} simulations because it enters the electronic equations of motion given in Eq.~\eqref{eq:tsh_eom}. In \textit{ab initio} molecular dynamics, the \acrshort{tdc} is commonly evaluated using the full \acrshort{nacv} as $\sigma_{IJ}=\symbf{v}\cdot\symbf{d}_{IJ}$, where $\symbf{v}$ is the nuclear velocity. When the dynamics evolves on analytic \acrshort{pes} or the \acrshort{nacv} is for any reason not available, it is often preferable to calculate the \acrshort{tdc} directly. The most straightforward strategy applies a finite-difference formula to the adiabatic wavefunctions
\begin{equation}\label{eq:tdc_fd1}
\sigma_{IJ}(t)=\frac{1}{\Delta t}\left(\Braket{\phi_I(t)\vert\phi_J(t+\Delta t)}-\Braket{\phi_I(t)\vert\phi_J(t)}\right)=\frac{1}{\Delta t}\left(\Braket{\phi_I(t)\vert\phi_J(t+\Delta t)}-\delta_{IJ}\right),
\end{equation}
with $\Delta t$ a small time step. When analytic \acrshort{pes} is available, the finite-difference estimate of the \acrshort{tdc} is straightforward to implement. In \textit{ab initio} molecular dynamics, however, it is usually more practical to recover this quantity from the non-adiabatic coupling vector, which most electronic-structure packages provide natively. The naïve first-order finite-difference formula suffers from well-known deficiencies: its truncation error scales linearly with the time step and it is highly sensitive to numerical noise. A simple remedy is to adopt the second-order approximation
\begin{equation}\label{eq:tdc_fd2}
\sigma_{IJ}(t)=\frac{1}{2\Delta t}\left(\Braket{\phi_I(t)\vert\phi_J(t+\Delta t)}-\Braket{\phi_I(t)\vert\phi_J(t-\Delta t)}\right).
\end{equation}
Although nominally second-order accurate, the approximation still retains significant shortcomings. In particular, it still violates the fundamental antisymmetry requirement, $\sigma_{IJ}^\ast\neq-\sigma_{JI}$, undermining the Hermiticity of the electronic Hamiltonian. The following sections tackle these deficiencies and introduce more accurate, numerically robust strategies for evaluating the \acrshort{tdc}.
\subsection{The Hammes-Schiffer Tully Scheme}\label{sec:hammes_schiffer_tully_scheme}
The \acrfull{hst}\supercite{10.1063/1.467455} scheme remedies the antisymmetry defect of Eq.~\eqref{eq:tdc_fd1} and Eq.~\eqref{eq:tdc_fd2}, making the electronic Hamiltonian Hermitian. To that end we first introduce a midpoint approximation for each adiabatic eigenstate $\phi_I$, obtained by linear interpolation between its values at the bracketing time slices
\begin{equation}\label{eq:hst_wfn}
\phi_I\left(t+\frac{\Delta t}{2}\right)=\frac{1}{2}\left(\phi_I(t+\Delta t)+\phi_I(t)\right).
\end{equation}
This midpoint state is not strictly normalized, a fully norm-preserving variant will be introduced in the next section. The time derivative at the same midpoint is obtained by combining a backward difference for $\phi_I(t+\Delta t)$ with a forward difference for $\phi_I(t)$, yielding
\begin{equation}\label{eq:hst_dwfn}
\frac{\partial \phi_I}{\partial t}\left(t+\frac{\Delta t}{2}\right)=\frac{1}{\Delta t}\left(\phi_I(t+\Delta t)-\phi_I(t)\right).
\end{equation}
The \acrshort{tdc} $\sigma_{IJ}=\Braket{\phi_I\vert\frac{\partial \phi_J}{\partial t}}$ at a time $t+\frac{\Delta t}{2}$ is then formed as an overlap beween the expressions in Eq.~\eqref{eq:hst_wfn} and Eq.~\eqref{eq:hst_dwfn}:
\begin{equation}\label{eq:hst_tdc}
\sigma_{IJ}\left(t+\frac{\Delta t}{2}\right)=\frac{1}{2\Delta t}\left(\Braket{\phi_I(t)\vert\phi_J(t+\Delta t)}-\Braket{\phi_I(t+\Delta t)\vert\phi_J(t)}\right).
\end{equation}
The \acrshort{hst} scheme in Eq.~\eqref{eq:hst_tdc} is a finite-difference approximation of the \acrshort{tdc}, which is also antisymmetric, i.e., $\sigma_{IJ}^\ast=-\sigma_{JI}$. The scheme is numerically stable, but it does not preserve the norm of the adiabatic wavefunction in between the timesteps. Any rapidly varying wavefunction will suffer from a significant normalization error, which can lead to unphysical results. Such a case occurs for two diabatic surfaces coupled by zero diabatic interaction: in the adiabatic basis the exact \acrshort{tdc} diverges, yet the \acrshort{hst} formula collapses to zero, predicting no population transfer. To address this issue, we can apply a \acrfull{npi} to the adiabatic wavefunction, which is discussed in the next section.
\subsection{The Norm-Preserving Interpolation}\label{sec:norm_preserving_interpolation}
\subsection{The Baeck--An Scheme}\label{sec:baeck_an}
\section{Ehrenfest Dynamics}\label{sec:ehrenfest_dynamics}
\section{Fewest Switches Surface Hopping}\label{sec:fewest_switches}
\section{Landau--Zener Surface Hopping}\label{sec:landau_zener}
\section{Mapping Approach to Surface Hopping}\label{sec:mapping_approach}
\section{Beyond Surface Hopping}\label{sec:beyond_surface_hopping}
\printglossary[type=\acronymtype]
\printbibliography
\end{document}
