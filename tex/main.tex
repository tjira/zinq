% Compile with the "lualatex main && biber main && makeglossaries main && lualatex main && lualatex main" command.

\DocumentMetadata{
    lang = en,
    pdfversion = 1.7,
    pdfstandard = A-3b,
}

\documentclass[headsepline=true,parskip=half,open=any,12pt]{scrbook}\title{Algorithms of Quantum Chemistry}\author{Tomáš \textsc{Jíra}}

\usepackage[hidelinks]{hyperref}\makeatletter\hypersetup{pdfauthor={\@author},pdftitle={\@title}}\makeatother % hyperlinks and metadata

\usepackage{amsmath} % all the math environments and symbols
\usepackage{amssymb} % additional math symbols
\usepackage[toc,page]{appendix} % appendices
\usepackage[backend=biber,style=chem-acs]{biblatex} % bibliography package
\usepackage{fontspec} % font selection
\usepackage{braket} % braket notation
\usepackage[format=plain,labelfont=bf]{caption} % bold captions
\usepackage[left=1.5cm,top=2cm,right=1.5cm,bottom=2cm]{geometry} % page layout
\usepackage[acronym,nogroupskip,nomain,toc]{glossaries} % glossary package
\usepackage{mathrsfs} % mathscr environment
\usepackage[automark]{scrlayer-scrpage} % page styles
\usepackage{subcaption} % subfigures
\usepackage{unicode-math} % unicode math support
\usepackage{microtype} % better typesetting

\addbibresource{library.bib} % set the library file
\clearpairofpagestyles % clear the default page styles
\ihead{\pagemark} % add page number to the inner header
\makeglossaries % make the glossary
\ohead{\headmark} % add chapter name to the outer header
\setmainfont{Libertinus Serif} % set the main font
\setmathfont{Libertinus Math} % set the math font
\setsansfont{Libertinus Sans} % set the sans-serif font

\newtheorem{definition}{Definition}\numberwithin{definition}{section}

\newacronym{rhf}{RHF}{Restricted Hartree--Fock}
\newacronym{post-hf}{post-HF}{post-Hartree--Fock}
\newacronym{hf}{HF}{Hartree--Fock}
\newacronym{mp2}{MP2}{Møller--Plesset Perturbation Theory of 2nd Order}
\newacronym{mp3}{MP3}{Møller--Plesset Perturbation Theory of 3rd Order}
\newacronym{mppt}{MPPT}{Møller--Plesset Perturbation Theory}
\newacronym{cisdt}{CISDT}{Configuration Interaction Singles, Doubles and Triples}
\newacronym{cisd}{CISD}{Configuration Interaction Singles and Doubles}
\newacronym{fci}{FCI}{Full Configuration Interaction}
\newacronym{ci}{CI}{Configuration Interaction}
\newacronym{ccsd}{CCSD}{Coupled Cluster Singles and Doubles}
\newacronym{ccd}{CCD}{Coupled Cluster Doubles}
\newacronym{cc}{CC}{Coupled Cluster}
\newacronym{tdse}{TDSE}{Time-Dependent Schrödinger Equation}
\newacronym{tise}{TISE}{Time-Independent Schrödinger Equation}
\newacronym{diis}{DIIS}{Direct Inversion in the Iterative Subspace}
\newacronym{ms}{MS}{Molecular Spinorbital}
\newacronym{fft}{FFT}{Fast Fourier Transform}
\newacronym{ift}{IFT}{Inverse Fourier Transform}
\newacronym{dft}{DFT}{Discrete Fourier Transform}
\newacronym{ft}{FT}{Fourier Transform}
\newacronym{scf}{SCF}{Self-Consistent Field}
\newacronym{tdc}{TDC}{Time Derivative Coupling}
\newacronym{nacv}{NACV}{Nonadiabatic Coupling Vector}
\newacronym{tsh}{TSH}{Trajectory Surface Hopping}
\newacronym{boa}{BOA}{Born--Oppenheimer Approximation}
\newacronym{pes}{PES}{Potential Energy Surface}
\newacronym{hst}{HST}{Hammes-Schiffer Tully}
\newacronym{npi}{NPI}{Norm-Preserving Interpolation}

\begin{document}
\makeatletter\begin{titlepage}
    \center
    \textsc{\LARGE University of Chemistry and Technology, Prague}\\[1.5cm]
    \rule{\linewidth}{0.5mm}\\[0.4cm]
    {\huge\bfseries\@title}\\[0.4cm]
    \rule{\linewidth}{0.5mm}\\[1.5cm]
    {\large\textit{Author}}\\\@author
    \vfill\vfill\vfill
    {\large\today}
    \vfill
\end{titlepage}\makeatother
\tableofcontents
\chapter{Mathematical Background}\label{sec:mathematical_background}
\section{Vector Spaces and Linear Operators}\label{sec:vector_spaces_and_linear_operators}
Many concepts in quantum mechanics rest on the algebra of vector spaces and the action of linear operators. We review what is needed here. Fuller accounts can be found in standard linear algebra and functional analysis texts. Throughout, the scalar field is a general field $F$, but in quantum mechanics we take $F=\mathbb{C}$ unless stated otherwise.
\subsection{Vector Spaces and Basic Properties}\label{sec:vector_spaces_basic_properties}
The object of interest is a vector space: a set with addition and scalar multiplication satisfying compatibility rules. The following definition states these axioms explicitly.
\begin{definition}
A vector space $V$ over a field $F$ is a non-empty set equipped with vector addition and scalar multiplication, which must satisfy the following axioms for all $\symbf{u},\symbf{v},\symbf{w}\in V$ and all scalars $a,b\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Associativity of vector addition: $(\symbf{u}+\symbf{v})+\symbf{w}=\symbf{u}+(\symbf{v}+\symbf{w})$
\item Commutativity of vector addition: $\symbf{u}+\symbf{v}=\symbf{v}+\symbf{u}$
\item Identity element of vector addition: $\exists\symbf{0}\in V$ such that $\symbf{u}+\symbf{0}=\symbf{u}$
\item Inverse element of vector addition: $\exists(-\symbf{u})\in V$ such that $\symbf{u}+(-\symbf{u})=\symbf{0}$
\item Compatibility of scalar multiplication with field multiplication: $a(b\symbf{u})=(ab)\symbf{u}$
\item Identity element of scalar multiplication: $1\symbf{u}=\symbf{u}$, where $1$ is the multiplicative identity in $F$
\item Distributivity of scalar multiplication with respect to vector addition: $a(\symbf{u}+\symbf{v})=a\symbf{u}+a\symbf{v}$
\item Distributivity of scalar multiplication with respect to field addition: $(a+b)\symbf{u}=a\symbf{u}+b\symbf{u}$
\end{enumerate}
\end{definition}
These axioms are necessary for a set to be considered a vector space. Examples of vector spaces include the Euclidean space $\mathbb{R}^n$, the set of all functions with continuous $m$-th derivatives $C^m(\mathbb{R}^n)$ or $C^\infty(\mathbb{R}^n)$, which is the set of all infinitely differentiable functions. In quantum mechanics we usually work in Hilbert spaces, which will be described in more detail in the next section.

Before we can use vector spaces effectively, we must identify and characterize smaller structures that inherit their properties. These are known as subspaces.
\begin{definition}
A subset $W\subset V$ is a subspace if it is non-empty and closed under addition and scalar multiplication. For all $\symbf{u},\symbf{v}\in W$ and $a\in F$, one has $\symbf{u}+\symbf{v}\in W$ and $a\symbf{u}\in W$. Equivalently, $W$ is a subspace iff $\symbf{0}\in W$, $\symbf{u}+\symbf{v}\in W$ and $a\symbf{u}\in W$.
\end{definition}
Every subspace necessarily contains the zero vector and is closed under both addition and scalar multiplication. Subspaces are important because they capture smaller, self-contained portions of a larger space where similar operations remain valid.

We now proceed to the concept of linear combinations, which provides a mechanism for generating new vectors from existing ones, and the related concept of the span, which describes the set of all such combinations.
\begin{definition}
Given a vector space $V$ over a field $F$, a linear combination of vectors $\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\in V$ is a vector
\begin{equation}
\symbf{u}=a_1\symbf{v}_1+a_2\symbf{v}_2+\ldots+a_n\symbf{v}_n,
\end{equation}
where $a_1,a_2,\ldots,a_n\in F$. The set of all linear combinations of ${\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n}$ is called the span of these vectors
\begin{equation}
\text{span}\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace=\left\lbrace\sum_{i=1}^n a_i\symbf{v}_i\middle\vert a_i\in F\right\rbrace.
\end{equation}
The span of a set of vectors is the smallest subspace of $V$ that contains all the vectors in the set.
\end{definition}
The span formalizes the idea of building new vectors from known ones. In finite-dimensional spaces, the span of a finite set can fill the entire space (as in the case of a basis) or a lower-dimensional subspace.

To understand how vectors relate to one another within a span, we must determine whether some vectors can be expressed as combinations of others. This leads us to the notion of linear independence.
\begin{definition}
A set of vectors $\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace$ in a vector space $V$ over a field $F$ is said to be linearly independent if the only solution to the equation
\begin{equation}
a_1\symbf{v}_1+a_2\symbf{v}_2+\ldots+a_n\symbf{v}_n=\symbf{0}
\end{equation}
is $a_1=a_2=\ldots=a_n=0$, where $a_1,a_2,\ldots,a_n\in F$. If there exists a non-trivial solution (i.e., not all $a_i$ are zero), then the set is said to be linearly dependent.
\end{definition}
Linear independence ensures that no vector in the set can be constructed from others. This property is essential when defining a minimal generating set for the entire space. For linear operators (defined later), eigenvectors associated with distinct eigenvalues are linearly independent. No orthogonality is assumed here.

Finally, the concepts of basis and dimension summarize the structure of a vector space in a concise form. A basis provides coordinates for states and a matrix representation for operators.
\begin{definition}
A basis of a vector space $V$ over a field $F$ is a set of vectors $\lbrace\symbf{v}_1,\symbf{v}_2,\ldots,\symbf{v}_n\rbrace$ in $V$ that is linearly independent and spans $V$. The dimension of $V$, denoted as $\text{dim}(V)$, is the number of vectors in any basis of $V$.
\end{definition}
Every vector in a vector space can be written uniquely as a linear combination of basis vectors. The dimension quantifies the minimal number of coordinates required to specify any element of the space, which is a fundamental concept underlying state representation in quantum mechanics.
\subsection{Normed, Inner Product and Complete Vector Space}\label{sec:normed_inner_product_complete_vector_space}
To introduce geometric concepts into vector spaces, we first define the normed vector space.
\begin{definition}
A normed vector space $(V,\lvert\cdot\rvert)$ is a vector space $V$ over a field $F$ (here, $F$ is $\mathbb{R}$ or $\mathbb{C}$) equipped with a norm, which is a function $\lvert\cdot\rvert:V\to\mathbb{R}$ that satisfies the following properties for all $\symbf{u},\symbf{v}\in V$ and all scalars $a\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Non-negativity: $\lvert\symbf{u}\rvert\geq 0$, with equality if and only if $\symbf{u}=\symbf{0}$
\item Absolute scalability: $\lvert a\symbf{u}\rvert=\lvert a\rvert\lvert\symbf{u}\rvert$
\item Triangle inequality: $\lvert\symbf{u}+\symbf{v}\rvert\leq\lvert\symbf{u}\rvert+\lvert\symbf{v}\rvert$
\item Positive-definiteness: $\lvert\symbf{u}\rvert=0$ if and only if $\symbf{u}=\symbf{0}$
\end{enumerate}
\end{definition}
A norm provides a measure of the "length" or "magnitude" of vectors in the space, allowing us to discuss concepts such as convergence and continuity. The norm induces a metric (or distance function) on the vector space defined briefly as
\begin{equation}
d(\symbf{u},\symbf{v})=\lvert\symbf{u}-\symbf{v}\rvert,
\end{equation}
which satisfies all the properties of a metric, thereby enabling the study of geometric properties within the vector space. More specialized normed vector space emerges after we define an inner product.

The inner product generalizes the familiar Euclidean dot product to abstract vector spaces.
\begin{definition}
An inner product on a vector space $V$ over a field $F$ is a function $\langle\cdot,\cdot\rangle:V\times V\to F$ that satisfies the following properties for all $\symbf{u},\symbf{v},\symbf{w}\in V$ and all scalars $a\in F$:
\begin{enumerate}\def\labelenumi{\arabic{enumi}.}
\item Conjugate symmetry: $\langle\symbf{u},\symbf{v}\rangle=\overline{\langle\symbf{v},\symbf{u}\rangle}$
\item Linearity in the first argument: $\langle a\symbf{u}+\symbf{v},\symbf{w}\rangle=a\langle\symbf{u},\symbf{w}\rangle+\langle\symbf{v},\symbf{w}\rangle$
\item Positive-definiteness: $\langle\symbf{u},\symbf{u}\rangle\geq 0$, with equality if and only if $\symbf{u}=\symbf{0}$
\end{enumerate}
\end{definition}
For real vector spaces, the conjugate symmetry reduces to ordinary symmetry. The inner product induces a norm (or length) of a vector $\symbf{u}$ defined by
\begin{equation}
\lvert\symbf{u}\rvert=\sqrt{\langle\symbf{u},\symbf{u}\rangle}.
\end{equation}
We can also generalize the concept of perpendicularity to abstract vector spaces using the inner product. Two vectors $\symbf{u}$ and $\symbf{v}$ are said to be orthogonal if their inner product is zero, i.e., $\langle\symbf{u},\symbf{v}\rangle=0$. If their lengths are both one, they are called orthonormal.

The inner product gives rise to two key inequalities that determine much of the geometric structure of the space. The Cauchy-Schwarz inequality states that for any vectors $\symbf{u},\symbf{v}\in V$,
\begin{equation}
\lvert\langle\symbf{u},\symbf{v}\rangle\rvert\leq\lvert\symbf{u}\rvert\lvert\symbf{v}\rvert.
\end{equation}
This inequality ensures that the angle between two vectors is well-defined and implies the triangle inequality for the induced norm. A vector space with an inner product is called an inner product space. Together, these results ensure that the norm induced by an inner product satisfies all metric axioms, endowing the space with a well-defined notion of distance. Thus, every inner product space is a normed vector space, although the converse is not generally true.

Before we get to linear operators, we add one final definition that will become important when we discuss infinite-dimensional spaces.
\begin{definition}
Let $(V,\lvert\cdot\rvert)$ be a normed vector space. A sequence $\lbrace\symbf{v}_n\rbrace$ in $V$ is called a Cauchy sequence if for every $\epsilon>0$, there exists an integer $N$ such that $\lvert\symbf{v}_m-\symbf{v}_n\rvert<\epsilon$ for all $m,n>N$. The space $V$ is said to be complete if every Cauchy sequence in $V$ converges to a limit that is also in $V$. A complete normed vector space is called a Banach space.
\end{definition}
\subsection{Linear Operators}\label{sec:linear_operators}
Linear operators lie at the core of quantum mechanics, representing physical observables and the evolution of quantum states. We now define linear operators and explore their properties.
\begin{definition}
A linear operator on avector space $V$ over a field $F$ is a map $A:V\to V$ with $A(\symbf{u}+\symbf{v})=A\symbf{u}+A\symbf{v}$ and $A(a\symbf{u})=aA\symbf{u}$ for all $\symbf{u},\symbf{v}\in V$, $a\in F$.
\end{definition}
Using a basis of a vector space, we can represent linear operators as matrices. If $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ is a basis of an $n$-dimensional vector space $V$, an operator $A$ can be represented by a matrix $[A]_{ij}=a_{ij}$ with the coefficients defined by
\begin{equation}
A\symbf{e}_j=\sum_{i=1}^n a_{ij}\symbf{e}_i.
\end{equation}
The action of $A$ on any vector $\symbf{v}=\sum_{j=1}^n v_j\symbf{e}_j$ can then be computed as
\begin{equation}
A\symbf{v}=\sum_{i=1}^n\left(\sum_{j=1}^n a_{ij}v_j\right)\symbf{e}_i.
\end{equation}
If the vector space $V$ is equipped with an inner product and the basis $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ is orthonormal, the matrix elements can also be expressed as
\begin{equation}
a_{ij}=\langle\symbf{e}_i,A\symbf{e}_j\rangle.
\end{equation}
Note that the matrix representation of an operator depends on the choice of basis. If $U$ is a change of basis matrix from a new basis $\lbrace\symbf{f}_i\rbrace_{i=1}^n$ to the old basis $\lbrace\symbf{e}_i\rbrace_{i=1}^n$ (i.e., $\symbf{e}_i=\sum_{j=1}^n U_{ji}\symbf{f}_j$), the matrix representation of $A$ in the new basis is given by
\begin{equation}
[A]_{\symbf{e}}=U^{-1}[A]_{\symbf{f}}U.
\end{equation}
Let's now define some additional properties of linear operators.
\begin{definition}
For $A:V\to V$, the kernel (or null space) of $A$ is the set of vectors $\symbf{v}\in V$ such that $A\symbf{v}=\symbf{0}$. The image (or range) of $A$ is the set of vectors $\symbf{w}\in V$ such that $\symbf{w}=A\symbf{v}$ for some $\symbf{v}\in V$. If $\text{dim}(V)$ is finite, the rank-nullity theorem states that $\text{dim}(\text{ker}(A))+\text{dim}(\text{im}(A))=\text{dim}(V)$.
\end{definition}
Another important concept is the concept of eigenvalues and eigenvectors.
\begin{definition}
A scalar $\lambda\in F$ is called an eigenvalue of a linear operator $A:V\to V$ if there exists a non-zero vector $\symbf{v}\in V$ such that $A\symbf{v}=\lambda\symbf{v}$. The vector $\symbf{v}$ is called an eigenvector associated with the eigenvalue $\lambda$. For a finite-dimensional vector space, the set of all eigenvalues of $A$ is called the spectrum of $A$. The subspace $E_\lambda=\text{ker}(A-\lambda I)$ is called the eigenspace associated with the eigenvalue $\lambda$, where $I$ is the identity operator on $V$.
\end{definition}
If $V$ is finite-dimensional, the eigenvalues of $A$ can be found by solving the characteristic polynomial equation $\text{det}(A-\lambda I)=0$. The eigenvalues may be real or complex, depending on the operator and the field $F$.
\section{Hilbert Spaces and Quantum States}\label{sec:hilbert_spaces}
Hilbert space is a fundamental concept in quantum mechanics, providing the mathematical framework for describing quantum states and their evolution. In this section, we will explore the definition of Hilbert spaces, their properties, and how they relate to quantum states. We will start with the definition.
\begin{definition}
A Hilbert space $\mathcal{H}$ is a complete inner product space, which means it is a vector space equipped with an inner product that allows for the definition of length and angle, and it is complete with respect to the norm induced by the inner product.
\end{definition}
Some examples of Hilbert spaces include the finite-dimensional vector spaces $\mathbb{R}^n$ or $\mathbb{C}^n$ or, as is often the case in quantum mechanics, the space of all square-integrable functions $L^2(\mathbb{R}^n)$. Formally
\begin{equation}
L^2(\mathbb{R}^n)=\left\lbrace\psi :\mathbb{R}^n\to\mathbb{C}\middle\vert\int_{\mathbb{R}^n}\psi^*(\symbf{r})\psi(\symbf{r})\mathrm{d}\symbf{r}<\infty\right\rbrace
\end{equation}
where $\psi^*(x)$ is the complex conjugate of $\psi(x)$. When dealing with quantum states, we often use the Dirac notation, where quantum states are represented as ket vectors $\ket{\psi}$. The inner product between two elements $\ket{\psi}$ and $\ket{\phi}$ in the Hilbert space $L^2(\mathbb{R}^n)$ is defined as
\begin{equation}
\braket{\psi\vert\phi}=\int_{\mathbb{R}^n}\psi^*(\symbf{r})\phi(\symbf{r})\,\mathrm{d}\symbf{r}.
\end{equation}
\section{Spectral Theory in Hilbert Spaces}\label{sec:spectral_theory}
\section{Adiabatic vs Diabatic Representation}\label{sec:adiabatic_vs_diabatic}
\chapter{Electronic Structure Methods}\label{sec:electronic_structure_methods}
This part provides an educational exploration into the computational techniques fundamental to understanding molecular electronic structure in quantum chemistry. Beginning with the \acrfull{hf} method, the text introduces this foundational approach for determining molecular orbitals and electronic energies by approximating the interactions of electrons through a mean-field approximation. The HF method forms the basis for subsequent methods and is presented with a practical coding exercise that guides readers in implementing and calculating \acrshort{hf} energies in Python.

Moving beyond HF, the text delves into \acrfull{mppt}, which improves \acrshort{hf} predictions by introducing corrections for electron correlation through a perturbative approach. This section includes exercises on calculating second- and third-order corrections, allowing readers to enhance their understanding of how electron interactions can be more accurately incorporated. \acrfull{ci} theory is then presented as an approach for representing the molecular wavefunction as a combination of electron configurations. Here, readers learn the theoretical basis of \acrshort{ci} and engage with practical examples focused on constructing the \acrshort{ci} Hamiltonian matrix and solving for molecular energies, particularly emphasizing \acrfull{fci} for high accuracy in small systems.

The part culminates with a discussion on the \acrfull{cc} theory, a highly accurate and computationally efficient method for capturing electron correlation effects, often used for small to medium-sized systems. By introducing truncations such as \acrfull{ccd} and \acrfull{ccsd}, the text demonstrates how electron correlation can be systematically included while balancing computational cost. The \acrshort{cc} section provides iterative coding exercises for calculating correlation energies, rounding out the document's comprehensive approach to electronic structure methods in computational quantum chemistry. Through this blend of theory, mathematical formulations, and hands-on coding exercises, the document serves as an invaluable resource for building a strong foundational understanding of electronic structure methods.
\section{Hartree--Fock Method}\label{sec:hartree_fock_method}
The \acrshort{hf} method is a foundational approach in quantum chemistry, aimed at solving the electronic structure problem in molecules by determining an optimal wavefunction. This method simplifies the complex interactions of electrons through a mean-field approximation, where each electron moves in an average field created by all others. This allows for the use of a single set of orbitals, leading to the construction of the Fock operator and iterative solutions to one-electron equations.

However, \acrshort{hf} has notable limitations. Its reliance on a single-determinant wavefunction means it struggles to account for electron correlation. This leads to inaccuracies in energy predictions, particularly for systems with strong electron interactions, such as transition metal complexes or molecules with delocalized electrons.
\subsection{Theoretical Background}\label{sec:hartree_fock_theoretical_background}
Our primary objective is to solve the Schrödinger equation in the form
\begin{equation}
\hat{\symbf{H}}\ket{\Psi}=E\ket{\Psi}
\end{equation}
where $\hat{\symbf{H}}$ denotes the molecular Hamiltonian operator, $\ket{\Psi}$ represents the molecular wavefunction, and $E$ is the total energy of the system. The \acrshort{hf} approximates the total wavefunction $\ket{\Psi}$ as a single Slater determinant, expressed as
\begin{equation}
\ket{\Psi}=\ket{\chi_1\chi_2\cdots\chi_N}
\end{equation}
where $\chi_i$ denotes a spin orbital, and $N$ is the total number of electrons. The goal of the \acrshort{hf} method is to optimize these orbitals in order to minimize the system's total energy, thereby providing a reliable estimate of the electronic structure.

In the \acrfull{rhf} method, we impose a constraint on electron spin, which allows us to work with spatial orbitals instead of spin orbitals. This reformulation expresses the Slater determinant in terms of spatial orbitals as
\begin{equation}
\ket{\Psi}=\ket{\Phi_1\Phi_2\cdots\Phi_{N/2}}
\end{equation}
where $\Phi_i$ represents a spatial orbital. Notably, the \acrshort{rhf} method requires an even number of electrons to satisfy spin-pairing. In practice, atomic orbitals (whether spin or spatial) are typically expanded in a set of basis functions $\left\lbrace\phi_i\right\rbrace$, which are often Gaussian functions, allowing for convenient computation with expansion coefficients. After performing some algebraic manipulations, the \acrshort{hf} method leads to the Roothaan equations, which are expressed as
\begin{equation}\label{eq:roothaan}
\symbf{FC}=\symbf{SC}\symbf{\varepsilon}
\end{equation}
where $\symbf{F}$ is the Fock matrix, $\symbf{C}$ is the matrix of orbital coefficients, $\symbf{S}$ is the overlap matrix, and $\symbf{\varepsilon}$ represents the orbital energies. These matrices will be defined in detail later.
\subsection{Implementation of the Restricted Hartree--Fock Method}\label{sec:implementation_of_rhf_method}
To begin, we define the core Hamiltonian, also known as the one-electron Hamiltonian. This component of the full Hamiltonian excludes electron-electron repulsion and is expressed in index notation as
\begin{equation}\label{eq:hamiltonian}
H_{\mu\nu}^{\symrm{core}}=T_{\mu\nu}+V_{\mu\nu}
\end{equation}
where $\mu$ and $\nu$ are indices of the basis functions. Here, $T_{\mu\nu}$ represents a kinetic energy matrix element, while $V_{\mu\nu}$ denotes a potential energy matrix element. These matrix elements are defined as
\begin{align}
T_{\mu\nu}&=\braket{\phi_{\mu}\vert\hat{T}\vert\phi_{\nu}} \\
V_{\mu\nu}&=\braket{\phi_{\mu}\vert\hat{V}\vert\phi_{\nu}}
\end{align}
Additionally, the overlap integrals, which describe the extent of overlap between basis functions, are given by
\begin{equation}\label{eq:overlap}
S_{\mu\nu}=\braket{\phi_{\mu}\vert\phi_{\nu}}
\end{equation}
and another essential component are the two-electron repulsion integrals, defined as
\begin{equation}\label{eq:coulomb}
J_{\mu\nu\kappa\lambda}=\braket{\phi_{\mu}\phi_{\mu}\vert\hat{J}\vert\phi_{\kappa}\phi_{\lambda}}
\end{equation}
which play crucial roles in the \acrshort{hf} calculation. All of these integrals over (Gausssian) basis functions are usually calculated using analytical expressions.\supercite{10.1016/S0065-3276!08!60019-2} The solution to the Roothaan equations~\eqref{eq:roothaan} requires an iterative procedure, since the Fock matrix defined as
\begin{equation}\label{eq:fock}
F_{\mu\nu}=H_{\mu\nu}^{\symrm{core}}+D_{\kappa\lambda}\left(J_{\mu\nu\kappa\lambda}-\frac{1}{2}J_{\mu\lambda\kappa\nu}\right)
\end{equation}
depends on the unknown density matrix $\symbf{D}$, defined as
\begin{equation}\label{eq:hf_density}
D_{\mu\nu}=2C_{\mu i}C_{\nu i}
\end{equation}
This iterative procedure is executed through the \acrfull{scf} method. An initial guess is made for the density matrix $\symbf{D}$ (usually zero), the Roothaan equations~\eqref{eq:roothaan} are solved and the density matix is updated using the equation~\eqref{eq:hf_density}. The total energy of the system is then calculated using the core Hamiltonian and the Fock matrix as
\begin{equation}
E=\frac{1}{2}D_{\mu\nu}(H_{\mu\nu}^{\symrm{core}}+F_{\mu\nu})+E_{\symrm{nuc}}
\end{equation}
After convergence of both the density matrix and total energy, the process concludes, yielding the optimized molecular orbitals. The total energy of the system also includes the nuclear repulsion energy, which is given by
\begin{equation}
E_{\symrm{nuc}}=\sum_{A}\sum_{B<A}\frac{Z_{A}Z_{B}}{R_{AB}}
\end{equation}
where $Z_A$ is the nuclear charge of atom $A$, and $R_{AB}$ is the distance between atoms $A$ and $B$. The Roothaan equations~\eqref{eq:roothaan} are a generalized eigenvalue problem, which can be transformed into a standard eigenvalue problem as explained in Section \ref{sec:generalized_eigenvalue_problem}.
\subsubsection{Direct Inversion in the Iterative Subspace}\label{sec:diis}
In the \acrshort{hf} method, convergence of the density matrix and energy can be significantly accelerated by employing the \acrfull{diis} technique. \acrshort{diis} achieves this by storing Fock matrices from previous iterations and constructing an optimized linear combination that minimizes the current iteration's error. This approach is especially valuable in \acrshort{hf} calculations for large systems, where convergence issues are more common and challenging to resolve.

We start by defining the error vector of $i$-th iteration $\symbf{e}_i$ as
\begin{equation}
\symbf{e}_i=\symbf{S}_i\symbf{D}_i\symbf{F}_i-\symbf{F}_i\symbf{D}_i\symbf{S}_i
\end{equation}
Our goal is to transform the Fock matrix $\symbf{F}_i$ as
\begin{equation}\label{eq:fock_extrapolate}
\symbf{F}_i=\sum_{j=i-(L+1)}^{i-1}c_j\symbf{F}_j
\end{equation}
where $c_j$ are the coefficients that minimize the error matrix $\symbf{e}_i$ and $L$ is the number of Fock matrices we store called the subspace size. To calculate the coefficients $c_j$, we solve the set linear equations
\begin{equation}
\begin{bmatrix}
\symbf{e}_1\cdot\symbf{e}_1 & \dots & \symbf{e}_1\cdot\symbf{e}_{L+1} & 1 \\
\vdots & \ddots & \vdots & \vdots \\
\symbf{e}_{L+1}\cdot\symbf{e}_1 & \dots & \symbf{e}_{L+1}\cdot\symbf{e}_{L+1} & 1 \\
1 & \dots & 1 & 0 \\
\end{bmatrix}
\begin{bmatrix}
c_1 \\
\vdots \\
c_{L+1} \\
\lambda \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
\vdots \\
0 \\
1 \\
\end{bmatrix}
\end{equation}
After solving the linear equations, we use the coefficients $c_j$ to construct the new Fock matrix $\symbf{F}_i$ according to the equation~\eqref{eq:fock_extrapolate} and proceed as usual with the \acrshort{hf} calculation.
\subsubsection{Gradient of the Restricted Hartree--Fock Method}\label{sec:hf_gradient}
If we perform the calculation as described above and get the density matrix $\symbf{D}$ we can evaluate the nuclear energy gradient as\supercite{10.1002/9780470749593.hrs006}
\begin{equation}\label{eq:hf_gradient}
\frac{\partial E}{\partial X_{A,i}}=\sum_{\mu\nu\in\left\lbrace\phi_A\right\rbrace}D_{\mu\nu}\frac{\partial H_{\mu\nu}^{\symrm{core}}}{\partial X_{A,i}}+2\sum_{\mu\nu\in\left\lbrace\phi_A\right\rbrace}D_{\mu\nu}D_{\kappa\lambda}\frac{\partial\left(J_{\mu\nu\kappa\lambda}-\frac{1}{2}J_{\mu\lambda\kappa\nu}\right)}{\partial X_{A,i}}-2W_{\mu\nu}\frac{\partial S_{\mu\nu}}{\partial X_{A,i}}
\end{equation}
where $A$ is the index of an atom, $i$ is the index of the coordinate, $\left\lbrace\phi_A\right\rbrace$ is the set of all basis functions located at atom $A$ and $\symbf{W}$ is energy weighed density matrix defined as
\begin{equation}
W_{\mu\nu}=2C_{\mu i}C_{\nu i}\varepsilon_i
\end{equation}
Keep in mind that the indices $\kappa$ and $\lambda$ in the gradient equation~\eqref{eq:hf_gradient} are summed over all basis functions.
\subsection{Integral Transforms to the Basis of Molecular Spinorbitals}\label{sec:integral_transform}
To carry out most \acrfull{post-hf} calculations, it is essential to transform the integrals into the \acrfull{ms} basis. We will outline this transformation process here and refer to it in subsequent sections on \acrshort{post-hf} methods. The \acrshort{post-hf} methods in this document will be presented using the integrals in the \acrshort{ms} basis (and in its antisymmetrized form for the two-electron integrals) as this approach is more general.

All the integrals defined in the equations~\eqref{eq:hamiltonian},~\eqref{eq:overlap}, and~\eqref{eq:coulomb}, as well as Fock matrix in the equation~\eqref{eq:fock} are defined in the basis of atomic orbitals. To transform these integrals to the \acrshort{ms} basis, we utilize the coefficient matrix $\symbf{C}$ obtained from the solution of the Roothaan equations~\eqref{eq:roothaan}. This coefficient matrix $\symbf{C}$ is initially calculated in the spatial molecular orbital basis (in the \acrshort{rhf} calculation).

The first step involves expanding the coefficient matrix $\symbf{C}$ to the \acrshort{ms} basis. This transformation can be mathematically expressed using the tiling matrix $\symbf{P}_{n\times 2n}$, defined as
\begin{equation}
\symbf{P}=
\begin{pmatrix}
e_1&e_1&e_2&e_2&\dots&e_n&e_n
\end{pmatrix}
\end{equation}
where $e_i$ represents the $i$-th column of the identity matrix $\symbf{I}_n$. Additionally, we define the matrices $\symbf{M}_{n\times 2n}$ and $\symbf{N}_{n\times 2n}$ with elements given by
\begin{equation}
M_{ij}=1-j\bmod 2,N_{ij}=j \bmod 2
\end{equation}
The coefficient matrix $\symbf{C}$ in the \acrshort{ms} basis can be then expressed as
\begin{equation}
\symbf{C}^{\symrm{MS}}=
\begin{pmatrix}
\symbf{CP} \\
\symbf{CP}
\end{pmatrix}
\odot
\begin{pmatrix}
\symbf{M} \\
\symbf{N}
\end{pmatrix}
\end{equation}
where $\odot$ denotes the Hadamard product. This transformed matrix $\symbf{C}^{\symrm{MS}}$ is subsequently used to transform the two-electron integrals $\symbf{J}$ to the \acrshort{ms} basis as
\begin{equation}
J_{pqrs}^{\symrm{MS}}=C_{\mu p}^{\symrm{MS}}C_{\nu q}^{\symrm{MS}}(\symbf{I}_{2}\otimes_K(\symbf{I}_{2}\otimes_K\symbf{J})^{(4,3,2,1)})_{\mu\nu\kappa\lambda}C_{\kappa r}^{\symrm{MS}}C_{\lambda s}^{\symrm{MS}}
\end{equation}
where the superscript $(4,3,2,1)$ denotes the axes transposition and $\otimes_K$ is the Kronecker product. This notation accommodates the spin modifications and ensures adherence to quantum mechanical principles. We also define the antisymmetrized two-electron integrals in physicists' notation as
\begin{equation}
\braket{pq\vert\vert rs}=(J_{pqrs}^{\symrm{MS}}-J_{psrq}^{\symrm{MS}})^{(1,3,2,4)}
\end{equation}
For the transformation of the one-electron integrals such as the core Hamiltonian, the overlap matrix and also the Fock matrix, we use the formula
\begin{equation}
A_{pq}^{\symrm{MS}}=C_{\mu p}^{\symrm{MS}}(\symbf{I}_{2}\otimes_K\symbf{A})_{\mu\nu}C_{\nu q}^{\symrm{MS}}
\end{equation}
where $\symbf{A}$ is an arbitrary matrix of one-electron integrals. Since many \acrshort{post-hf} methods rely on differences of orbital energies in the denominator, we define the tensors
\begin{align}
\varepsilon^{a}_{i}&=\varepsilon_i-\varepsilon_a \\
\varepsilon^{ab}_{ij}&=\varepsilon_i+\varepsilon_j-\varepsilon_a-\varepsilon_b \\
\varepsilon^{abc}_{ijk}&=\varepsilon_i+\varepsilon_j+\varepsilon_k-\varepsilon_a-\varepsilon_b-\varepsilon_c
\end{align}
for convenience. These tensors enhance code readability and efficiency, making it easier to understand and work with the underlying mathematical framework. Here and also throughout the rest of the document, the indices $i$, $j$ and $k$ run over occupied orbitals, whereas the indices $a$, $b$ and $c$ run over virtual orbitals.
\section{Møller--Plesset Perturbation Theory}\label{sec:moller_plesset_perturbation_theory}
\acrshort{mppt} is a quantum mechanical method used to improve the accuracy of electronic structure calculations within the framework of \acrshort{hf} theory. It involves treating electron-electron correlation effects as a perturbation to the reference \acrshort{hf} wave function. The method is named after its developers, physicists C. Møller and M. S. Plesset. By systematically including higher-order corrections, \acrshort{mppt} provides more accurate predictions of molecular properties compared to the initial \acrshort{hf} approximation.
\subsection{Theory of the Perturbative Approach}\label{sec:mppt_theory}
As for the \acrshort{hf} method, we start with the Schrödinger equation in the form
\begin{equation}
\hat{\symbf{H}}\ket{\Psi}=E\ket{\Psi}
\end{equation}
where $\hat{\symbf{H}}$ is the molecular Hamiltonian operator, $\ket{\Psi}$ is the molecular wave function, and $E$ is the total energy of the system. In the Møller--Plesset perturbation theory we write the Hamiltonian operator as
\begin{equation}
\hat{\symbf{H}}=\hat{\symbf{H}}^{(0)}+\lambda\hat{\symbf{H}}^{'}
\end{equation}
where $\hat{\symbf{H}}^{(0)}$ is the Hamiltonian used in the \acrshort{hf} method (representing electrons moving in the mean field), $\lambda$ is a parameter between 0 and 1, and $\hat{\symbf{H}}^{'}$ is the perturbation operator representing the missing electron-electron interactions not included in the \acrshort{hf} approximation. We then expand the wavefunction $\ket{\Psi}$ and total energy $E$ as a power series in $\lambda$ as
\begin{align}
\ket{\Psi}&=\ket{\Psi^{(0)}}+\lambda\ket{\Psi^{(1)}}+\lambda^2\ket{\Psi^{(2)}}+\dots \\
E&=E^{(0)}+\lambda E^{(1)}+\lambda^2 E^{(2)}+\dots
\end{align}
and ask, how how does the total energy change with the included terms. After some algebra, we can show that the first order correction to the total energy is zero, the second order correction is given by
\begin{equation}
E_{\symrm{corr}}^{\symrm{MP2}}=\sum_{s>0}\frac{H_{0s}^{'}H_{s0}^{'}}{E_0-E_s}
\end{equation}
where $s$ runs over all doubly excited determinants, $H_{0s}^{'}$ is the matrix element of the perturbation operator between the \acrshort{hf} determinant and the doubly excited determinant, and $E_0$ and $E_s$ are the energies of the reference and doubly excited determinants, respectively.\supercite{10.1002/wcms.58,1014569052} We could express all higher-order corrections in a similar way, using only the matrix elements of the perturbation operator and the energies of the determinants. For practical calculations, we apply Slater-Condon rules to evaluate the matrix elements and use the orbital energies obtained from the \acrshort{hf} calculation. The expressions for calculation are summarised below.
\subsection{Implementation of 2nd and 3rd Order Corrections}\label{sec:mppt_implementation}
Having the antisymmetrized two-electron integrals in the \acrshort{ms} basis and physicists' notation defined in Section \ref{sec:integral_transform}, we can now proceed with the calculation of the correlation energy. The 2nd order correlation energy can be expressed as
\begin{equation}
E_{\symrm{corr}}^{\symrm{MP2}}=\frac{1}{4}\sum_{ijab}\frac{\braket{ab\vert\vert ij}\braket{ij\vert\vert ab}}{\varepsilon_{ij}^{ab}}
\end{equation}
and the 3rd order correlation energy as
\begin{align}
E_{\symrm{corr}}^{\symrm{MP3}}=&\frac{1}{8}\sum_{ijab}\frac{\braket{ab\vert\vert ij}\braket{cd\vert\vert ab}\braket{ij\vert\vert cd}}{\varepsilon_{ij}^{ab}\varepsilon_{ij}^{cd}}+\nonumber \\
&+\frac{1}{8}\sum_{ijab}\frac{\braket{ab\vert\vert ij}\braket{ij\vert\vert kl}\braket{kl\vert\vert ab}}{\varepsilon_{ij}^{ab}\varepsilon_{kl}^{ab}}+\nonumber \\
&+\sum_{ijab}\frac{\braket{ab\vert\vert ij}\braket{cj\vert\vert kb}\braket{ik\vert\vert ac}}{\varepsilon_{ij}^{ab}\varepsilon_{ik}^{ac}}
\end{align}
To calculate the 4th order correction, we would need to write 39 terms, which is not practical. Higher-order corrections are usually not programmed this way, instead, the diagrammatic approach is used.\supercite{1014569052,10.1016/0010-4655!73!90016-7,10.1016/0010-4655!73!90017-9}
\section{Configuration Interaction}\label{sec:configuration_interaction}
\acrshort{ci} is a \acrshort{post-hf}, utilizing a linear variational approach to address the nonrelativistic Schrödinger equation under the Born--Oppenheimer approximation for multi-electron quantum systems. \acrshort{ci} mathematically represents the wave function as a linear combination of Slater determinants. The term ``configuration'' refers to different ways electrons can occupy orbitals, while ``interaction'' denotes the mixing of these electronic configurations or states. \acrshort{ci} computations, however, are resource-intensive, requiring significant CPU time and memory, limiting their application to smaller molecular systems. While \acrshort{fci} considers all possible electronic configurations, making it computationally prohibitive for larger systems, truncated versions like \acrfull{cisd} or \acrfull{cisdt} are more feasible and commonly employed in quantum chemistry studies.
\subsection{Theoretical Background of General Configuration Interaction}\label{sec:ci_theoretical_background}
In \acrshort{ci} theory, we expand the wavefunction $\ket{\Psi}$ in terms of the \acrshort{hf} reference determinant and its excited configurations as
\begin{equation}
\ket{\Psi}=c_0\ket{\Psi_0}+\left(\frac{1}{1!}\right)^2c_i^a\ket{\Psi_i^a}+\left(\frac{1}{2!}\right)^2c_{ij}^{ab}\ket{\Psi_{ij}^{ab}}+\left(\frac{1}{3!}\right)^2c_{ijk}^{abc}\ket{\Psi_{ijk}^{abc}}+\dots
\end{equation}
where we seek the coefficients $\symbf{c}$ that minimize the energy. To determine these coefficients, we construct and diagonalize the Hamiltonian matrix in the basis of these excited determinants. The \acrshort{ci} Hamiltonian matrix $\symbf{H}^{\symrm{CI}}$ is represented as
\begin{equation}\label{eq:ci-hamiltonian}
\symbf{H}^{\symrm{CI}}=
\begin{bmatrix}
\braket{\Psi_0\vert\hat{H}\vert\Psi_0} & \braket{\Psi_0\vert\hat{H}\vert\Psi_i^a} & \braket{\Psi_0\vert\hat{H}\vert\Psi_{ij}^{ab}} & \dots \\
\braket{\Psi_i^a\vert\hat{H}\vert\Psi_0} & \braket{\Psi_i^a\vert\hat{H}\vert\Psi_i^a} & \braket{\Psi_i^a\vert\hat{H}\vert\Psi_{ij}^{ab}} & \dots \\
\braket{\Psi_{ia}^{jb}\vert\hat{H}\vert\Psi_0} & \braket{\Psi_{ia}^{jb}\vert\hat{H}\vert\Psi_1} & \braket{\Psi_{ia}^{jb}\vert\hat{H}\vert\Psi_{ia}^{jb}} & \dots \\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\end{equation}
After the Hamiltonian matrix is constructed, we solve the eigenvalue problem
\begin{equation}\label{eq:ci-eigenvalue-problem}
\symbf{H}^{\symrm{CI}}\symbf{C}^{\symrm{CI}}=\symbf{C}^{\symrm{CI}}\symbf{\varepsilon}^{\symrm{CI}}
\end{equation}
where $\symbf{C}^{\symrm{CI}}$ is a matrix of coefficients and $\symbf{\varepsilon}^{\symrm{CI}}$ is a diagonal matrix of eigenvalues. The lowest eigenvalue gives the ground-state energy, and the corresponding eigenvector provides the coefficients that minimize the energy. The elements of the \acrshort{ci} Hamiltonian matrix are computed using the Slater--Condon rules, summarized in one function as
\begin{equation}\label{eq:slater-condon-rules}
\symbf{H}_{ij}^{\symrm{CI}}=
\begin{cases} 
\displaystyle \sum_kH_{kk}^{\symrm{core},\symrm{MS}}+\frac{1}{2}\sum_k\sum_l\braket{kl\vert\vert kl}&D_i=D_j \\
\displaystyle H_{pr}^{\symrm{core},\symrm{MS}}+\sum_k\braket{pk\vert\vert rk}&D_i=\left\lbrace\dotsi p\dotsi\right\rbrace\land D_j=\left\lbrace\dotsi r\dotsi\right\rbrace \\
\displaystyle \vphantom{\sum_k}\braket{pq\vert\vert rs}&D_i=\left\lbrace\dotsi p\dotsi q\dotsi\right\rbrace\land D_j=\left\lbrace\dotsi r\dotsi s\dotsi\right\rbrace \\
\displaystyle \vphantom{\sum_k}0&\text{otherwise}
\end{cases}
\end{equation}
where $D_i$ and $D_j$ are Slater determinants, $\symbf{H}^{\symrm{core},\symrm{MS}}$ is the core Hamiltonian in the \acrshort{ms} basis, and $\braket{pk\vert\vert lk}$ are the antisymmetrized two-electron integrals in \acrshort{ms} basis and physicists' notation. The sums extend over all spinorbitals common between the two determinants. These integrals were previously transformed in Section \ref{sec:integral_transform}. Keep in mind, that to apply the Slater-Condon rules, the determinants must be aligned, and the sign of the matrix elements must be adjusted accordingly, based on the number of permutations needed to align the determinants.

An important caveat in \acrshort{ci} theory is its lack of size-extensivity, which implies that the energy does not scale linearly with the number of electrons. This drawback stems from the fact that the \acrshort{ci} wavefunction is not size-consistent, meaning the energy of a combined system is not simply the sum of the energies of its isolated parts. This limitation restricts the application of \acrshort{ci} mainly to small molecular systems.
\subsection{Full Configuration Interaction Implementation}\label{sec:fci_implementation}
In \acrshort{fci}, we aim to account for all possible electronic configurations within a chosen basis set, offering the most accurate wavefunction representation for the given basis. Although this method yields highly precise electronic structure information, it is computationally intensive. Its cost scales exponentially with both the number of electrons and basis functions, limiting its feasibility to smaller systems.

The \acrshort{fci} process involves constructing all possible Slater determinants for a system. For simplicity, we'll assume that we want to include both singlet and triplet states in our determinant space. The total number of these determinants $N_D$ can be calculated using binomial coefficients
\begin{equation}
N_D=\binom{n}{k}
\end{equation}
where $k$ is the total number of electrons, and $n$ is the total number of spinorbitals. For practical representation, it's useful to describe determinants as arrays of numbers, where each number corresponds to the index of an occupied orbitals. For example, the ground state determinant for a system with 6 electrons can be represented as $\left\lbrace 0,1,2,3,4,5\right\rbrace$, whereas the determinant $\left\lbrace 0,1,2,3,4,6\right\rbrace$ represents an excited state with one electron excited from orbital 5 to orbital 6. Using the determinants, the \acrshort{ci} Hamiltonian matrix~\eqref{eq:ci-hamiltonian} can be constructed, and the eigenvalue problem~\eqref{eq:ci-eigenvalue-problem} can be solved to obtain the ground and excited state energies.
\section{Coupled Cluster Theory}\label{sec:coupled_cluster_theory}
\acrshort{cc} theory is a \acrshort{post-hf} method used in quantum chemistry to achieve highly accurate solutions to the electronic Schrödinger equation, particularly for ground states and certain excited states. It improves upon \acrshort{hf} by incorporating electron correlation effects through a systematic inclusion of excitations (singles, doubles, triples, etc.) from a reference wavefunction, usually the \acrshort{hf} wavefunction. The method uses an exponential ansatz to account for these excitations, leading to a size-consistent and size-extensive approach, making it one of the most accurate methods available for small to medium-sized molecular systems.

Within \acrshort{cc} theory, specific truncations are often applied to manage computational cost. The \acrshort{ccd} method considers only double excitations, capturing electron correlation more effectively than simpler methods like \acrshort{hf}, but at a lower computational expense than higher-level methods. \acrshort{ccsd} extends this approach by including both single and double excitations, offering greater accuracy, particularly for systems where single excitations play a significant role. \acrshort{ccsd} is widely used due to its balance between accuracy and computational feasibility, making it a reliable choice for many chemical systems.
\subsection{Coupled Cluster Formalism}\label{sec:cc_formalism}
In the \acrshort{cc} formalism, we write the total wavefunction in an exponential form as
\begin{equation}
\ket{\Psi}=e^{\hat{\symbf{T}}}\ket{\Psi_0}
\end{equation}
where $\ket{\Psi_0}$ is the reference wavefunction, usually the \acrshort{hf} wavefunction, and $\hat{T}$ is the cluster operator that generates excitations from the reference wavefunction. The cluster operator is defined as
\begin{equation}
\hat{\symbf{T}}=\hat{\symbf{T}}_1+\hat{\symbf{T}}_2+\hat{\symbf{T}}_3+\dots
\end{equation}
where $\hat{\symbf{T}}_1$ generates single excitations, $\hat{\symbf{T}}_2$ generates double excitations, and so on. For example
\begin{equation}
\hat{\symbf{T}}_1\ket{\Psi_0}=\left(\frac{1}{1!}\right)^2t_i^a\ket{\Psi_i^a}
\end{equation}
where $t_i^a$ are the single excitation amplitudes. These amplitudes are just expansion coefficients that determine the contribution of each excitation to the total wavefunction. In the context of configuration interaction, we denoted these coefficients as $c_i^a$. Now that we have the total wavefunction, we want to solve the Schrödinger equation
\begin{equation}
\hat{\symbf{H}}\ket{\Psi}=E\ket{\Psi}
\end{equation}
where $\hat{H}$ is the molecular Hamiltonian operator, $E$ is the total energy of the system, and $\ket{\Psi}$ is the total wavefunction. In the \acrshort{cc} theory, we usually rewrite the Schrödinger equation in the exponential form as
\begin{equation}
e^{-\hat{\symbf{T}}}\hat{\symbf{H}}e^{\hat{\symbf{T}}}\ket{\Psi_0}=E\ket{\Psi_0}
\end{equation}
because we can then express the \acrshort{cc} energy as
\begin{equation}
E=\braket{\Psi_0\vert e^{-\hat{\symbf{T}}}\hat{\symbf{H}}e^{\hat{\symbf{T}}}\vert\Psi_0}
\end{equation}
taking advantage of the exponential form of the wavefunction. We could then proceed to express the total energy for various \acrshort{cc} methods like \acrshort{ccd} and \acrshort{ccsd}, but the equations would be quite lengthy. Instead, we will leave the theory here and proceed to the actual calculations. One thing to keep in mind is that the \acrshort{cc} equations are nonlinear and require iterative solution methods to obtain the final amplitudes.
\subsection{Implementation of Truncated Coupled Cluster Methods}\label{sec:cc_implementation}
We will not go into the details here, but we will provide the final expressions for the \acrshort{ccd} and \acrshort{ccsd} methods.\supercite{10.1063/1.460620} The \acrshort{ccd} and \acrshort{ccsd} methods are the most commonly used \acrshort{cc} methods, and they are often used as benchmarks for other methods. All we need for the evaluation of the expressions below are the two-electron integrals in the \acrshort{ms} basis and physicists' notation, Fock matrix in the \acrshort{ms} basis and the orbital energy tensors obtained from the \acrshort{hf} calculation. All these transformations are already explained in Section \ref{sec:integral_transform}. The expressions for the \acrshort{ccd} can be written as
\begin{equation}
E_{\text{CCD}}=\frac{1}{4}\braket{ij\vert\vert ab}t_{ij}^{ab}
\end{equation}
where the double excitation amplitudes $t_{ij}^{ab}$ are determined by solving the \acrshort{ccd} amplitude equation. The \acrshort{ccd} amplitude equations are given by
\begin{align}
t_{ij}^{ab}=&\braket{ab\vert\vert ij}+\frac{1}{2}\braket{ab\vert\vert cd}t_{cd}^{ij}+\frac{1}{2}\braket{kl\vert\vert ij}t_{ab}^{kl}+\hat{P}_{(a/b)}\hat{P}_{(i/j)}\braket{ak\vert\vert ic}t_{cb}^{ij}-\nonumber \\
&-\frac{1}{2}\hat{P}_{(a/b)}\braket{kl\vert\vert cd}t_{ac}^{ij}t_{bd}^{kl}-\frac{1}{2}\hat{P}_{(i/j)}\braket{kl\vert\vert cd}t_{ab}^{ik}t_{cd}^{jl}+\nonumber \\
&+\frac{1}{4}\braket{kl\vert\vert cd}t_{cd}^{ij}t_{ab}^{kl}+\hat{P}_{(i/j)}\braket{kl\vert\vert cd}t_{ac}^{ik}t_{bd}^{jl}
\end{align}
where $\hat{P}_{(a/b)}$ and $\hat{P}_{(i/j)}$ are permutation operators that ensure the correct antisymmetry of the amplitudes. The \acrshort{ccsd} energy expression is given by
\begin{equation}
E_{\text{CCSD}}=F_{ia}^{\symrm{MS}}t_a^i+\frac{1}{4}\braket{ij\vert\vert ab}t_{ij}^{ab}+\frac{1}{2}\braket{ij\vert\vert ab}t_{i}^{a}t_{b}^{j}
\end{equation}
where the single and double excitation amplitudes $t_a^i$ and $t_{ij}^{ab}$ are determined by solving the \acrshort{ccsd} amplitude equations. To simplify the notation a little bit, we define the the $\mathscr{F}$ and $\mathscr{W}$ intermediates as
\begin{align}
\mathscr{F}_{ae}=&\left(1-\delta_{ae}\right)F_{ae}-\frac{1}{2}F_{me}t_m^a+t_m^f\braket{ma\vert\vert fe}-\frac{1}{2}\tilde{\tau}_{mn}^{af}\braket{mn\vert\vert ef} \\
\mathscr{F}_{mi}=&\left(1-\delta_{mi}\right)F_{mi}+\frac{1}{2}F_{me}t_i^e+t_n^e\braket{mn\vert\vert ie}+\frac{1}{2}\tilde{\tau}_{in}^{ef}\braket{mn\vert\vert ef} \\
\mathscr{F}_{me}=&F_{me}+t_n^f\braket{mn\vert\vert ef} \\
\mathscr{W}_{mnij}=&\braket{mn\vert\vert ij}+\hat{P}_{(i/j)}t_j^e\braket{mn\vert\vert ie}+\frac{1}{4}\tau_{ij}^{ef}\braket{mn\vert\vert ef} \\
\mathscr{W}_{abef}=&\braket{ab\vert\vert ef}-\hat{P}_{(a/b)}t_m^b\braket{am\vert\vert ef}+\frac{1}{4}\tau_{mn}^{ab}\braket{mn\vert\vert ef} \\
\mathscr{W}_{mbej}=&\braket{mb\vert\vert ej}+t_j^f\braket{mb\vert\vert ef}-t_n^b\braket{mn\vert\vert ej}-\left(\frac{1}{2}t_{jn}^{fb}+t_j^ft_n^b\right)\braket{mn\vert\vert ef}
\end{align}
and two-particle excitation operators as
\begin{align}
\tilde{\tau}_{ij}^{ab}=&t_{ij}^{ab}+\frac{1}{2}\left(t_i^at_j^b-t_i^bt_j^a\right) \\
\tau_{ij}^{ab}=&t_{ij}^{ab}+t_i^at_j^b-t_i^bt_j^a
\end{align}
The \acrshort{ccsd} single excitations amplitude equations are then given by
\begin{align}
t_i^a=&F_{ai}^{\symrm{MS}}+t_i^e\mathscr{F}_{ae}-t_m^a\mathscr{F}_{mi}t_{im}^{ae}\mathscr{F}_{me}-t_n^f\braket{na\vert\vert if}-\nonumber-\frac{1}{2}t_{im}^{ef}\braket{ma\vert\vert ef}- \\
&-\frac{1}{2}t_{mn}^{ae}\braket{nm\vert\vert ei}
\end{align}
and the \acrshort{ccsd} double excitations amplitude equations are given by
\begin{align}
t_{ij}^{ab}=&\braket{ab\vert\vert ij}+\hat{P}_{(a/b)}t_{ij}^{ae}\left(\mathscr{F}_{be}-\frac{1}{2}t_m^b\mathscr{F}_{ae}\right)-\hat{P}_{(i/j)}t_{im}^{ab}\left(\mathscr{F}_{mi}+\frac{1}{2}t_j^e\mathscr{F}_{me}\right)+\nonumber \\
&+\frac{1}{2}\tau_{mn}^{ab}\mathscr{W}_{mnij}+\frac{1}{2}\tau_{ij}^{ef}\mathscr{W}_{abef}+\hat{P}_{(i/j)}\hat{P}_{(a/b)}\left(t_{im}^{ae}\mathscr{W}_{mbej}-t_i^et_m^a\braket{mb\vert\vert ej}\right)+\nonumber \\
&+\hat{P}_{(i/j)}t_i^e\braket{ab\vert\vert ej}-\hat{P}_{(a/b)}t_m^a\braket{mb\vert\vert ij}
\end{align}
The \acrshort{ccsd} amplitude equations are, again, nonlinear and require iterative solution methods to obtain the final amplitudes. The initial guess for the amplitudes is often set to zero, and the equations are solved iteratively until convergence is achieved.
\section{Multireference Methods}\label{sec:multireference_methods}
\chapter{Time Evolution in Quantum Mechanics}\label{sec:time_evolution}
Quantum dynamics describes the evolution of quantum systems over time and is fundamental in quantum chemistry for understanding molecular behavior at the atomic level. It is governed by the principles of quantum mechanics, particularly the \acrfull{tdse}, which dictates how wavefunctions change with time. Due to the complex nature of these equations, practical solutions are often limited to small or model systems.

There are two main approaches to quantum dynamics: real time propagation and imaginary time propagation. Real time propagation follows the natural evolution of a system in time, making it crucial for studying dynamic processes such as chemical reactions, electronic excitations, and non-equilibrium phenomena. Imaginary time propagation, on the other hand, is used to find the ground state of a quantum system by evolving it in an artificial imaginary time direction, gradually filtering out higher energy states. While these techniques provide deep insights into molecular quantum behavior, their application is restricted to relatively small or model systems due to the exponential increase in computational cost with system size.
\section{Real Time Propagation}\label{sec:real_time_propagation}
Real time propagation is a computational approach used to study the time evolution of quantum systems according to the \acrshort{tdse}. It is essential for modeling dynamic processes such as electronic excitations, molecular vibrations, and non equilibrium phenomena in quantum chemistry. By solving the wavefunction's evolution step by step in real time, this method captures transient states and ultrafast reactions. However, due to the exponential growth of computational complexity, real time propagation is feasible only for small systems or simplified models.

The time evolution in quantum mechanics is governed by the \acrshort{tdse} in the form
\begin{equation}\label{eq:tdse}
\symrm{i}\hbar\frac{\symrm{d}}{\symrm{d}t}\ket{\Psi\left(t\right)}=\hat{H}\ket{\Psi\left(t\right)},
\end{equation}
where $i$ is the imaginary unit, $\hbar$ is the reduced Planck constant, $\ket{\Psi\left(t\right)}$ is the time dependent wavefunction, $\hat{H}$ is the Hamiltonian operator, which encodes the total energy of the system. In the position representation, one could write $\Psi\left(x,t\right)$, where $x$ is the position coordinate for a quantum particle such as an electron or proton.

A general solution of the \acrshort{tdse} can be written as
\begin{equation}\label{eq:tdse_propagator}
\ket{\Psi\left(t\right)}=\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}\ket{\Psi\left(0\right)}.
\end{equation}
The action of $\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}$ on the initial wavefunction $\ket{\Psi\left(0\right)}$ propagates the state from time $0$ to time $t$, carrying all the information about the time evolution of the system. This exponential operator contains the complete description of how the system evolves with time. In practical applications, however, evaluating $\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}$ analytically is rarely straightforward for nontrivial Hamiltonians, which often necessitates numerical methods for an accurate treatment of quantum dynamics.
\subsection{Space Discretization and the Diabatic Basis}\label{sec:space_discretization_diabatic_basis}
To represent the wavefunction numerically, we sample it on a spatial grid. In practical terms, this means introducing a dependence on the coordinate $x$, so that $\ket{\Psi\left(t\right)}$ becomes $\ket{\Psi\left(x,t\right)}$. More importantly, we expand the wavefunction $\ket{\Psi\left(x,t\right)}$ in a set of basis functions $\lbrace\Phi_i\rbrace_{i=1}^N$, where $N$ is the number of states included in the dynamics. The basis set will be orthonormal and satisfy the condition
\begin{equation}\label{eq:diabatic_condition}
\bra{\Phi_i}\frac{\partial}{\partial x}\ket{\Phi_j}=0,\,i\neq j.
\end{equation}
Because these diabatic basis states are frequently taken simply as the canonical basis in $\mathbb{R}^N$ when defining model potential energy surfaces, their explicit functional forms are not crucial in most model-based simulations. Nonetheless, from a physical standpoint, these states are usually determined through quantum chemical computations that ensure minimal coordinate dependence in the electronic mixing. For a n-state system, the wavefunction in the diabatic basis can be written as
\begin{equation}\label{eq:diabatic_wavefunction}
\ket{\Psi\left(x,t\right)}=\sum_{i=1}^n c_i\left(x,t\right)\ket{\Phi_i}
\end{equation}
where $c_i\left(x,t\right)$ are the expansion coefficients of the wavefunction in the diabatic basis. The Hamiltonian operator $\hat{H}$ in the diabatic basis takes the form
\begin{equation}\label{eq:diabatic_hamiltonian}
\hat{H}=\hat{T}+\hat{V}=-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\hat{I}+\hat{V}
\end{equation}
where $\hat{I}$ is the identity operator, $\hat{T}$ is the kinetic energy operator, and $\hat{V}$ is the potential energy operator.
\subsection{Wavefunction Propagation}\label{sec:wavefunction_propagation}
We aim to reduce the action of the propagator on the diabatic wavefunction~\eqref{eq:diabatic_wavefunction} to a simple matrix multiplication. The main difficulty is the differential form of the kinetic operator in the equation~\eqref{eq:diabatic_hamiltonian}. To handle this, we use a Fourier-based method for applying the kinetic operator $\hat{T}$ on a wavefunction $\ket{\Psi\left(x,t\right)}$. Taking the \acrfull{ft} of $\hat{T}\ket{\Psi\left(x,t\right)}$ yields
\begin{align}\label{eq:kinetic_fourier_method}
\mathcal{F}\left\lbrace\hat{T}\ket{\Psi\left(x,t\right)}\right\rbrace&=\mathcal{F}\left\lbrace-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\ket{\Psi\left(x,t\right)}\right\rbrace=\frac{\hbar^2 k^2}{2m}\mathcal{F}\left\lbrace\ket{\Psi\left(x,t\right)}\right\rbrace \\
\hat{T}\ket{\Psi\left(x,t\right)}&=\frac{\hbar^2}{2m}\mathcal{F}^{-1}\left\lbrace k^2\mathcal{F}\left\lbrace\ket{\Psi\left(x,t\right)}\right\rbrace\right\rbrace,
\end{align}
where $k$ is the coordinate in the Fourier space. In other words, to apply the kinetic operator on the wavefunction, we need to \acrshort{ft} the wavefunction, multiply it by $\frac{\hbar^2k^2}{2m}$, and then apply the \acrfull{ift}. This method is computationally efficient, since the \acrshort{ft} can be done using the \acrfull{fft} algorithm, which has a complexity of $\mathcal{O}(N\log N)$, where $N$ is the number of grid points (contrary to the straightforward \acrfull{dft} with $\mathcal{O}(N^2)$ complexity).

A remaining challenge is the non-commutativity of the potential and kinetic operators. Because $\hat{V}$ and $\hat{T}$ do not commute we cannot factorize the propagator as
\begin{equation}
\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}\neq\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{V}t}\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{T}t}.
\end{equation}
If such a factorization were possible, we could apply the exponential of $\hat{V}$ in the position space and the exponential of $\hat{T}$ momentum space separately. To address this, we split the total propagation time $t$ into $n$ short steps $\Delta t$, so the full propagator becomes a product of short-time propagators as
\begin{equation}
\hat{U}(t)=\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}t}=\symbf{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t}\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t} \cdots\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t},
\end{equation}
where $\Delta t$ is a fixed time step. Now, each individual short-time propagator acts on the wave function and causes a small evolution. We can then use the symmetric splitting approximation
\begin{equation}
\symrm{e}^{-\frac{\symrm{i}}{\hbar}\hat{H}\Delta t}=\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}\hat{V}}\symrm{e}^{-\frac{\symrm{i}\Delta t}{\hbar}\hat{T}}\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}\hat{V}}+\mathcal{O}(\Delta t^3),
\end{equation}
which is valid for small $\Delta t$. Now we have everything we need to propagate the wavefunction in time. The formula for propagating the wavefunction from time $t$ to time $t+\Delta t$ is
\begin{equation}
\ket{\Psi\left(x,t+\Delta t\right)}=\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}V}\mathcal{F}^{-1}\left\lbrace\symrm{e}^{-\frac{\symrm{i}\Delta t}{\hbar}T}\mathcal{F}\left\lbrace\symrm{e}^{-\frac{\symrm{i}\Delta t}{2\hbar}V}\ket{\Psi\left(x,t\right)}\right\rbrace\right\rbrace,
\end{equation}
where $\hat{T}$ is the kinetic matrix in the momentum space, and $\hat{V}$ is the potential matrix in the position space. The matrix exponential is now simply a mathematical problem and is described .

Now that we have the wavefunction an any given time, we can calculate observables, such as the density matrix, position, momentum, etc. The expectation value of position is trivial to calculate, since the wavefunction is already in the position space. The expectation value of momentum can be calculated by applying \acrshort{ft} on the wavefunction and multiplying by the momentum operator. The density matrix can be calculated by taking the outer product of the wavefunction with itself.
\subsection{The Adiabatic Transform}\label{sec:the_adiabatic_transform}
Sometimes, we would like to see the results from the dynamics in the adiabatic basis (i.e., the basis where the potential matrix $V$ is diagonal). Most of the quantum chemistry software for molecular dynamics use the adiabatic basis, since the diabatic basis is not known. For that reason, most of the surface hopping algorithms also work in the adiabatic basis so it is convenient to know how to transform the wavefunction from the diabatic basis to the adiabatic basis to compare the results. To find the transformation from the diabatic basis to the adiabatic basis, we need to find a matrix $\symbf{U}$ that diagonalizes the potential matrix $V$. To do that, we solve the eigenvalue problem
\begin{equation}
V\symbf{U}=\symbf{U}\symbf{E},
\end{equation}
for each coordinate of the grid, where $\symbf{E}$ is a diagonal matrix with the eigenvalues of $V$ on the diagonal. The columns of $\symbf{U}$ are the eigenvectors of $V$. The matrix $\symbf{U}$ is the transformation matrix from the diabatic basis to the adiabatic basis. To transform the wavefunction from the diabatic basis to the adiabatic basis, we multiply the wavefunction by the transformation matrix $\symbf{U}^\dagger$ as
\begin{equation}
\ket{\Psi_{\text{adiabatic}}\left(x,t\right)}=\symbf{U}^\dagger\ket{\Psi_{\text{diabatic}}\left(x,t\right)}.
\end{equation}
The transformation matrix $\symbf{U}$ can be used to transform matrix representations of any operator from the diabatic basis to the adiabatic basis.
\section{Imaginary Time Propagation}\label{sec:imaginary_time_propagation}
Imaginary time propagation is a numerical technique used to find the ground state of a quantum system by evolving it in imaginary time instead of real time. By replacing time with an imaginary variable, higher energy states decay exponentially, leaving only the lowest-energy state. This method is widely used in quantum chemistry and condensed matter physics to determine electronic and nuclear ground states. However, like real time propagation, its applicability is limited to small systems due to the high computational cost of solving the underlying equations.

Similar to real time propagation, we start with the \acrshort{tdse}, but this time we perform a suspicious substitution $\tau\rightarrow\symrm{i}t$ to obtain the equation
\begin{equation}\label{eq:tdse_it}
\hbar\frac{\symrm{d}}{\symrm{d}\tau}\ket{\Psi\left(\tau\right)}=-\hat{H}\ket{\Psi\left(\tau\right)},
\end{equation}
which has a formal solution given by
\begin{equation}\label{eq:wf_it}
\ket{\Psi\left(\tau\right)}=\symrm{e}^{-\frac{\hat{H}\tau}{\hbar}}\ket{\Psi\left(0\right)}.
\end{equation}
We now expand the wavefunction in the basis of the eigenstates of the Hamiltonian $\hat{H}$ as
\begin{equation}\label{eq:wf_expansion_eigenstates}
\ket{\Psi\left(\tau\right)}=\sum_{n}c_{n}\left(\tau\right)\ket{\phi_{n}},
\end{equation}
where $\ket{\phi_{n}}$ are the eigenstates of the Hamiltonian $\hat{H}$ and $c_{n}\left(\tau\right)$ are the expansion coefficients. Substituting the wavefunction expression~\eqref{eq:wf_expansion_eigenstates} into the solution of \acrshort{tdse} in imaginary time~\eqref{eq:wf_it} we obtain
\begin{equation}\label{eq:tdse_it_expansion}
\ket{\Psi\left(\tau\right)}=\sum_{n}c_{n}\left(\tau\right)\symrm{e}^{-\frac{E_{n}\tau}{\hbar}}\ket{\phi_{n}},
\end{equation}
where $E_{n}$ are the eigenvalues of the Hamiltonian $\hat{H}$. We now see, that the bigger the value of $E_n$, the faster the corresponding term in the sum decays. This means that, in the limit $\tau\rightarrow\infty$, only the term corresponding to the smallest eigenvalue $E_{0}$ will survive, and the wavefunction will converge to the ground state of the system. Note that we need to normalize the wavefunction at each step of the imaginary time propagation to ensure that the wavefunction remains normalized, otherwise the wavefunction will decay to zero.
\chapter{Mixed Quantum-Classical Dynamics}\label{sec:mixed_quantum_classical_dynamics}
\section{Quantum Amplitude Propagation in Mixed Schemes}\label{sec:quantum_amplitude_propagation_in_mixed_themes}
Under the \acrfull{boa}, the total molecular wavefunction, $\ket{\Psi(t)}$, is formally separable into electronic and nuclear parts because the nuclear masses greatly exceed those of the electrons. However, when two or more electronic states approach degeneracy, the \acrshort{boa} separation fails and nonadiabatic effects become significant. \acrfull{tsh} is a general framework that treats nuclei and electrons separately: nuclei are propagated classically on an adiabatic \acrfull{pes}, while electrons are treated quantum mechanically.

In \acrshort{tsh}, the nuclei follow Newton's equations of motion on a single adiabatic \acrshort{pes} at any instant:
\begin{equation}
\symbf{M}\ddot{\symbf{R}}(t)=-\nabla_{\symbf{R}}E_j,
\end{equation}
where $\symbf{M}$ is the diagonal matrix with masses for each coordinate, $\symbf{R}(t)$ is nuclear geometry at time $t$, and $E_j$ is the electronic energy of the state $j$ at the nuclear configuration $\symbf{R}(t)$. Meanwhile, the electrons evolve according to the \acrshort{tdse}, with the electronic wavefunction $\ket{\Phi(t)}$ expanded in the instantaneous adiabatic eigenvectors of the electronic Hamiltonian $\hat{H}^{\symrm{el}}$ as
\begin{equation}
\Phi(t)=\sum_j c_j(t)\ket{\phi_j},
\end{equation}
where $\phi_j$ satisfies the \acrfull{tise}
\begin{equation}
\hat{H}^{\symrm{el}}\ket{\phi_j}=E_j\ket{\phi_j},
\end{equation}
and the complex coefficients $c_j$ encode the instantaneous probability amplitudes for occupying each adiabatic state. For notational brevity, we will often suppress explicit dependence on the variables where no ambiguity arises.

Substituting this ansatz into the full electronic \acrshort{tdse},
\begin{equation}
i\hbar\frac{\partial}{\partial t}\ket{\Phi(t)}=\hat{H}^{\symrm{el}}\ket{\Phi(t)},
\end{equation}
and projecting onto a particular adiabatic state $\ket{\phi_j}$, we obtain a set of coupled equations for the coefficients $c_j$:
\begin{equation}\label{eq:tsh_eom}
i\hbar\frac{\symrm{d}c_j}{\symrm{d}t}=\sum_k\left(\hat{H}_{jk}^{\symrm{el}}-i\hbar\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial t}}\right)c_k,
\end{equation}
where $\sigma_{jk}=\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial t}}$ is the \acrfull{tdc}. In the purely adiabatic basis, $\hat{H}^{\symrm{el}}$ is diagonal, so electronic population transfer between states is mediated entirely by $\sigma_{jk}$:
\begin{equation}
i\hbar\frac{\partial c_j}{\partial t}=E_j c_j-i\hbar\sum_k\sigma_{jk}c_k.
\end{equation}
To evaluate $\sigma_{jk}$ in practice, note that each adiabatic eigenfunction depends parametrically on the instantaneous nuclear coordinates $\symbf{R}(t)$. By the chain rule,
\begin{equation}
\sigma_{jk}=\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial t}}=\dot{\symbf{R}}\cdot\Braket{\phi_j\vert\frac{\partial\phi_k}{\partial\symbf{R}}}=\symbf{v}\cdot\symbf{d}_{jk},
\end{equation}
where $\symbf{v}=\dot{\symbf{R}}$ is the nuclear velocity and $\symbf{d}_{jk}$ is the \acrfull{nacv}. Although many quantum‐chemistry packages provide \acrshort{nacv} and thus allow computation of the \acrshort{tdc} via $\sigma_{jk}=\symbf{v}\cdot\symbf{d}_{jk}$, \acrshort{nacv} can become ill-defined or numerically unstable near conical intersections (e.g., in multi-reference methods). In cases where a reliable \acrshort{nacv} is not available, one may instead employ a direct \acrshort{tdc} approximation using wavefunction overlaps.
\section{The Time Derivative Coupling}\label{sec:time_derivative_coupling}
\acrshort{tdc} is a pivotal quantity in \acrshort{tsh} simulations because it enters the electronic equations of motion given in Eq.~\eqref{eq:tsh_eom}. In \emph{ab initio} molecular dynamics, the \acrshort{tdc} is commonly evaluated using the full \acrshort{nacv} as $\sigma_{jk}=\symbf{v}\cdot\symbf{d}_{jk}$, where $\symbf{v}$ is the nuclear velocity. When the dynamics evolves on analytic \acrshort{pes} or the \acrshort{nacv} is for any reason not available, it is often preferable to calculate the \acrshort{tdc} directly. The most straightforward strategy applies a finite-difference formula to the adiabatic wavefunctions
\begin{equation}\label{eq:tdc_fd1}
\sigma_{jk}(t)=\frac{1}{\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\Braket{\phi_j(t)\vert\phi_k(t)}\right)=\frac{1}{\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\delta_{jk}\right),
\end{equation}
with $\Delta t$ a small time step. When analytic \acrshort{pes} is available, the finite-difference estimate of the \acrshort{tdc} is straightforward to implement. In \emph{ab initio} molecular dynamics, however, it is usually more practical to recover this quantity from the non-adiabatic coupling vector, which most electronic-structure packages provide natively. The naïve first-order finite-difference formula suffers from well-known deficiencies: its truncation error scales linearly with the time step and it is highly sensitive to numerical noise. A simple remedy is to adopt the second-order approximation
\begin{equation}\label{eq:tdc_fd2}
\sigma_{jk}(t)=\frac{1}{2\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\Braket{\phi_j(t)\vert\phi_k(t-\Delta t)}\right).
\end{equation}
Although nominally second-order accurate, the approximation still retains significant shortcomings. In particular, it still violates the fundamental antisymmetry requirement, $\sigma_{jk}\neq-\sigma_{kj}$, undermining the Hermiticity of the electronic Hamiltonian. The following sections tackle these deficiencies and introduce more accurate, numerically robust strategies for evaluating the \acrshort{tdc}.
\subsection{The Hammes-Schiffer Tully Scheme}\label{sec:hammes_schiffer_tully_scheme}
The \acrfull{hst} scheme remedies the antisymmetry defect of Eq.~\eqref{eq:tdc_fd1} and Eq.~\eqref{eq:tdc_fd2}, making the electronic Hamiltonian Hermitian. To that end we first introduce a midpoint approximation for each adiabatic eigenstate $\phi_j$, obtained by linear interpolation between its values at the bracketing time slices
\begin{equation}\label{eq:hst_wfn}
\phi_j\left(t+\frac{\Delta t}{2}\right)=\frac{1}{2}\left(\phi_j(t+\Delta t)+\phi_j(t)\right).
\end{equation}
This midpoint state is not strictly normalized, a fully norm-preserving variant will be introduced in the next section. The time derivative at the same midpoint is obtained by combining a backward difference for $\phi_j(t+\Delta t)$ with a forward difference for $\phi_j(t)$, yielding
\begin{equation}\label{eq:hst_dwfn}
\frac{\partial \phi_j}{\partial t}\left(t+\frac{\Delta t}{2}\right)=\frac{1}{\Delta t}\left(\phi_j(t+\Delta t)-\phi_j(t)\right).
\end{equation}
The \acrshort{tdc} $\sigma_{jk}=\Braket{\phi_j\vert\frac{\partial \phi_k}{\partial t}}$ at a time $t+\frac{\Delta t}{2}$ is then formed as an overlap beween the expressions in Eq.~\eqref{eq:hst_wfn} and Eq.~\eqref{eq:hst_dwfn}:
\begin{equation}\label{eq:hst_tdc}
\sigma_{jk}\left(t+\frac{\Delta t}{2}\right)=\frac{1}{2\Delta t}\left(\Braket{\phi_j(t)\vert\phi_k(t+\Delta t)}-\Braket{\phi_j(t+\Delta t)\vert\phi_k(t)}\right).
\end{equation}
The \acrshort{hst} scheme in Eq.~\eqref{eq:hst_tdc} is a finite-difference approximation of the \acrshort{tdc}, which is also antisymmetric, i.e., $\sigma_{jk}=-\sigma_{kj}$. The scheme is numerically stable, but it does not preserve the norm of the adiabatic wavefunction in between the timesteps. Any rapidly varying wavefunction will suffer from a significant normalization error, which can lead to unphysical results. Such a case occurs for two diabatic surfaces coupled by zero diabatic interaction: in the adiabatic basis the exact \acrshort{tdc} diverges, yet the \acrshort{hst} formula collapses to zero, predicting no population transfer. To address this issue, we can apply a \acrfull{npi} to the adiabatic wavefunction, which is discussed in the next section.
\subsection{The Norm-Preserving Interpolation}\label{sec:norm_preserving_interpolation}
\subsection{The Baeck--An Scheme}\label{sec:baeck_an}
\section{Ehrenfest Dynamics}\label{sec:ehrenfest_dynamics}
\section{Fewest Switches Surface Hopping}\label{sec:fewest_switches}
\section{Landau--Zener Surface Hopping}\label{sec:landau_zener}
\section{Mapping Approach to Surface Hopping}\label{sec:mapping_approach}
\section{Beyond Surface Hopping}\label{sec:beyond_surface_hopping}
\begin{appendices}
\chapter{Mathematical Methods}\label{sec:mathematical_methods}
\section{Matrix Exponential}\label{sec:matrix_exponential}
\section{Generalized Eigenvalue Problem}\label{sec:generalized_eigenvalue_problem}
The generalized eigenvalue problem is a mathematical problem that arises in many areas of science and engineering. It is a generalization of the standard eigenvalue problem, where we seek the eigenvalues and eigenvectors of a square matrix. In the generalized eigenvalue problem, we consider two square matrices $\symbf{A}$ and $\symbf{B}$, and we seek the eigenvalues $\lambda$ and eigenvectors $\symbf{x}$ that satisfy the equation
\begin{equation}\label{eq:gen_eig}
\symbf{A}\symbf{C}=\symbf{B}\symbf{C}\symbf{\Lambda},
\end{equation}
where $\symbf{C}$ is a matrix of eigenvectors and $\symbf{\Lambda}$ is a diagonal matrix of eigenvalues. The quick way to solve the generalized eigenvalue problem is to transform it into a standard eigenvalue problem by multiplying both sides of the equation by the inverse of $\symbf{B}$ as
\begin{equation}
\symbf{B}^{-1}\symbf{A}\symbf{C}=\symbf{C}\symbf{\Lambda}.
\end{equation}
This method is not always numerically stable, especially when the matrices $\symbf{A}$ and $\symbf{B}$ are ill-conditioned. Should you try to use this method for the Roothaan equations in the \acrshort{hf} method, you would find that the solution is is not correct. A more stable approach is to modify the equation~\eqref{eq:gen_eig} as
\begin{align}
\symbf{A}\symbf{C}&=\symbf{B}\symbf{C}\symbf{\Lambda}\nonumber \\
\symbf{B}^{-\frac{1}{2}}\symbf{A}\symbf{C}&=\symbf{B}^{\frac{1}{2}}\symbf{C}\symbf{\Lambda}\nonumber \\
\symbf{B}^{-\frac{1}{2}}\symbf{A}\symbf{B}^{-\frac{1}{2}}\symbf{B}^{\frac{1}{2}}\symbf{C}&=\symbf{B}^{\frac{1}{2}}\symbf{C}\symbf{\Lambda},
\end{align}
where we solve the standard eigenvalue problem for the matrix $\symbf{B}^{-\frac{1}{2}}\symbf{A}\symbf{B}^{-\frac{1}{2}}$ and obtain the eigenvectors $\symbf{B}^{\frac{1}{2}}\symbf{C}$ and eigenvalues $\symbf{\Lambda}$.\supercite{10.48550/arXiv.1903.11240} The eigenvectors of the original problem are then given by $\symbf{C}=\symbf{B}^{-\frac{1}{2}}\symbf{B}^{\frac{1}{2}}\symbf{C}$ and the eigenvalues are the same.
\end{appendices}
\printglossary[type=\acronymtype]
\printbibliography
\end{document}
